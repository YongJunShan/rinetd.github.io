<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>时光小栈 on 时光小栈</title>
        <link>/</link>
        <language>zh-CN</language>
        <author>rinetd</author>
        <rights>Copyright (c) 2015, rinetd; all rights reserved.</rights>
        <updated>Wed, 11 Dec 2019 11:17:08 CST</updated>
        
        <item>
            <title>Linux下头文件以及库 编译链接运行时的搜寻路径顺序</title>
            <link>/language/clang/make-gcc-include-order/</link>
            <pubDate>Wed, 11 Dec 2019 11:17:08 CST</pubDate>
            <author>rinetd</author>
            <guid>/language/clang/make-gcc-include-order/</guid>
            <description>

&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/avideointerfaces/article/details/96157710&#34; target=&#34;_blank&#34;&gt;Linux下头文件以及库 编译链接运行时的搜寻路径顺序 - ltshan139的专栏&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;LIBRARY_PATH 环境变量用于在程序&lt;strong&gt;编译期间&lt;/strong&gt;查找动态链接库时指定查找共享库的路径，例如，指定gcc编译需要用到的动态链接库的目录。&lt;br /&gt;
LD_LIBRARY_PATH 环境变量用于在程序加载&lt;strong&gt;运行期间&lt;/strong&gt;查找动态链接库时指定除了系统默认路径之外的其他路径，注意，LD_LIBRARY_PATH中指定的路径会在系统默认路径之前进行查找。&lt;/p&gt;

&lt;h3 id=&#34;gcc-编译时头文件-h的搜寻路径&#34;&gt;gcc 编译时头文件.h的搜寻路径&lt;/h3&gt;

&lt;p&gt;其搜寻优先顺序由高到低为：&lt;/p&gt;

&lt;p&gt;1）先从 -I（大写i）指定的头文件目录开始找&lt;/p&gt;

&lt;p&gt;2）然后从gcc环境变量 C_INCLUDE_PATH ，cplus_Include_path , OBJC_Include_Path指定的路径来寻找&lt;/p&gt;

&lt;p&gt;3）最后从系统目录  /usr/include 或 /usr/local/include 或/usr/lib/gcc_lib/i386-linux/2.952/include 下寻找&lt;/p&gt;

&lt;p&gt;4)  补充：从当前目录来搜寻 ./&lt;/p&gt;

&lt;h3 id=&#34;ld-链接时动态库-so或静态库-a的搜寻路径&#34;&gt;ld 链接时动态库.so或静态库.a的搜寻路径&lt;/h3&gt;

&lt;p&gt;其优先顺序：&lt;/p&gt;

&lt;p&gt;1）先从 -L指定的目录来搜寻库&lt;/p&gt;

&lt;p&gt;2）然后从gcc环境变量 LIBRARY_PATH指定的目录寻找&lt;/p&gt;

&lt;p&gt;3）最后从系统目录 /lib 或 /usr/lib 或 /usr/local/lib来寻找&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&#34;运行时动态库-so的加载路径&#34;&gt;运行时动态库.so的加载路径&lt;/h3&gt;

&lt;p&gt;注意，只有动态库so文件才需要考虑运行时的加载。 动态库经常因为路径的原因导致无法加载的错误，这使得动态库使用起来相对比较麻烦些，但是它的优势也很明显：编译时不需要真正链接到可执行文件里面。这样意味着，每次动态库升级，不需要重新编译可执行文件，只需要把升级后的so文件拷贝到正确的加载路径即可。 &lt;/p&gt;

&lt;p&gt;其加载优先顺序为：&lt;/p&gt;

&lt;p&gt;1）-L指定的动态库搜寻路径（通常不好使。 毕竟嵌入式环境中，编译和运行平台完全不一样）&lt;/p&gt;

&lt;p&gt;2）环境变量LD_LIBRARY_PATH指定的动态库搜寻路径&lt;/p&gt;

&lt;p&gt;3）/etc/ld.so.conf 里面添加动态库搜索路径，记得要执行ldconfig来生效。&lt;/p&gt;

&lt;p&gt;4）默认系统运行库搜索路径  /lib 或 /usr/lib&lt;br /&gt;
————————————————&lt;br /&gt;
版权声明：本文为CSDN博主「ltshan139」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。&lt;br /&gt;
原文链接：&lt;a href=&#34;https://blog.csdn.net/avideointerfaces/article/details/96157710&#34; target=&#34;_blank&#34;&gt;https://blog.csdn.net/avideointerfaces/article/details/96157710&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>clang define extern c</title>
            <link>/language/clang/clang-define-extern-c/</link>
            <pubDate>Wed, 11 Dec 2019 10:23:02 CST</pubDate>
            <author>rinetd</author>
            <guid>/language/clang/clang-define-extern-c/</guid>
            <description>&lt;p&gt;extern &amp;quot;C&amp;quot; 在C/C++中的用法&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;无论是C++调用C api 还是C调用C++ API， 必须先在API所对应得头文件对API进行 extern &amp;quot;C&amp;quot;声明，并用#ifdef __cplusplus进行保护。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;extern &amp;quot;C&amp;quot; 的作用是告诉C++编译器，函数名在编译的时候按照C的格式来进行处理，不要添加冗余的字段。&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#ifndef TEST_C
#define TEST_C

#ifdef __cplusplus
extern &amp;quot;C&amp;quot; {
#endif
 
int add(int a, int b);
 
#ifdef __cplusplus
}
#endif 
 
#endif

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;前言&lt;br /&gt;
在重构sample sdk代码，并编译成静态或动态库来提供给上层调用的API时，不可避免会遇到c++调用c或c调用c++的问题。看我博客的网友也提到了这个问题，所以藉这个机会来把C和C++混合编程（即相互调用）的问题透彻的弄清楚。&lt;/p&gt;

&lt;p&gt;问题背景&lt;br /&gt;
C和C++直接相互调用 之所以会出问题的原因是， C++里面有函数重载的概念，从而导致编译出来的函数名会带上参数类型，而C编译出来的函数名简单的就是其本身。&lt;/p&gt;

&lt;p&gt;下面定义了两个简单的c、c++头文件和实现文件&lt;/p&gt;

&lt;p&gt;/&lt;em&gt;test_c.h&lt;/em&gt;/&lt;br /&gt;
#ifndef TEST_C&lt;br /&gt;
#define TEST_C&lt;/p&gt;

&lt;p&gt;int add(int a, int b);&lt;/p&gt;

&lt;p&gt;#endif&lt;br /&gt;
/&lt;em&gt;test_c.c&lt;/em&gt;/&lt;br /&gt;
#include &amp;quot;test_c.h&amp;quot;&lt;/p&gt;

&lt;p&gt;int add(int a, int b)&lt;br /&gt;
{&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int c;
c = a + b;
return c;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;br /&gt;
 &lt;/p&gt;

&lt;p&gt;/&lt;em&gt;test_c++.h&lt;/em&gt;/&lt;br /&gt;
#ifndef TEST_C_PLUSPLUS&lt;br /&gt;
#define TEST_C_PLUSPLUS&lt;/p&gt;

&lt;p&gt;int substract(int a, int b);&lt;/p&gt;

&lt;p&gt;#endif&lt;br /&gt;
/&lt;em&gt;test_c++.cpp&lt;/em&gt;/&lt;br /&gt;
#include &amp;quot;test_c++.h&amp;quot;&lt;/p&gt;

&lt;p&gt;int substract(int a, int b)&lt;br /&gt;
{&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int c;
c = a - b;
return c;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;br /&gt;
 分别使用两个命令生成obj文件。&lt;/p&gt;

&lt;p&gt;gcc -c test_c.c&lt;br /&gt;
gcc -c test_c++.cpp&lt;br /&gt;
然后使用命令： nm test_c.o | grep add  和  nm test_c++.o | grep substract 得到如下结果：&lt;/p&gt;

&lt;p&gt;add 和 _Z9substractii。  这个结果进一步验证了前面所说的，即C和C++编译出来的函数名会不一样。&lt;/p&gt;

&lt;p&gt;C调用C++ &lt;br /&gt;
添加一个C测试文件main.c，并调用add和substract API。&lt;/p&gt;

&lt;p&gt;/&lt;em&gt;main.c&lt;/em&gt;/&lt;br /&gt;
#include &lt;stdio.h&gt;&lt;br /&gt;
#include &amp;quot;test_c.h&amp;quot;&lt;br /&gt;
#include &amp;quot;test_c++.h&amp;quot;&lt;/p&gt;

&lt;p&gt;int main(void)&lt;br /&gt;
{&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int x = 3;
int y = 5;

printf(&amp;quot;add = %d \n&amp;quot;, add(x, y));
printf(&amp;quot;sub = %d \n&amp;quot;, substract(x, y));

return 0;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;br /&gt;
使用命令如下来将它们编译成一个可执行文件test_c：&lt;/p&gt;

&lt;p&gt;gcc test_c.c test_c++.cpp main.c -o test_c&lt;br /&gt;
但这个时候会报链接错误： main.c:(.text+0x43): undefined reference to &#39;substract&#39; 。明明我们在test_c++.cpp里面定义了substract()，但是main.c不识别。 这也说明了 c直接调用c++代码出现错误。&lt;/p&gt;

&lt;p&gt;解决办法就是引入 extern &amp;quot;C&amp;quot;。 关于它有两个重要注意点：&lt;/p&gt;

&lt;p&gt;1）extern &amp;quot;C&amp;quot;是c++语法，只能被c++编译器认识，其目的就是告诉C++编译器，其被extern &amp;quot;C&amp;quot;所包含的函数得以C方式（即简单函数名）去链接。&lt;/p&gt;

&lt;p&gt;2) 为了避免C编译器来编译它来产生编译错误， 保险起见，会将 extern &amp;quot;C&amp;quot; {和}用#ifdef _&lt;em&gt;cplusplus... #endif修饰起来。这里很容易犯得一个错误就是 cplusplus前面是两根下划线&amp;quot;&lt;/em&gt;&amp;quot;。&lt;/p&gt;

&lt;p&gt;回到上面得错误，我们得在test_c++.cpp编译时，得用c方式去来生成substract函数。所以test_cpp.h更新如下，并用__cplusplus保护起来，避免main.c编译时报错。&lt;/p&gt;

&lt;p&gt;/&lt;em&gt;test_c++.h&lt;/em&gt;/&lt;br /&gt;
#ifndef TEST_C_PLUSPLUS&lt;br /&gt;
#define TEST_C_PLUSPLUS&lt;/p&gt;

&lt;p&gt;#ifdef __cplusplus&lt;br /&gt;
extern &amp;quot;C&amp;quot; {&lt;br /&gt;
#endif&lt;/p&gt;

&lt;p&gt;int substract(int a, int b);&lt;/p&gt;

&lt;p&gt;#ifdef __cplusplus&lt;br /&gt;
}&lt;br /&gt;
#endif&lt;/p&gt;

&lt;p&gt;#endif&lt;br /&gt;
再次编译链接： gcc test_c.c test_c++.cpp main.c -o test_c没有问题，并执行./test_c 结果也完全正确。&lt;/p&gt;

&lt;p&gt;C++调用C&lt;br /&gt;
添加一个C++测试文件main.cpp来调用add和substract。&lt;/p&gt;

&lt;p&gt;/&lt;em&gt;main.cpp&lt;/em&gt;/&lt;br /&gt;
#include &lt;stdio.h&gt;&lt;br /&gt;
#include &amp;quot;test_c.h&amp;quot;&lt;br /&gt;
#include &amp;quot;test_c++.h&amp;quot;&lt;/p&gt;

&lt;p&gt;int main(void)&lt;br /&gt;
{&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int x = 3;
int y = 5;

printf(&amp;quot;add = %d \n&amp;quot;, add(x, y));
printf(&amp;quot;sub = %d \n&amp;quot;, substract(x, y));

return 0;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;br /&gt;
使用类似命令来编译链接： gcc test_c.c test_c++.cpp main.cpp -o test_cpp，结果也出现main.cpp里面找不到定义在test_c.c里面得add函数。&lt;/p&gt;

&lt;p&gt;同样地，必须在test_c.h里面添加extern ”C“来告诉main.cpp编译链接时得以c方式来寻找add函数。同时使用__cplusplus修饰避免 test_c.c编译失败。&lt;/p&gt;

&lt;p&gt;/&lt;em&gt;test_c.h&lt;/em&gt;/&lt;br /&gt;
#ifndef TEST_C&lt;br /&gt;
#define TEST_C&lt;/p&gt;

&lt;p&gt;#ifdef __cplusplus&lt;br /&gt;
extern &amp;quot;C&amp;quot; {&lt;br /&gt;
#endif&lt;/p&gt;

&lt;p&gt;int add(int a, int b);&lt;/p&gt;

&lt;p&gt;#ifdef __cplusplus&lt;br /&gt;
}&lt;br /&gt;
#endif&lt;br /&gt;
#endif&lt;br /&gt;
使用编译链接命令：gcc test_c.c test_c++.cpp main.cpp -o test_cpp 没有问题，并执行./test_cpp 结果也完全正确。&lt;/p&gt;

&lt;p&gt;结论&lt;br /&gt;
无论是C++调用C api 还是C调用C++ API， 必须先在API所对应得头文件对API进行 extern &amp;quot;C&amp;quot;声明，并用#ifdef __cplusplus进行保护。&lt;br /&gt;
————————————————&lt;br /&gt;
版权声明：本文为CSDN博主「ltshan139」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。&lt;br /&gt;
原文链接：&lt;a href=&#34;https://blog.csdn.net/avideointerfaces/article/details/97765339&#34; target=&#34;_blank&#34;&gt;https://blog.csdn.net/avideointerfaces/article/details/97765339&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>clang 3维数组的初始化</title>
            <link>/language/clang/clang-array/</link>
            <pubDate>Wed, 11 Dec 2019 10:20:42 CST</pubDate>
            <author>rinetd</author>
            <guid>/language/clang/clang-array/</guid>
            <description>&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt; int a[2][3][4] = {
        {{1,2,3,4},{1,2,3,4},{1,2,3,4}},
        {{5,6,7,8},{5,6,7,8},{5,6,7,8}}
    };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意初始化要先从最外围开始。&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>cnn iou_roi_roc_auc</title>
            <link>/ai/cnn-iou_roi_roc_auc/</link>
            <pubDate>Wed, 11 Dec 2019 10:05:16 CST</pubDate>
            <author>rinetd</author>
            <guid>/ai/cnn-iou_roi_roc_auc/</guid>
            <description>

&lt;h1 id=&#34;做机器学习-再别把iou-roi-和-roc-auc-搞混了-聊聊目标检测-医疗领域的那些评价函数&#34;&gt;做机器学习，再别把IoU，ROI 和 ROC，AUC 搞混了 ！聊聊目标检测，医疗领域的那些评价函数&lt;/h1&gt;

&lt;p&gt;涉及领域不多的机器学习爱好者经常会把IoU，ROI 和 ROC，AUC 这样的评价函数（Metric functions）搞混。其实记住它们也没那么难，David 9今天就来帮大家理一理：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;IoU&lt;/strong&gt; (Intersection over Union)，交集并集比&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;ROI&lt;/strong&gt; (region of interest) , 感兴趣区域,就是人工labelImg标注的绿色框，就是真实感兴趣区域&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;ROC&lt;/strong&gt; (Receiver Operating Characteristic curve) 受试者工作特征曲线&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;AUC&lt;/strong&gt; (Area Under the Curve) ， 曲线下区域&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;首先要区分，&lt;strong&gt;前两个&lt;/strong&gt; 是&lt;strong&gt;目标检测领域&lt;/strong&gt;的术语；&lt;br /&gt;
&lt;strong&gt;后两个&lt;/strong&gt;是从&lt;strong&gt;医疗领域&lt;/strong&gt;引进的，但是所有机器学习准确率都可能用到该指标。&lt;/p&gt;

&lt;p&gt;最容易理解的是第2个&lt;strong&gt;ROI&lt;/strong&gt;，我们做任何目标检测在准备数据集时都要&lt;strong&gt;选择感兴趣区域&lt;/strong&gt;, &lt;a href=&#34;http://nooverfit.com/wp/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%A1%A5%E4%B9%A0%E8%B4%B4%E4%B9%8Br-cnn%E7%B3%BB%E5%88%97-r-cnn-fast-r-cnn-faster-r-cnn/&#34; target=&#34;_blank&#34;&gt;我们之前的文章也提到过&lt;/a&gt;：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://nooverfit.com/wp/wp-content/uploads/2018/01/demo3.jpg&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://nooverfit.com/wp/wp-content/uploads/2018/01/demo3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/tzutalin/labelImg&#34; target=&#34;_blank&#34;&gt;labelImg&lt;/a&gt; 就是一个不错的标注工具。帮助你选择&lt;strong&gt;目标检测的感兴趣框&lt;/strong&gt;。是的这不是一个评价函数，是一个&lt;strong&gt;概念&lt;/strong&gt;而已。&lt;/p&gt;

&lt;p&gt;目标检测的训练集图片准备完成后，真正的评价函数是&lt;strong&gt;IoU（Intersection over Union）。为什么要用交集与并集的比值呢？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;让我们看看&lt;strong&gt;一味地追求交集最大会发生什么&lt;/strong&gt;：&lt;/p&gt;

&lt;figure id=&#34;attachment_4262&#34; aria-describedby=&#34;caption-attachment-4262&#34; style=&#34;width: 399px&#34; class=&#34;wp-caption aligncenter&#34;&gt;[![](http://nooverfit.com/wp/wp-content/uploads/2018/01/QQ%E6%88%AA%E5%9B%BE20180126154935.png)](http://nooverfit.com/wp/wp-content/uploads/2018/01/QQ%E6%88%AA%E5%9B%BE20180126154935.png)
&lt;figcaption id=&#34;caption-attachment-4262&#34; class=&#34;wp-caption-text&#34;&gt;来自：https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;上图绿色框是&lt;strong&gt;真实感兴趣区域&lt;/strong&gt;，红色框是&lt;strong&gt;预测区域&lt;/strong&gt;，这种情况下交集确实是最大的，但是红色框并不能准确预测物体位置。因为&lt;strong&gt;预测区域总是试图覆盖目标物体而不是正好预测物体位置&lt;/strong&gt;。这时如果我们能除以一个并集的大小，就可以规避这种问题：&lt;/p&gt;

&lt;figure id=&#34;attachment_4250&#34; aria-describedby=&#34;caption-attachment-4250&#34; style=&#34;width: 400px&#34; class=&#34;wp-caption aligncenter&#34;&gt;[![](http://nooverfit.com/wp/wp-content/uploads/2018/01/QQ%E6%88%AA%E5%9B%BE20180126150126.png)](http://nooverfit.com/wp/wp-content/uploads/2018/01/QQ%E6%88%AA%E5%9B%BE20180126150126.png)
&lt;figcaption id=&#34;caption-attachment-4250&#34; class=&#34;wp-caption-text&#34;&gt;来自：https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;这样，如果我们控制并集不要让并集太大，对准确预测是有益的。具体IoU的计算大家可以参考文章：&lt;a href=&#34;https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/&#34; target=&#34;_blank&#34;&gt;https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;接下来，是两个&lt;strong&gt;分类准确率&lt;/strong&gt;的指标：&lt;strong&gt;ROC和AUC&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;首先ROC受试者工作特征曲线，听名字就是从医疗领域引入的。理解之前我们先要回顾一下&lt;strong&gt;真阳率（TP），假阳率（FP），假阴率(FN)，真阴率(TN)&lt;/strong&gt;的概念：&lt;/p&gt;

&lt;figure id=&#34;attachment_4267&#34; aria-describedby=&#34;caption-attachment-4267&#34; style=&#34;width: 812px&#34; class=&#34;wp-caption aligncenter&#34;&gt;[![](http://nooverfit.com/wp/wp-content/uploads/2018/01/QQ%E6%88%AA%E5%9B%BE20180126160615.png)](http://nooverfit.com/wp/wp-content/uploads/2018/01/QQ%E6%88%AA%E5%9B%BE20180126160615.png)
&lt;figcaption id=&#34;caption-attachment-4267&#34; class=&#34;wp-caption-text&#34;&gt;来自：https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;很熟悉吧？&lt;strong&gt;真阳率（TP）就是预测类别1而事实上是类别1， 假阳率（FP）就是预测类别1但是事实上部署类别1.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;把&lt;strong&gt;TP作为纵坐标，FP作为横坐标，就是一个类别的ROC曲线了&lt;/strong&gt; ：&lt;/p&gt;

&lt;figure id=&#34;attachment_4269&#34; aria-describedby=&#34;caption-attachment-4269&#34; style=&#34;width: 533px&#34; class=&#34;wp-caption aligncenter&#34;&gt;[![](http://nooverfit.com/wp/wp-content/uploads/2018/01/QQ%E6%88%AA%E5%9B%BE20180126161740.png)](http://nooverfit.com/wp/wp-content/uploads/2018/01/QQ%E6%88%AA%E5%9B%BE20180126161740.png)
&lt;figcaption id=&#34;caption-attachment-4269&#34; class=&#34;wp-caption-text&#34;&gt;来自; https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;曲线下的蓝色区域就是曲线下区域&lt;strong&gt;AUC&lt;/strong&gt; (Area Under the Curve)了 如果&lt;strong&gt;AUC&lt;/strong&gt;的面积为1，恭喜你，在这个类别上你的准确率是最高的。AUC一般越大越好，说明某个&lt;strong&gt;类别的分类准确度越高&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;参考文献：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/&#34; target=&#34;_blank&#34;&gt;https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.learnopencv.com/how-to-select-a-bounding-box-roi-in-opencv-cpp-python/&#34; target=&#34;_blank&#34;&gt;https://www.learnopencv.com/how-to-select-a-bounding-box-roi-in-opencv-cpp-python/&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it&#34; target=&#34;_blank&#34;&gt;https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tzutalin/labelImg&#34; target=&#34;_blank&#34;&gt;https://github.com/tzutalin/labelImg&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Region_of_interest&#34; target=&#34;_blank&#34;&gt;https://en.wikipedia.org/wiki/Region_of_interest&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
        <item>
            <title>目标检测中region proposal的作用</title>
            <link>/ai/frcn-regional_proposal/</link>
            <pubDate>Wed, 11 Dec 2019 10:01:13 CST</pubDate>
            <author>rinetd</author>
            <guid>/ai/frcn-regional_proposal/</guid>
            <description>&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/265345106&#34; target=&#34;_blank&#34;&gt; 目标检测中region proposal的作用？ - 知乎&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;RPN (Region Proposal Network):&lt;/p&gt;

&lt;p&gt;Bounding-box： RPN同时也会在feature map上框定这些ROI感兴趣区域的大致位置，即输出Bounding-box。&lt;/p&gt;

&lt;p&gt;作者：YJHMITWEB&lt;br /&gt;
链接：&lt;a href=&#34;https://www.zhihu.com/question/265345106/answer/294410307&#34; target=&#34;_blank&#34;&gt;https://www.zhihu.com/question/265345106/answer/294410307&lt;/a&gt;&lt;br /&gt;
来源：知乎&lt;br /&gt;
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。&lt;/p&gt;

&lt;p&gt;先更正下提问者的描述，yolo本身不含有anchor机制。以下回答适合对目标检测中anchor的作用和机制比较了解的读者阅读。首先我们明确一个定义，当前主流的Object Detection框架分为1 stage和2 stage，而2 stage多出来的这个stage就是Regional Proposal过程，明确这一点后，我们继续讲。Regional Proposal的输出到底是什么？我们首先看一下以Faster R-CNN为代表的2 stage目标检测方法&lt;br /&gt;
&lt;img src=&#34;https://pic3.zhimg.com/50/v2-68dbe8ed2d039e925fffa055244439c5_hd.jpg&#34; data-rawwidth=&#34;1471&#34; data-rawheight=&#34;1206&#34; data-size=&#34;normal&#34; class=&#34;origin_image zh-lightbox-thumb&#34; width=&#34;1471&#34; data-original=&#34;https://pic3.zhimg.com/v2-68dbe8ed2d039e925fffa055244439c5_r.jpg&#34;/&gt;图1可以看到，图中有两个Classification loss和两个Bounding-box regression loss，有什么区别呢？1、Input Image经过CNN特征提取，首先来到Region Proposal网络。由Region Proposal Network输出的Classification，这并不是判定物体在COCO数据集上对应的80类中哪一类，而是输出一个Binary的值p，可以理解为  ，人工设定一个threshold=0.5。RPN网络做的事情就是，如果一个Region的  ，则认为这个Region中可能是80个类别中的某一类，具体是哪一类现在还不清楚。到此为止，Network只需要把这些可能含有物体的区域选取出来就可以了，这些被选取出来的Region又叫做ROI （Region of Interests），即感兴趣的区域。当然了，RPN同时也会在feature map上框定这些ROI感兴趣区域的大致位置，即输出Bounding-box。&lt;/p&gt;

&lt;p&gt;这里以faster rcnn举例。在faster rcnn里面，anchor（或者说RPN网络）的作用是代替以往rcnn使用的selective search的方法寻找图片里面可能存在物体的区域。&lt;br /&gt;
当一张图片输入resnet或者vgg，在最后一层的feature map上面，寻找可能出现物体的位置，&lt;br /&gt;
这时候分别以这张feature map的每一个点为中心，在原图上画出9个尺寸不一anchor。然后计算anchor与GT（ground truth） box的iou（重叠率），满足一定iou条件的anchor，便认为是这个anchor包含了某个物体。&lt;/p&gt;

&lt;p&gt;目标检测的思想是，首先在图片中寻找“可能存在物体的位置（regions）”，然后再判断“这个位置里面的物体是什么东西”，所以region proposal就参与了判断物体可能存在位置的过程。&lt;/p&gt;

&lt;p&gt;region proposal是让模型学会去看哪里有物体，GT box就是给它进行参考，告诉它是不是看错了，该往哪些地方看才对。&lt;br /&gt;
建议详细阅读这个领域一系列的论文，从rcnn、sppnet、frcnn、faster rcnn到ssd、yolo，整条线看下来就能大概明白目标检测的“套路”。&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>nnie mapper</title>
            <link>/hardware/hisilicon/nnie/nnie-mapper/</link>
            <pubDate>Tue, 10 Dec 2019 19:32:45 CST</pubDate>
            <author>rinetd</author>
            <guid>/hardware/hisilicon/nnie/nnie-mapper/</guid>
            <description>

&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_42464187/article/details/102955640&#34; target=&#34;_blank&#34;&gt;Ubuntu18.04下NNIE模型转换环境搭建 - Ray的博客&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://blog.csdn.net/avideointerfaces/article/details/102417997&#34; target=&#34;_blank&#34;&gt;nnie上进行图像数据预处理（Normalize）的五种方式 - ltshan139的专栏&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-ubuntu18-04-nnie-mapper-12-环境安装&#34;&gt;1. Ubuntu18.04 nnie_mapper_12 环境安装&lt;/h2&gt;

&lt;p&gt;参考文档 《HiSVP 开发指南》3.4节 Linux 版 NNIE mapper 安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;linux 版本的 NNIE mapper 编译环境：ubuntu 14.04，gcc version 4.8.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下文以 ubuntu14.04 为例安装 linux 版本的 NNIE mapper。&lt;/p&gt;

&lt;h4 id=&#34;3-4-1-mapper-依赖库-protobuf-安装与配置&#34;&gt;3.4.1 mapper 依赖库 Protobuf 安装与配置&lt;/h4&gt;

&lt;p&gt;请至 &lt;a href=&#34;https://github.com/google/protobuf/releases/tag/v3.5.1&#34; target=&#34;_blank&#34;&gt;https://github.com/google/protobuf/releases/tag/v3.5.1&lt;/a&gt; ，进入下载页面点击 protobufall-3.5.1.tar.gz：&lt;/p&gt;

&lt;p&gt;export PATH=/home/test/protobuflib/protobuf-3.5.1/bin:$PATH&lt;br /&gt;
export LD_LIBRARY_PATH=/home/test/protobuflib/protobuf-3.5.1/lib:$LD_LIBRARY_PATH&lt;br /&gt;
export PKG_CONFIG_PATH=/home/test/protobuflib/protobuf-3.5.1/lib/pkgconfig&lt;/p&gt;

&lt;p&gt;3.4.2 mapper 依赖库 OpenCV 安装与配置 3.4.2.1 OpenCV 安装包下载&lt;br /&gt;
请到 OpenCV 的官方网站:&lt;a href=&#34;http://opencv.org/releases.html&#34; target=&#34;_blank&#34;&gt;http://opencv.org/releases.html&lt;/a&gt; 下载 Opencv3.4.0&lt;/p&gt;

&lt;h3 id=&#34;模型转换&#34;&gt;模型转换&lt;/h3&gt;

&lt;p&gt;注意：marked_prototxt是选择自己的caffe model后自动生成的并更新的，无法自己填写。理论上是每次选择caffe文件后都会自动生成一个并更新过去，有时软件问题会生成文件但无法自动更新，此时需要将.cfg文件 open with -  text， 在文件的第一行自己填写在mark_prototxt下最新生成的结构文件，如图&lt;/p&gt;

&lt;p&gt;————————————————&lt;br /&gt;
版权声明：本文为CSDN博主「他们叫我高老师」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。&lt;br /&gt;
原文链接：&lt;a href=&#34;https://blog.csdn.net/qq_34533248/article/details/102496209&#34; target=&#34;_blank&#34;&gt;https://blog.csdn.net/qq_34533248/article/details/102496209&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;注意 ：&lt;br /&gt;
你的gcc的版本最好是4.8，我的是4.8.5，而且你的protobuf需要是在这个版本下编译的，否则会报错 undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringE&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GCC 版本必须是4.8&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Protobuf版本必须是 3.5.1&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;用命令：&lt;code&gt;nm -s libprotobuf.so|grep _ZN6google8protobuf8internal26fixed_address_empty_stringE&lt;/code&gt;会发现有这个符号，但是多了B5cxx11，这个是和编译程序的gcc程序有关系，由于protobuf的源码中含有c++程序，所以需要使用gcc4.8.5和g++4.8.5来编译protobuf才行。&lt;/p&gt;

&lt;p&gt;模型转换命令：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nnie_mapper_12 yolov3_inst.cfg&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;yolov3_inst.cfg 文件介绍：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-s&#34;&gt;[prototxt_file]  ./yolov3.prototxt        # caffe 的prototxt  
[caffemodel_file] ./yolov3.caffemodel     # caffe 的model文件
[batch_num] 1                            # [单张模式/多张模式]
[net_type] 0                             
[sparse_rate] 0                          # 0 - 不稀疏处理 0.5 - 0.5稀疏
[compile_mode] 1                         # 0 - 高速模式损失精度 1 - 高精度模式
[is_simulation] 0                        # 【0 - inst芯片模式,芯片中运行必须为0】, 1 - func 仿真模式用于pc端 
[log_level] 2
[instruction_name] ./data/yolov3_inst            # 【生成NNIE模型*.wk文件名】
[image_list] ./file_list.txt             # 用于存储数据的决定路径，30张左右的样例数据用于量化模型
[image_type] 1                           # 1 - 表示网络数据输入为 SVP_BLOB_TYPE_U8(普通的灰度图和RGB图)类型; 此时要求 image_list 配置是 RGB 图或者灰度图片的 list 文件;
[RGB_order] BGR 
[internal_stride] 16                     # align_byte 数据的维度必须是16的倍数，否则按16的倍数补全，默认为16，修改无效

[norm_type] 3                            # 0 - 不做预处理 5 - 减通道均值后再乘以 data_scale
[data_scale] 0.0039062                           # 0.00390625 = 1/ 256；1 就是不缩放
[mean_file] ./mean.txt                   # 均值文件，用于数据的标准化，每行代表一个通道要减去的值

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-s&#34;&gt;[prototxt_file] ./mark_prototxt/yolov3_mark_nnie_20191210173440.prototxt
[caffemodel_file] ./../data/detection/yolov3/model/yolov3.caffemodel
[batch_num] 1
[net_type] 0
[sparse_rate] 0
[compile_mode] 0
[is_simulation] 1          # func 功能仿真
[log_level] 2
[instruction_name] ./../data/detection/yolov3/inst/inst_yolov3_func
[RGB_order] BGR
[data_scale] 0.0039062
[internal_stride] 16
[image_list] ./../data/detection/yolov3/image_ref_list.txt
[image_type] 1
[mean_file] null
[norm_type] 3

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;大家知道，深度学习算法模型在推理前，都会对图像数据进行预处理，即RGB三个通道上的数据(0~255) normalize成0~1以内的值。值得注意的是，normalize方式有好几种，而且不同的算法模型所采用的预处理方式还不太一样。这就要求在nnie开发中，将caffe算法模型转换成wk文件时，必须选择合适的预处理方式。否则可能会导致识别结果不正常。&lt;/p&gt;

&lt;p&gt;norm_type:  normalize方式的解释&lt;/p&gt;

&lt;p&gt;在使用Ruyistudio进行模型文件转换时，支持下面5中方式，如下所示&lt;/p&gt;

&lt;p&gt;下面对这5种方式分别进行解释：&lt;/p&gt;

&lt;p&gt;1） mean file：&lt;/p&gt;

&lt;p&gt;如果选择这种norm type， 就要选择一个后缀名为binaryproto的文件。 它是caffe框架中常用的一种均值数据格式。常见的做法就是统计train lmdb里面图像数据的均值（每个像素的每个channel累加和再除以image size）。其对应预处理方式是将待识别的图像每个像素值减去binaryproto里面对应的像素值。&lt;/p&gt;

&lt;p&gt;2）channel mean value&lt;/p&gt;

&lt;p&gt;该方式对应的均值文件就是里面含3个channel均值的txt文件，如mobilenet ssd的就是 127.5 127.5 127.5。 其预处理过程就是将图像上每个像素的三个通道数据分别减去127.5。注意，经过该预处理后，图像数据并没有normalize成0~1范围内。&lt;/p&gt;

&lt;p&gt;3）data scale&lt;/p&gt;

&lt;p&gt;顾名思义就是直接对图像数据除以255缩小到0~1内。1/255=0.0039216，但其缺省的scale值为1/256=0.0039062。&lt;/p&gt;

&lt;p&gt;4）mean file with data scale&lt;/p&gt;

&lt;p&gt;上面提到，光减去mean值，并不能使得图像数据落入到绝对值0~1范围内，所以，一般地，需要在这个基础上再做一个scale。&lt;/p&gt;

&lt;p&gt;5）channel mean value with data scale&lt;/p&gt;

&lt;p&gt;同上，每个像素的各个通道减去mean value后还得再除以mean value。  还是以mobilenet ssd为例子，减去127.5后还得再乘以scale值0.007843（约等于1/127.5)。&lt;/p&gt;

&lt;p&gt;————————————————&lt;br /&gt;
版权声明：本文为CSDN博主「ltshan139」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。&lt;br /&gt;
原文链接：&lt;a href=&#34;https://blog.csdn.net/avideointerfaces/article/details/102417997&#34; target=&#34;_blank&#34;&gt;https://blog.csdn.net/avideointerfaces/article/details/102417997&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;本博文则是讲解如何在linux环境中来运行nnie mapper。在海思sdk里面已经提供了nnie mapper的可执行文件，其父目录为mapper，如下红框所示。&lt;/p&gt;

&lt;p&gt;在mapper里面有两个版本，分别为cpu和gpu版本。注意一下，for 3519av100， 其nnie mapper版本为1.2。&lt;/p&gt;

&lt;p&gt;另外一个需要注意的地方就是，nnie mapper的源代码不开放，只有可执行文件。而根据&lt;Hisvp开发指南&gt;，它们是在ubuntu14， protobuf3.5.1，opencv3.4.0以及gcc4.8.5上编译出来的。所以直接来运行nnie mapper可执行文件，肯定会遇到一些问题。我们这里只考虑cpu版本，因为gpu版本依赖库更多，更复杂。&lt;/p&gt;

&lt;p&gt;问题&lt;br /&gt;
下面就我遇到的问题进行逐一分析。&lt;/p&gt;

&lt;p&gt;1）找不到opencv相关库。 先运行命令： readelf -d nnie_mapper_12 看看该可执行文件都依赖什么opencv库。&lt;/p&gt;

&lt;p&gt;下载opencv3.4.x版本的代码，去掉opencv_world勾项，重新编译即可生成包括上面红框所示的库，然后将这三个库拷到/usr/lib/下面即可。&lt;/p&gt;

&lt;p&gt;2）第二个问题则比较麻烦。如上图所示，该可执行文件还需要libprotobuf.so，所以按照开发指南，先下载对应版本的protobuf源代码，然后进行编译。&lt;/p&gt;

&lt;p&gt;解压缩后，在其根目录下创建build子目录，然后进入build，输入下面命令：&lt;/p&gt;

&lt;p&gt;sudo cmake -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_BUILD_SHARED_LIBS=ON DCMAKE_INSTALL_PREFIX=/home/test/protobuflib/protobuf-3.5.1 ../cmake&lt;br /&gt;
最后输入命令进行编译：sudo make -j8。 编译完后，将build目录中生成的libprotobuf.so拷贝到/usr/lib下面。&lt;/p&gt;

&lt;p&gt;重点来了，这时遇到另外一个错误：&lt;/p&gt;

&lt;p&gt;通过nm -s libprotobuf.so | grep google8protobuf8internal26fixed_address_empty 可以发现实际上libprotobuf.so里面是有下面这个符号的：_ZN6google8protobuf8internal26fixed_address_empty_stringEB5cxx11，即多了字符串：B5cxx11。后来查明是和gcc版本有关系。需要使用gcc4.8.5来编译protobuf源代码重新生成libprotobuf.so才能解决这个问题。&lt;/p&gt;

&lt;p&gt;安装gcc-4.8.5，并切换gcc version到4.8.5的命令如下：&lt;/p&gt;

&lt;p&gt;sudo apt-get install gcc-4.8&lt;/p&gt;

&lt;p&gt;sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.8 100&lt;br /&gt;
这时输入命令： gcc --version 发现打印出来的gcc版本号变成4.8了。 但是重新编译protobuf代码，却发现问题依旧在。&lt;/p&gt;

&lt;p&gt;后来才知道， 因为protobuf里面有cpp文件，所以g++也要切换成4.8&lt;/p&gt;

&lt;p&gt;sudo apt-get install g++-4.8&lt;/p&gt;

&lt;p&gt;sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/g++-4.8 100&lt;br /&gt;
将生成的新libprotobuf.so拷贝到/usr/lib/，重新输入如下类似的命令，就能够运行成功。当然要保证参数xxxfunc.cfg里面的路径是正确的。&lt;/p&gt;

&lt;p&gt;nnie_mapper_12 ./data/classification/alexnet/alexnet_no_group_func.cfg&lt;br /&gt;
————————————————&lt;br /&gt;
版权声明：本文为CSDN博主「ltshan139」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。&lt;br /&gt;
原文链接：&lt;a href=&#34;https://blog.csdn.net/avideointerfaces/article/details/100178343&#34; target=&#34;_blank&#34;&gt;https://blog.csdn.net/avideointerfaces/article/details/100178343&lt;/a&gt;&lt;/p&gt;

&lt;p&gt; NNIE 板端运行、仿真，必须按上表的芯片型号和 mapper 版本匹配，否则可能出现&lt;br /&gt;
意想不到的错误。&lt;br /&gt;
 GPU 版本的 nnie_mapper 都需要安装 CUDA8.0。&lt;br /&gt;
 GPU 版本 mapper 和 CPU 版本的 mapper 编译出来的 wk 会有不同的地方，原因是&lt;br /&gt;
cuda 版本和 CPU 版本浮点的表示引起，caffe 的 CPU 和 GPU 版本也会有些许差&lt;br /&gt;
异。&lt;/p&gt;

&lt;h3 id=&#34;3-5-2-配置文件说明&#34;&gt;3.5.2 配置文件说明&lt;/h3&gt;

&lt;p&gt;nnie_mapper 配置选项说明如表 3-6 所示。&lt;br /&gt;
表3-6 nnie_mapper 配置选项说明&lt;br /&gt;
配置选项 取值范围 描述&lt;br /&gt;
prototxt_file - 网络描述文件，详细要求见 3.2 “Prototxt 要求”，描&lt;br /&gt;
述外支持情况与 caffe 相同。&lt;br /&gt;
net_type {0, 1, 2} 网络的类型。&lt;br /&gt;
0：CNN（不包含&lt;br /&gt;
LSTM/RNN/ROIPooling/PSROIPooling 的任意网&lt;br /&gt;
络）；&lt;br /&gt;
1：ROI/PSROI（包含 ROI Pooling 和 PSROI Pooling&lt;br /&gt;
的网络）；&lt;br /&gt;
2：Recurrent（包含 LSTM、RNN 的网络）；&lt;br /&gt;
caffemodel_file - 网络模型数据文件。&lt;br /&gt;
image_list - NNIE mapper 用于数据量化的参考图像 list 文件或&lt;br /&gt;
feature map 文件。该配置跟 image_type 相关。&lt;br /&gt;
NNIE mapper 量化时需要的图片是典型场景图片，建&lt;br /&gt;
议从网络模型的测试场景随机选择 20~50 张作为参考&lt;br /&gt;
图片进行量化，选择的图像要尽量覆盖模型的各个场&lt;br /&gt;
景（图像要包含分类或检测的目标，如分类网的目标&lt;br /&gt;
是苹果、梨、桃子，则参考图像至少要包含苹果、&lt;br /&gt;
梨、桃子。比如检测人、车的模型，参考图像中必须&lt;br /&gt;
由人、车，不能仅使用人或者无人无车的图像进行量&lt;br /&gt;
化）。图片影响量化系数，选择典型场景的图片计算&lt;br /&gt;
出来的量化系数对典型场景的量化误差越小。所以请&lt;br /&gt;
不要选择偏僻场景、过度曝光、纯黑、纯白的图片，&lt;br /&gt;
请选择识别率高，色彩均匀的典型场景图片。&lt;br /&gt;
网络中如果存在多个输入层，则需要配置多个&lt;br /&gt;
image_list 顶，顺序、个数与 prototxt 完全对应。&lt;br /&gt;
如果网络的数据输入是灰度或者 RGB 图像输入，即&lt;br /&gt;
image_type 配置不为 0，image_list 配置为所有参考图&lt;/p&gt;

&lt;p&gt;配置选项 取值范围 描述&lt;br /&gt;
片的 list，内容示意如下图图示，图片的格式支持以下&lt;br /&gt;
几种：&lt;br /&gt;
&amp;quot;.bmp&amp;quot;, &amp;quot;.dib&amp;quot;, &amp;quot;.jpeg&amp;quot;, &amp;quot;.jpg&amp;quot;, &amp;quot;.jpe&amp;quot;, &amp;quot;.jp2&amp;quot;, &amp;quot;.png&amp;quot;,&lt;br /&gt;
&amp;quot;.webp&amp;quot;, &amp;quot;.pbm&amp;quot;, &amp;quot;.pgm&amp;quot;, &amp;quot;.ppm&amp;quot;, &amp;quot;.sr&amp;quot;, &amp;quot;.ras&amp;quot;, &amp;quot;.tiff&amp;quot;,&lt;br /&gt;
&amp;quot;.tif&amp;quot;, &amp;quot;.BMP&amp;quot;, &amp;quot;.DIB&amp;quot;, &amp;quot;.JPEG&amp;quot;, &amp;quot;.JPG&amp;quot;, &amp;quot;.JPE&amp;quot;, &amp;quot;.JP2&amp;quot;,&lt;br /&gt;
&amp;quot;.PNG&amp;quot;, &amp;quot;.WEBP&amp;quot;, &amp;quot;.PBM&amp;quot;, &amp;quot;.PGM&amp;quot;, &amp;quot;.PPM&amp;quot;, &amp;quot;.SR&amp;quot;,&lt;br /&gt;
&amp;quot;.RAS&amp;quot;, &amp;quot;.TIFF&amp;quot;, &amp;quot;.TIF&amp;quot;&lt;br /&gt;
如果网络的输入是 feature map 或者 FC 向量输入，即&lt;br /&gt;
image_type 配置为 0，将 c*h*w 个点（即一个完整的&lt;br /&gt;
张量）以浮点文本的形式输出在一行内，点与点之间&lt;br /&gt;
以空格或逗号分隔。如果是多帧输入，则每一行输出&lt;br /&gt;
一个完整的张量。图示如下：&lt;br /&gt;
Recurrent 输入时，格式等同于 feature map 输入，每行&lt;br /&gt;
一个向量，一句话写成连续的多行，多句量化时需要&lt;br /&gt;
将每一句的帧数都补齐为最大帧数。&lt;br /&gt;
image_type {0,1,3,5} 表示网络实际执行时输入给网络的数据类型，该配置&lt;br /&gt;
跟 image_list 相关。&lt;br /&gt;
0：表示网络数据输入为 SVP_BLOB_TYPE_S32（参&lt;br /&gt;
考《HiSVP API 参考》）或者向量的类型（VEC_S32&lt;br /&gt;
和 SEQ_S32）；此时要求 image_list 配置为 feature&lt;br /&gt;
map 文件；&lt;br /&gt;
1：表示网络数据输入为 SVP_BLOB_TYPE_U8（普通&lt;br /&gt;
的灰度图和 RGB 图）类型； 此时要求 image_list 配&lt;br /&gt;
置是 RGB 图或者灰度图片的 list 文件；&lt;br /&gt;
3：网络数据输入为 SVP_BLOB_TYPE_YUV420SP 类&lt;br /&gt;
型；&lt;br /&gt;
5：网络数据输入为 SVP_BLOB_TYPE_YUV422SP 类&lt;br /&gt;
型；&lt;br /&gt;
当配置为 3 或者 5 时，image_list 配置为 RGB 图片的&lt;br /&gt;
list 文件。&lt;br /&gt;
norm_type {0, 1, 2, 3, 表示对网络数据输入的预处理方法。注意 image_type&lt;/p&gt;

&lt;p&gt;配置选项 取值范围 描述&lt;br /&gt;
4, 5} 配置为 0 时，norm_type 只能配置为 0；image_type 配&lt;br /&gt;
置为 3 或者 5 时，网络输入数据为 YUV 图像，但是&lt;br /&gt;
NNIE 硬件会根据 RGB_order 配置项自动转为 RGB 或&lt;br /&gt;
者 BGR 图像，此时 norm_type 配置方法跟 image_type&lt;br /&gt;
为 1 时一致。&lt;br /&gt;
0：不做任何预处理；&lt;br /&gt;
1：mean file，减图像均值；&lt;br /&gt;
2：channel mean_value，减通道均值；&lt;br /&gt;
3：data_scale，对图像像素值乘以 data_scale；&lt;br /&gt;
4：mean filewith data_scale，减图像均值后再乘以&lt;br /&gt;
data_scale；&lt;br /&gt;
5：channel mean_value with data_scale，减通道均值后&lt;br /&gt;
再乘以 data_scale。&lt;br /&gt;
data_scale (1/4096,&lt;br /&gt;
FLT_MA&lt;br /&gt;
X)&lt;br /&gt;
default:&lt;br /&gt;
0.0039062&lt;br /&gt;
5&lt;br /&gt;
数据预处理缩放比例，配置为浮点数，配合&lt;br /&gt;
norm_type 使用&lt;br /&gt;
本参数可省略，默认为 0.00390625=1/256。FLT_MAX&lt;br /&gt;
等于 3.402823466e+38。&lt;br /&gt;
mean_file - norm_type 为 1、4 时，表示均值文件&lt;br /&gt;
xxx.binaryproto；&lt;br /&gt;
norm_type 为 2、5 时，表示通道均值文件；&lt;br /&gt;
norm_type 为 0、3 时，用户也需要配置 mean_file&lt;br /&gt;
项，但具体内容可以是一个无效路径，比如 null；通&lt;br /&gt;
道均值文件 mean.txt 中每一行的浮点数表示对应的通&lt;br /&gt;
道均值，如单通道只有一个值。&lt;br /&gt;
batch_num [0, 256]&lt;br /&gt;
default:&lt;br /&gt;
256&lt;br /&gt;
0/1：single（单张）模式；&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;1：batch（多张）模式。&lt;br /&gt;
采用 single 模式 mapper 一个任务只能处理一张图片，&lt;br /&gt;
内部存储全部为一张图片分配，减少数据调度次数。&lt;br /&gt;
采用 batch 模式，在计算 FC 时 batch_num 张图片同时&lt;br /&gt;
计算，计算资源利用率高。&lt;br /&gt;
sparse_rate [0, 1]&lt;br /&gt;
default: 0&lt;br /&gt;
NNIE 引擎采用了参数压缩技术以减少带宽占用，为&lt;br /&gt;
了提高压缩率，可通对 FC 参数进稀疏处理。&lt;br /&gt;
用户通过 sparse_rate 数值指定多少比例的 FC 参数稀&lt;br /&gt;
疏为 0，例如配 0.5，则 FC 参数有 50%将被稀疏为&lt;br /&gt;
0，由于数据变的稀疏，压缩模块会获得更好的压缩&lt;br /&gt;
率。稀疏值越高，计算 FC 时所需参数带宽越低，但&lt;br /&gt;
精度会有所下降。&lt;br /&gt;
compile_mode {0, 1, 2} 0：Low-bandwidth(低带宽模式，默认)：通过量化算&lt;br /&gt;
法使参数与数据位宽最少，使系统所需带宽达到最&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;配置选项 取值范围 描述&lt;br /&gt;
default: 0 小，但会有精度损失；&lt;br /&gt;
1：High-precision(高精度模式): 结果精度最好，但是&lt;br /&gt;
性能会下降；；&lt;br /&gt;
2：User-specify(用户配置模式): 需要用户在 prototxt&lt;br /&gt;
中标明所有使用高精度计算的层，标注规则请见&lt;br /&gt;
prototxt_file 说明；&lt;br /&gt;
compress_mode {0, 1}&lt;br /&gt;
default: 0&lt;br /&gt;
配置压缩模式。&lt;br /&gt;
0：Normal 模式（包含 Normal、Ternary、Binary、&lt;br /&gt;
Sparse 四种压缩模式的自动切换）；&lt;br /&gt;
1：Bypass 模式，关闭压缩功能。&lt;br /&gt;
要求：&lt;br /&gt;
可不填，默认为 Normal 模式；用户提供的参数只有&lt;br /&gt;
三种值且正负对称时，nnie_mapper 会自动进入&lt;br /&gt;
Ternary 模式；用户提供的参数只有两种值且包含 0&lt;br /&gt;
时，nnie_mapper 会自动进入 Binary 模式；&lt;br /&gt;
max_roi&lt;em&gt;frame&lt;/em&gt;&lt;br /&gt;
cnt&lt;br /&gt;
[1, 5000]&lt;br /&gt;
default:&lt;br /&gt;
300&lt;br /&gt;
包含 ROI/PSROI 网络的 RPN 阶段输出的候选框最大&lt;br /&gt;
数目。&lt;br /&gt;
默认值：300。&lt;br /&gt;
roi_coordinate_f&lt;br /&gt;
ile&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mapper 在网络模型转化过程中输入给 ROI Pooling 或&lt;br /&gt;
PSROI Pooling 层的配置参数，用于指定 ROI 框的坐&lt;br /&gt;
标信息，每一行五个值，分别代表 batch_index(int)、&lt;br /&gt;
left_x（float）、top_y（float）、right_x（float）、&lt;br /&gt;
bottom_y（float），不同的框以换行符分隔。&lt;br /&gt;
框坐标是在 caffe 中使用输入给 mapper 的 image_list&lt;br /&gt;
的相同图片运行到 RPN 层的输出结果，如 Faster&lt;br /&gt;
RCNN 网络中 Proposal 层的 top 为 rois，在 caffe&lt;br /&gt;
forward 结束后，通过 np.savetxt(&#39;rois.txt&#39;,&lt;br /&gt;
net.blobs[&#39;rois&#39;].data[...], fmt=&amp;quot;%.6f&amp;quot;) 保存框坐标为文&lt;br /&gt;
件。需要保证两者图片输入顺序相同，同时要保证&lt;br /&gt;
caffe 运行时输入给网络的分辨率跟配置给 mapper 的&lt;br /&gt;
prototxt 中的分辨率相同。&lt;br /&gt;
For example:&lt;br /&gt;
0 734.01 147.02 806.03 294.04&lt;br /&gt;
0 723.05 157.06 818.07 306.08&lt;br /&gt;
1 749.09 170.10 817.11 310.12&lt;br /&gt;
1 678.13 220.14 855.15 374.16&lt;br /&gt;
如果一个网络中有多个 ROI Pooling 或 PSROI Pooling&lt;br /&gt;
层，则需要配置多行坐标文件，个数与 ROI Pooling&lt;br /&gt;
或 PSROI Pooling 层个数对应，配置的顺序也需要与&lt;br /&gt;
prototxt 内对应层顺序相同；&lt;br /&gt;
is_simulation {0, 1} 网络模型转化类型&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;配置选项 取值范围 描述&lt;br /&gt;
default: 0 0：Chip，芯片模式，网络模型转化成在芯片上加载的&lt;br /&gt;
wk 文件，指令仿真也使用此模式；&lt;br /&gt;
1：Simulation，仿真模式，网络模型转化成在 PC 端&lt;br /&gt;
仿真上加载的 wk 文件，功能仿真使用此模式；&lt;br /&gt;
instructions_na&lt;br /&gt;
me&lt;br /&gt;
string&lt;br /&gt;
length &amp;lt;&lt;br /&gt;
120&lt;br /&gt;
default:&lt;br /&gt;
inst&lt;br /&gt;
nnie_mapper 生成的知识库文件名称。&lt;br /&gt;
默认生成如下格式的知识库名：inst.wk；用户也可以&lt;br /&gt;
自行修改生成的知识库名字。&lt;br /&gt;
internal_stride {16, 32}&lt;br /&gt;
default: 16&lt;br /&gt;
用户根据 DDR 颗粒对应的最佳读写效率配置中间结&lt;br /&gt;
果的对齐方式。&lt;br /&gt;
要求：&lt;br /&gt;
DDR3 对应 16，DDR4 对应 32，可不填，默认为 16；&lt;br /&gt;
is_check_protot&lt;br /&gt;
xt&lt;br /&gt;
{0, 1}&lt;br /&gt;
default: 0&lt;br /&gt;
检查网络描述文件标志。&lt;br /&gt;
0：mapper 模式，对 prototxt、caffemodel 等进行转&lt;br /&gt;
化。&lt;br /&gt;
1：网络过滤器模式，对 prototxt 文件是否符合支持规&lt;br /&gt;
格进行检查。&lt;br /&gt;
log_level {0, 1, 2, 3}&lt;br /&gt;
default: 0&lt;br /&gt;
设置是否开启日志文件，以及配置打印的等级，本参&lt;br /&gt;
数可省略，当省略时，为不打印日志文件。&lt;br /&gt;
0：打印 main 函数流程，cfg 文件等信息；&lt;br /&gt;
1：打印 nnie_mapper 解析到的文件信息，包含&lt;br /&gt;
image_list、prototxt、内存分配过程；&lt;br /&gt;
2：打印中间表示信息；&lt;br /&gt;
3：打印详细信息，有大量文件输出，转化耗时较&lt;br /&gt;
长，请谨慎使用；&lt;br /&gt;
recurrent_tmax [1, 1024]&lt;br /&gt;
default:&lt;br /&gt;
1024&lt;br /&gt;
Recurrent 网络（包含 LSTM/RNN 层）每一句话的最&lt;br /&gt;
大桢数，支持[1, 1024]范围内的配置，减小配置值可&lt;br /&gt;
以减小临时缓存大小。&lt;br /&gt;
RGB_order {RGB,&lt;br /&gt;
BGR}&lt;br /&gt;
default:&lt;br /&gt;
BGR&lt;br /&gt;
image_type 设置为 0 时，该参数无效；&lt;br /&gt;
image_type 设置为 1 时，表示输入给网络的 RGB 图像&lt;br /&gt;
的 RGB 三通道的顺序；&lt;br /&gt;
image_type 设置为 3、5 时，表示 YUV 图像数据转成&lt;br /&gt;
RGB Planar 或者 BGR Planar 图像输入给网络。&lt;br /&gt;
本参数可省略&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>【YOLO】详解：YOLO-darknet训练自己的数据</title>
            <link>/ai/yolo/yolov3-train/</link>
            <pubDate>Tue, 10 Dec 2019 16:40:00 CST</pubDate>
            <author>rinetd</author>
            <guid>/ai/yolo/yolov3-train/</guid>
            <description>

&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/lilai619/article/details/79695109&#34; target=&#34;_blank&#34;&gt;目标检测：YOLOv3: 训练自己的数据 - 微风❤水墨&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/Jinlong_Xu/article/details/75577007&#34; target=&#34;_blank&#34;&gt;【YOLO】详解：YOLO-darknet训练自己的数据 - Jinlong_Xu的博客&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;准备训练数据&lt;br /&gt;
使用darknet训练自己的YOLO模型需要将数据转成darknet需要的格式，每张图片对应一个.txt的label文件，文件格式如下：&lt;/p&gt;

&lt;p&gt;&lt;object-class&gt; &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt;&lt;/p&gt;

&lt;p&gt;object-class是类的索引，后面的4个值都是相对于整张图片的比例。&lt;br /&gt;
x是ROI中心的x坐标，y是ROI中心的y坐标，width是ROI的宽，height是ROI的高。&lt;/p&gt;

&lt;p&gt;我需要用到Pascal VOC、MSCOCO、ImageNet和自己标记的一些图片。&lt;br /&gt;
混用这些数据集有一个严重的问题，有一些需要标记的物体没有被标记。&lt;br /&gt;
如ImageNet的200种物体中有iPod并做了标记，而MSCOCO中有一些图片中有iPod却没有标记出来，这会导致模型的精度下降。该问题可以通过对这部分图片重新标记来解决（工作量很大）；也可以修改损失函数，对不同数据集的image计算不同的损失，同时针对不同数据集中的数据使用不同的object_scale和noobject_scale。&lt;/p&gt;

&lt;p&gt;整合这些数据集首先要准备一个list，list中列出了要识别的物体。&lt;br /&gt;
如paul_list.txt&lt;/p&gt;

&lt;p&gt;0,ambulance&lt;br /&gt;
1,apple&lt;br /&gt;
2,automat&lt;br /&gt;
3,backpack&lt;br /&gt;
4,baggage&lt;br /&gt;
5,banana&lt;br /&gt;
6,baseball&lt;br /&gt;
7,basketball&lt;br /&gt;
8,bed&lt;br /&gt;
9,bench&lt;/p&gt;

&lt;p&gt;转换Pascal VOC&lt;br /&gt;
darknet作者提供了voc_label.py脚本来实现该功能，我们只需修改脚本中的classes为我们需要的classes即可，然后在VOCdevkit的父目录执行voc_label.py即可。&lt;/p&gt;

&lt;p&gt;classes = [&amp;quot;ambulance&amp;quot;, &amp;quot;apple&amp;quot;, &amp;quot;automat&amp;quot;, &amp;quot;backpack&amp;quot;, &amp;quot;baggage&amp;quot;, &amp;quot;banana&amp;quot;, &amp;quot;baseball&amp;quot;, &amp;quot;basketball&amp;quot;, &amp;quot;bed&amp;quot;,&amp;quot;bench&amp;quot;]&lt;/p&gt;

&lt;p&gt;转换MSCOCO&lt;br /&gt;
查看coco的80种物体有哪些是我们需要的，制作coco_list.txt，格式为,。如：&lt;/p&gt;

&lt;p&gt;1,apple&lt;br /&gt;
3,backpack&lt;br /&gt;
5,banana&lt;br /&gt;
8,bed&lt;br /&gt;
9,bench&lt;/p&gt;

&lt;p&gt;安装MSCOCO提供的Python API库，然后执行coco_label.py。&lt;br /&gt;
coco_label.py见github。&lt;br /&gt;
&lt;a href=&#34;https://github.com/PaulChongPeng/darknet/blob/master/tools/coco_label.py&#34; target=&#34;_blank&#34;&gt;https://github.com/PaulChongPeng/darknet/blob/master/tools/coco_label.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;执行脚本前需要修改dataDir和classes为自己的COCO数据集路径和coco_list.txt路径&lt;/p&gt;

&lt;h1 id=&#34;coding-utf-8&#34;&gt;coding=utf-8&lt;/h1&gt;

&lt;h1 id=&#34;使用说明&#34;&gt;使用说明&lt;/h1&gt;

&lt;h1 id=&#34;需要先安装coco-tools&#34;&gt;需要先安装coco tools&lt;/h1&gt;

&lt;h1 id=&#34;git-clone-https-github-com-pdollar-coco-git&#34;&gt;git clone &lt;a href=&#34;https://github.com/pdollar/coco.git&#34; target=&#34;_blank&#34;&gt;https://github.com/pdollar/coco.git&lt;/a&gt;&lt;/h1&gt;

&lt;h1 id=&#34;cd-coco-pythonapi&#34;&gt;cd coco/PythonAPI&lt;/h1&gt;

&lt;h1 id=&#34;make-install-可能会缺少相关依赖-根据提示安装依赖即可&#34;&gt;make install(可能会缺少相关依赖，根据提示安装依赖即可)&lt;/h1&gt;

&lt;h1 id=&#34;执行脚本前需在train2014和val2014目录下分别创建jpegimages和labels目录-并将原来train2014和val2014目录下的图片移到jpegimages下&#34;&gt;执行脚本前需在train2014和val2014目录下分别创建JPEGImages和labels目录，并将原来train2014和val2014目录下的图片移到JPEGImages下&lt;/h1&gt;

&lt;h1 id=&#34;coco数据集的filelist目录下会生成图片路径列表&#34;&gt;COCO数据集的filelist目录下会生成图片路径列表&lt;/h1&gt;

&lt;h1 id=&#34;coco数据集的子集的labels目录下会生成yolo需要的标注文件&#34;&gt;COCO数据集的子集的labels目录下会生成yolo需要的标注文件&lt;/h1&gt;

&lt;p&gt;from pycocotools.coco import COCO&lt;br /&gt;
import shutil&lt;br /&gt;
import os&lt;/p&gt;

&lt;h1 id=&#34;将roi的坐标转换为yolo需要的坐标&#34;&gt;将ROI的坐标转换为yolo需要的坐标&lt;/h1&gt;

&lt;h1 id=&#34;size是图片的w和h&#34;&gt;size是图片的w和h&lt;/h1&gt;

&lt;h1 id=&#34;box里保存的是roi的坐标-x-y的最大值和最小值&#34;&gt;box里保存的是ROI的坐标（x，y的最大值和最小值）&lt;/h1&gt;

&lt;h1 id=&#34;返回值为roi中心点相对于图片大小的比例坐标-和roi的w-h相对于图片大小的比例&#34;&gt;返回值为ROI中心点相对于图片大小的比例坐标，和ROI的w、h相对于图片大小的比例&lt;/h1&gt;

&lt;p&gt;def convert(size, box):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dw = 1. / size[0]
dh = 1. / size[1]
x = box[0] + box[2] / 2.0
y = box[1] + box[3] / 2.0
w = box[2]
h = box[3]
x = x * dw
w = w * dw
y = y * dh
h = h * dh
return (x, y, w, h)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;获取所需要的类名和id&#34;&gt;获取所需要的类名和id&lt;/h1&gt;

&lt;h1 id=&#34;path为类名和id的对应关系列表的地址-标注文件中可能有很多类-我们只加载该path指向文件中的类&#34;&gt;path为类名和id的对应关系列表的地址（标注文件中可能有很多类，我们只加载该path指向文件中的类）&lt;/h1&gt;

&lt;h1 id=&#34;返回值是一个字典-键名是类名-键值是id&#34;&gt;返回值是一个字典，键名是类名，键值是id&lt;/h1&gt;

&lt;p&gt;def get_classes_and_index(path):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;D = {}
f = open(path)
for line in f:
    temp = line.rstrip().split(&#39;,&#39;, 2)
    print(&amp;quot;temp[0]:&amp;quot; + temp[0] + &amp;quot;\n&amp;quot;)
    print(&amp;quot;temp[1]:&amp;quot; + temp[1] + &amp;quot;\n&amp;quot;)
    D[temp[1]] = temp[0]
return D
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;dataDir = &#39;/mnt/large4t/pengchong&lt;em&gt;data/Data/COCO&#39;  # COCO数据集所在的路径&lt;br /&gt;
dataType = &#39;train2014&#39;  # 要转换的COCO数据集的子集名&lt;br /&gt;
annFile = &#39;%s/annotations/instances&lt;/em&gt;%s.json&#39; % (dataDir, dataType)  # COCO数据集的标注文件路径&lt;br /&gt;
classes = get_classes_and_index(&#39;/mnt/large4t/pengchong_data/Tools/Yolo_paul/darknet/data/coco_list.txt&#39;)&lt;/p&gt;

&lt;h1 id=&#34;labels-目录若不存在-创建labels目录-若存在-则清空目录&#34;&gt;labels 目录若不存在，创建labels目录。若存在，则清空目录&lt;/h1&gt;

&lt;p&gt;if not os.path.exists(&#39;%s/%s/labels/&#39; % (dataDir, dataType)):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;os.makedirs(&#39;%s/%s/labels/&#39; % (dataDir, dataType))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;else:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shutil.rmtree(&#39;%s/%s/labels/&#39; % (dataDir, dataType))
os.makedirs(&#39;%s/%s/labels/&#39; % (dataDir, dataType))
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;filelist-目录若不存在-创建filelist目录&#34;&gt;filelist 目录若不存在，创建filelist目录。&lt;/h1&gt;

&lt;p&gt;if not os.path.exists(&#39;%s/filelist/&#39; % dataDir):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;os.makedirs(&#39;%s/filelist/&#39; % dataDir)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;coco = COCO(annFile)  # 加载解析标注文件&lt;br /&gt;
list_file = open(&#39;%s/filelist/%s.txt&#39; % (dataDir, dataType), &#39;w&#39;)  # 数据集的图片list保存路径&lt;/p&gt;

&lt;p&gt;imgIds = coco.getImgIds()  # 获取标注文件中所有图片的COCO Img ID&lt;br /&gt;
catIds = coco.getCatIds()  # 获取标注文件总所有的物体类别的COCO Cat ID&lt;/p&gt;

&lt;p&gt;for imgId in imgIds:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;objCount = 0  # 一个标志位，用来判断该img是否包含我们需要的标注
print(&#39;imgId :%s&#39; % imgId)
Img = coco.loadImgs(imgId)[0]  # 加载图片信息
print(&#39;Img :%s&#39; % Img)
filename = Img[&#39;file_name&#39;]  # 获取图片名
width = Img[&#39;width&#39;]  # 获取图片尺寸
height = Img[&#39;height&#39;]  # 获取图片尺寸
print(&#39;filename :%s, width :%s ,height :%s&#39; % (filename, width, height))
annIds = coco.getAnnIds(imgIds=imgId, catIds=catIds, iscrowd=None)  # 获取该图片对应的所有COCO物体类别标注ID
print(&#39;annIds :%s&#39; % annIds)
for annId in annIds:
    anns = coco.loadAnns(annId)[0]  # 加载标注信息
    catId = anns[&#39;category_id&#39;]  # 获取该标注对应的物体类别的COCO Cat ID
    cat = coco.loadCats(catId)[0][&#39;name&#39;]  # 获取该COCO Cat ID对应的物体种类名
    # print &#39;anns :%s&#39; % anns
    # print &#39;catId :%s , cat :%s&#39; % (catId,cat)

    # 如果该类名在我们需要的物体种类列表中，将标注文件转换为YOLO需要的格式
    if cat in classes:
        objCount = objCount + 1
        out_file = open(&#39;%s/%s/labels/%s.txt&#39; % (dataDir, dataType, filename[:-4]), &#39;a&#39;)
        cls_id = classes[cat]  # 获取该类物体在yolo训练中的id
        box = anns[&#39;bbox&#39;]
        size = [width, height]
        bb = convert(size, box)
        out_file.write(str(cls_id) + &amp;quot; &amp;quot; + &amp;quot; &amp;quot;.join([str(a) for a in bb]) + &#39;\n&#39;)
        out_file.close()

if objCount &amp;gt; 0:
    list_file.write(&#39;%s/%s/JPEGImages/%s\n&#39; % (dataDir, dataType, filename))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;list_file.close()&lt;/p&gt;

&lt;p&gt;转换ImageNet&lt;br /&gt;
我使用的是ILSVRC2016的数据，查看200种物体中有哪些是我们需要的，然后制作imagenet_list.txt。&lt;/p&gt;

&lt;p&gt;需要注意，ImageNet的标注文件中的object name使用的物体的WordNetID，所以imagenet_list.txt中需要使用WordNetID，如：&lt;/p&gt;

&lt;p&gt;1,n07739125&lt;br /&gt;
3,n02769748&lt;br /&gt;
5,n07753592&lt;br /&gt;
6,n02799071&lt;br /&gt;
7,n02802426&lt;br /&gt;
9,n02828884&lt;/p&gt;

&lt;p&gt;为了方便获取WordNetID在ImageNet中的物体名词（paul_list.txt中的名词未必和ImageNet中的一致），可以制作一个imagenet_map.txt，如：&lt;/p&gt;

&lt;p&gt;1,apple,n07739125&lt;br /&gt;
3,backpack,n02769748&lt;br /&gt;
5,banana,n07753592&lt;br /&gt;
6,baseball,n02799071&lt;br /&gt;
7,basketball,n02802426&lt;br /&gt;
9,bench,n02828884&lt;/p&gt;

&lt;p&gt;制作imagenet_list.txt和imagenet_map.txt需要知道WordNetID和名词间的映射关系，有两个办法。&lt;/p&gt;

&lt;p&gt;离线版：&lt;/p&gt;

&lt;p&gt;从ImageNet下载words.txt（WordNetID和名词间的映射）和gloss.txt（WordNetID对应的名词的定义）,然后查询。如果没有梯子，国内访问ImageNet龟速，文件被我备份在GitHub。&lt;br /&gt;
&lt;a href=&#34;https://github.com/PaulChongPeng/darknet/blob/32dddd8509de4bf57cad0aa330160d57d33d0c66/data/words.txt&#34; target=&#34;_blank&#34;&gt;https://github.com/PaulChongPeng/darknet/blob/32dddd8509de4bf57cad0aa330160d57d33d0c66/data/words.txt&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://github.com/PaulChongPeng/darknet/blob/32dddd8509de4bf57cad0aa330160d57d33d0c66/data/gloss.txt&#34; target=&#34;_blank&#34;&gt;https://github.com/PaulChongPeng/darknet/blob/32dddd8509de4bf57cad0aa330160d57d33d0c66/data/gloss.txt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在线版：&lt;/p&gt;

&lt;p&gt;访问 &lt;a href=&#34;http://image-net.org/challenges/LSVRC/2015/browse-det-synsets&#34; target=&#34;_blank&#34;&gt;http://image-net.org/challenges/LSVRC/2015/browse-det-synsets&lt;/a&gt; 。请自备梯子，不然慢的令人发指。&lt;/p&gt;

&lt;p&gt;点击需要查询的名词，如Volleyball，会跳转到对应的网页，我们需要的是网页地址后的wnid。如 &lt;a href=&#34;http://imagenet.stanford.edu/synset?wnid=n04540053&#34; target=&#34;_blank&#34;&gt;http://imagenet.stanford.edu/synset?wnid=n04540053&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;制作好list后，将imagenet_to_yolo.py放在ILSVRC2016/bject_detection/ILSVRC目录下，并将Data文件夹重命名为JPEGImages（因为darknet找图片对应的标记文件是直接替换JPEGImages为labels，图片后缀名替换为txt）。修改classes为自己的list路径后直接运行脚本即可。&lt;/p&gt;

&lt;p&gt;imagenet_to_yolo.py 我放在了GitHub上：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/PaulChongPeng/darknet/blob/master/tools/imagenet_to_yolo.py&#34; target=&#34;_blank&#34;&gt;https://github.com/PaulChongPeng/darknet/blob/master/tools/imagenet_to_yolo.py&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;coding-utf-8-1&#34;&gt;coding=utf-8&lt;/h1&gt;

&lt;h1 id=&#34;使用说明-1&#34;&gt;使用说明&lt;/h1&gt;

&lt;h1 id=&#34;将该文件放在ilsvrc2016-bject-detection-ilsvrc目录下-并将data文件夹重命名为jpegimages&#34;&gt;将该文件放在ILSVRC2016/bject_detection/ILSVRC目录下，并将Data文件夹重命名为JPEGImages&lt;/h1&gt;

&lt;h1 id=&#34;执行该工具-lists目录下会生成图片路径列表&#34;&gt;执行该工具，Lists目录下会生成图片路径列表&lt;/h1&gt;

&lt;h1 id=&#34;labels目录下会生成yolo需要的标注文件&#34;&gt;labels目录下会生成yolo需要的标注文件&lt;/h1&gt;

&lt;p&gt;import xml.etree.ElementTree as ET&lt;br /&gt;
import pickle&lt;br /&gt;
import os&lt;br /&gt;
from os import listdir, getcwd&lt;br /&gt;
from os.path import join&lt;br /&gt;
import shutil&lt;/p&gt;

&lt;h1 id=&#34;获取所有包含标注文件的的目录路径&#34;&gt;获取所有包含标注文件的的目录路径&lt;/h1&gt;

&lt;p&gt;def get_dirs():&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dirs = [&#39;DET/train/ILSVRC2014_train_0006&#39;, &#39;DET/train/ILSVRC2014_train_0005&#39;, &#39;DET/train/ILSVRC2014_train_0004&#39;,
        &#39;DET/train/ILSVRC2014_train_0003&#39;, &#39;DET/train/ILSVRC2014_train_0002&#39;, &#39;DET/train/ILSVRC2014_train_0001&#39;,
        &#39;DET/train/ILSVRC2014_train_0000&#39;, &#39;DET/val&#39;]
dirs_2013 = os.listdir(&#39;JPEGImages/DET/train/ILSVRC2013_train/&#39;)
for dir_2013 in dirs_2013:
    dirs.append(&#39;DET/train/ILSVRC2013_train/&#39; + dir_2013)
return dirs
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;获取所需要的类名和id-1&#34;&gt;获取所需要的类名和id&lt;/h1&gt;

&lt;h1 id=&#34;path为类名和id的对应关系列表的地址-标注文件中可能有很多类-我们只加载该path指向文件中的类-1&#34;&gt;path为类名和id的对应关系列表的地址（标注文件中可能有很多类，我们只加载该path指向文件中的类）&lt;/h1&gt;

&lt;h1 id=&#34;返回值是一个字典-键名是类名-键值是id-1&#34;&gt;返回值是一个字典，键名是类名，键值是id&lt;/h1&gt;

&lt;p&gt;def get_classes_and_index(path):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;D = {}
f = open(path)
for line in f:
    temp = line.rstrip().split(&#39;,&#39;, 2)
    D[temp[1]] = temp[0]
return D
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;将roi的坐标转换为yolo需要的坐标-1&#34;&gt;将ROI的坐标转换为yolo需要的坐标&lt;/h1&gt;

&lt;h1 id=&#34;size是图片的w和h-1&#34;&gt;size是图片的w和h&lt;/h1&gt;

&lt;h1 id=&#34;box里保存的是roi的坐标-x-y的最大值和最小值-1&#34;&gt;box里保存的是ROI的坐标（x，y的最大值和最小值）&lt;/h1&gt;

&lt;h1 id=&#34;返回值为roi中心点相对于图片大小的比例坐标-和roi的w-h相对于图片大小的比例-1&#34;&gt;返回值为ROI中心点相对于图片大小的比例坐标，和ROI的w、h相对于图片大小的比例&lt;/h1&gt;

&lt;p&gt;def convert(size, box):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dw = 1. / size[0]
dh = 1. / size[1]
x = (box[0] + box[1]) / 2.0
y = (box[2] + box[3]) / 2.0
w = box[1] - box[0]
h = box[3] - box[2]
x = x * dw
w = w * dw
y = y * dh
h = h * dh
return (x, y, w, h)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;将labelimg-生成的xml文件转换为yolo需要的txt文件&#34;&gt;将labelImg 生成的xml文件转换为yolo需要的txt文件&lt;/h1&gt;

&lt;h1 id=&#34;image-dir-图片所在的目录的路径&#34;&gt;image_dir 图片所在的目录的路径&lt;/h1&gt;

&lt;h1 id=&#34;image-id图片名&#34;&gt;image_id图片名&lt;/h1&gt;

&lt;p&gt;def convert_annotation(image_dir, image_id):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;in_file = open(&#39;Annotations/%s/%s.xml&#39; % (image_dir, image_id))
obj_num = 0  # 一个标志位，用来判断该img是否包含我们需要的标注
tree = ET.parse(in_file)
root = tree.getroot()
size = root.find(&#39;size&#39;)
w = int(size.find(&#39;width&#39;).text)
h = int(size.find(&#39;height&#39;).text)

for obj in root.iter(&#39;object&#39;):
    cls = obj.find(&#39;name&#39;).text
    if cls not in classes:
        continue
    obj_num = obj_num + 1
    if obj_num == 1:
        out_file = open(&#39;labels/%s/%s.txt&#39; % (image_dir, image_id), &#39;w&#39;)
    cls_id = classes[cls]  # 获取该类物体在yolo训练中的id
    xmlbox = obj.find(&#39;bndbox&#39;)
    b = (float(xmlbox.find(&#39;xmin&#39;).text), float(xmlbox.find(&#39;xmax&#39;).text), float(xmlbox.find(&#39;ymin&#39;).text),
         float(xmlbox.find(&#39;ymax&#39;).text))
    bb = convert((w, h), b)
    out_file.write(str(cls_id) + &amp;quot; &amp;quot; + &amp;quot; &amp;quot;.join([str(a) for a in bb]) + &#39;\n&#39;)

if obj_num &amp;gt; 0:
    list_file = open(&#39;Lists/%s.txt&#39; % image_dir.split(&#39;/&#39;)[-1], &#39;a&#39;)  # 数据集的图片list保存路径
    list_file.write(&#39;%s/JPEGImages/%s/%s.JPEG\n&#39; % (wd, image_dir, image_id))
    list_file.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;def IsSubString(SubStrList, Str):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flag = True
for substr in SubStrList:
    if not (substr in Str):
        flag = False

return flag
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;获取findpath路径下指定格式-flagstr-的文件名-不包含后缀名-列表&#34;&gt;获取FindPath路径下指定格式（FlagStr）的文件名（不包含后缀名）列表&lt;/h1&gt;

&lt;p&gt;def GetFileList(FindPath, FlagStr=[]):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os
FileList = []
FileNames = os.listdir(FindPath)
if (len(FileNames) &amp;gt; 0):
    for fn in FileNames:
        if (len(FlagStr) &amp;gt; 0):
            if (IsSubString(FlagStr, fn)):
                FileList.append(fn[:-4])
        else:
            FileList.append(fn)

if (len(FileList) &amp;gt; 0):
    FileList.sort()

return FileList
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;classes = get_classes_and_index(&#39;/mnt/large4t/pengchong_data/Tools/Yolo_paul/darknet/data/imagenet_list.txt&#39;)&lt;br /&gt;
dirs = get_dirs()&lt;/p&gt;

&lt;p&gt;wd = getcwd()&lt;/p&gt;

&lt;h1 id=&#34;lists-目录若不存在-创建lists目录-若存在-则清空目录&#34;&gt;Lists 目录若不存在，创建Lists目录。若存在，则清空目录&lt;/h1&gt;

&lt;p&gt;if not os.path.exists(&#39;Lists/&#39;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;os.makedirs(&#39;Lists/&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;else:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shutil.rmtree(&#39;Lists/&#39;)
os.makedirs(&#39;Lists/&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;for image_dir in dirs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if not os.path.exists(&#39;JPEGImages/&#39; + image_dir):
    print(&amp;quot;JPEGImages/%s dir not exist&amp;quot; % image_dir)
    continue
# labels 目录若不存在，创建labels目录。若存在，则清空目录
if not os.path.exists(&#39;labels/%s&#39; % (image_dir)):
    os.makedirs(&#39;labels/%s&#39; % (image_dir))
else:
    shutil.rmtree(&#39;labels/%s&#39; % (image_dir))
    os.makedirs(&#39;labels/%s&#39; % (image_dir))
image_ids = GetFileList(&#39;Annotations/&#39; + image_dir, [&#39;xml&#39;])
for image_id in image_ids:
    print(image_id)
    convert_annotation(image_dir, image_id)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;转换自己的数据&lt;br /&gt;
我使用的labelImg工具做的图像标注，标记格式大体和VOC一致。&lt;br /&gt;
工具地址见GitHub： &lt;a href=&#34;https://github.com/tzutalin/labelImg&#34; target=&#34;_blank&#34;&gt;https://github.com/tzutalin/labelImg&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;只需要简单修改voc_label.py就可以转换自己的数据。修改后的脚本命名为lableImg_voc_to_yolo.py。我放在了GitHub上：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/PaulChongPeng/darknet/blob/master/tools/lableImg_voc_to_yolo.py&#34; target=&#34;_blank&#34;&gt;https://github.com/PaulChongPeng/darknet/blob/master/tools/lableImg_voc_to_yolo.py&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;coding-utf-8-2&#34;&gt;coding=utf-8&lt;/h1&gt;

&lt;h1 id=&#34;使用说明-2&#34;&gt;使用说明&lt;/h1&gt;

&lt;h1 id=&#34;要转换的数据集目录结构为&#34;&gt;要转换的数据集目录结构为：&lt;/h1&gt;

&lt;h1 id=&#34;paul-time-class-annotations-xml文件&#34;&gt;Paul/time/class/annotations/xml文件&lt;/h1&gt;

&lt;h1 id=&#34;paul-time-class-images-jpg文件&#34;&gt;Paul/time/class/images/jpg文件&lt;/h1&gt;

&lt;h1 id=&#34;paul-time-class-labels-即将生成的yolo需要的txt文件&#34;&gt;Paul/time/class/labels/即将生成的yolo需要的txt文件&lt;/h1&gt;

&lt;h1 id=&#34;该文件需放在paul目录下-该目录下将会生成名为-日期-的txt文件-文件内容为日期文件夹下所有图片的路径&#34;&gt;该文件需放在Paul目录下，该目录下将会生成名为“日期”的txt文件，文件内容为日期文件夹下所有图片的路径&lt;/h1&gt;

&lt;h1 id=&#34;有多少个日期的文件夹-就将多少个文件夹的名字加入sets&#34;&gt;有多少个日期的文件夹，就将多少个文件夹的名字加入sets&lt;/h1&gt;

&lt;h1 id=&#34;需要生成多少种物体的标签-就将多少种物体加入classes&#34;&gt;需要生成多少种物体的标签，就将多少种物体加入classes&lt;/h1&gt;

&lt;h1 id=&#34;labels目录下生成的txt文件中的第一个数字就是物体种类在classes中的索引&#34;&gt;labels目录下生成的txt文件中的第一个数字就是物体种类在classes中的索引&lt;/h1&gt;

&lt;p&gt;import xml.etree.ElementTree as ET&lt;br /&gt;
import pickle&lt;br /&gt;
import os&lt;br /&gt;
from os import listdir, getcwd&lt;br /&gt;
from os.path import join&lt;br /&gt;
import shutil&lt;/p&gt;

&lt;p&gt;sets = [&#39;20170401&#39;, &#39;20170414&#39;]&lt;/p&gt;

&lt;h1 id=&#34;获取所需要的类名和id-2&#34;&gt;获取所需要的类名和id&lt;/h1&gt;

&lt;h1 id=&#34;path为类名和id的对应关系列表的地址-标注文件中可能有很多类-我们只加载该path指向文件中的类-2&#34;&gt;path为类名和id的对应关系列表的地址（标注文件中可能有很多类，我们只加载该path指向文件中的类）&lt;/h1&gt;

&lt;h1 id=&#34;返回值是一个字典-键名是类名-键值是id-2&#34;&gt;返回值是一个字典，键名是类名，键值是id&lt;/h1&gt;

&lt;p&gt;def get_classes_and_index(path):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;D = {}
f = open(path)
for line in f:
    temp = line.rstrip().split(&#39;,&#39;, 2)
    print(&amp;quot;temp[0]:&amp;quot; + temp[0] + &amp;quot;\n&amp;quot;)
    print(&amp;quot;temp[1]:&amp;quot; + temp[1] + &amp;quot;\n&amp;quot;)
    D[temp[1].replace(&#39; &#39;, &#39;&#39;)] = temp[0]
return D
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;将roi的坐标转换为yolo需要的坐标-2&#34;&gt;将ROI的坐标转换为yolo需要的坐标&lt;/h1&gt;

&lt;h1 id=&#34;size是图片的w和h-2&#34;&gt;size是图片的w和h&lt;/h1&gt;

&lt;h1 id=&#34;box里保存的是roi的坐标-x-y的最大值和最小值-2&#34;&gt;box里保存的是ROI的坐标（x，y的最大值和最小值）&lt;/h1&gt;

&lt;h1 id=&#34;返回值为roi中心点相对于图片大小的比例坐标-和roi的w-h相对于图片大小的比例-2&#34;&gt;返回值为ROI中心点相对于图片大小的比例坐标，和ROI的w、h相对于图片大小的比例&lt;/h1&gt;

&lt;p&gt;def convert(size, box):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dw = 1. / size[0]
dh = 1. / size[1]
x = (box[0] + box[1]) / 2.0
y = (box[2] + box[3]) / 2.0
w = box[1] - box[0]
h = box[3] - box[2]
x = x * dw
w = w * dw
y = y * dh
h = h * dh
return (x, y, w, h)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;将labelimg-生成的xml文件转换为yolo需要的txt文件-1&#34;&gt;将labelImg 生成的xml文件转换为yolo需要的txt文件&lt;/h1&gt;

&lt;h1 id=&#34;path到类名一级的目录路径&#34;&gt;path到类名一级的目录路径&lt;/h1&gt;

&lt;h1 id=&#34;image-id图片名-1&#34;&gt;image_id图片名&lt;/h1&gt;

&lt;p&gt;def convert_annotation(path, image_id):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;in_file = open(&#39;%s/annotations/%s.xml&#39; % (path, image_id))
out_file = open(&#39;%s/labels/%s.txt&#39; % (path, image_id), &#39;w&#39;)
tree = ET.parse(in_file)
root = tree.getroot()
size = root.find(&#39;size&#39;)
w = int(size.find(&#39;width&#39;).text)
h = int(size.find(&#39;height&#39;).text)

for obj in root.iter(&#39;object&#39;):
    cls = obj.find(&#39;name&#39;).text.replace(&#39; &#39;, &#39;&#39;)
    # 如果该类物体不在我们的yolo训练列表中，跳过
    if cls not in classes:
        continue
    cls_id = classes[cls]  # 获取该类物体在yolo训练列表中的id
    xmlbox = obj.find(&#39;bndbox&#39;)
    b = (float(xmlbox.find(&#39;xmin&#39;).text), float(xmlbox.find(&#39;xmax&#39;).text), float(xmlbox.find(&#39;ymin&#39;).text),
         float(xmlbox.find(&#39;ymax&#39;).text))
    bb = convert((w, h), b)
    out_file.write(str(cls_id) + &amp;quot; &amp;quot; + &amp;quot; &amp;quot;.join([str(a) for a in bb]) + &#39;\n&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;def IsSubString(SubStrList, Str):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flag = True
for substr in SubStrList:
    if not (substr in Str):
        flag = False

return flag
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;获取findpath路径下指定格式-flagstr-的文件名-不包含后缀名-列表-1&#34;&gt;获取FindPath路径下指定格式（FlagStr）的文件名（不包含后缀名）列表&lt;/h1&gt;

&lt;p&gt;def GetFileList(FindPath, FlagStr=[]):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os
FileList = []
FileNames = os.listdir(FindPath)
if (len(FileNames) &amp;gt; 0):
    for fn in FileNames:
        if (len(FlagStr) &amp;gt; 0):
            if (IsSubString(FlagStr, fn)):
                FileList.append(fn[:-4])
        else:
            FileList.append(fn)

if (len(FileList) &amp;gt; 0):
    FileList.sort()

return FileList
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;获取目录下子目录的目录名列表&#34;&gt;获取目录下子目录的目录名列表&lt;/h1&gt;

&lt;p&gt;def get_dirs(time):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dirs = []
dirs_temp = os.listdir(time)
for dir_name in dirs_temp:
    dirs.append(time + &#39;/&#39; + dir_name)
return dirs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;wd = getcwd()&lt;/p&gt;

&lt;p&gt;classes = get_classes_and_index(&#39;/raid/pengchong_data/Tools/Paul_YOLO/data/Paul_list.txt&#39;)&lt;/p&gt;

&lt;p&gt;for time in sets:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dirs = get_dirs(time)
list_file = open(&#39;%s.txt&#39; % time, &#39;w&#39;)  # 数据集的图片list保存路径
for path in dirs:
    print(path)
    if not os.path.exists(&#39;%s/annotations/&#39; % path):
        os.makedirs(&#39;%s/annotations/&#39; % path)
    if not os.path.exists(&#39;%s/labels/&#39; % path):
        os.makedirs(&#39;%s/labels/&#39; % path)
    else:
        shutil.rmtree(&#39;%s/labels/&#39; % path)
        os.makedirs(&#39;%s/labels/&#39; % path)
    image_ids = GetFileList(path + &#39;/annotations/&#39;, [&#39;xml&#39;])
    for image_id in image_ids:
        print(image_id)
        list_file.write(&#39;%s/%s/images/%s.jpg\n&#39; % (wd, path, image_id))
        convert_annotation(path, image_id)
list_file.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将各个数据集的标注文件转换成YOLO需要的格式后，将脚本生成的图像地址list的内容全部拷贝到paul.txt中，然后使用partial.py脚本随机分割为train,val,test data。脚本已上传至GitHut，可根据自己的需要进行修改。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/PaulChongPeng/darknet/blob/master/tools/partial.py&#34; target=&#34;_blank&#34;&gt;https://github.com/PaulChongPeng/darknet/blob/master/tools/partial.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;数据准备工作到此就算结束了。&lt;/p&gt;

&lt;p&gt;准备配置文件&lt;br /&gt;
在cfg目录下添加paul.data,内容如下：&lt;/p&gt;

&lt;p&gt;classes=10                                                      要识别物体的种类数&lt;br /&gt;
train  = data/paul_train.txt                                    训练集图片list&lt;br /&gt;
valid = data/paul_val.txt                                       验证集图片list&lt;br /&gt;
names = data/paul.names                                         要识别的物体list&lt;br /&gt;
backup = /mnt/large4t/pengchong_data/Tools/darknet/backup/      训练时权重文件备份路径&lt;/p&gt;

&lt;p&gt;在cfg目录下添加yolo-paul.cfg文件，该文件内容复制自默认的yolo-voc.cfg，根据自己的训练集和机器配置做修改，具体参数意义可以参考我之前的文章：&lt;/p&gt;

&lt;p&gt;我修改的内容如下：&lt;/p&gt;

&lt;p&gt;[net]&lt;br /&gt;
batch=27                       每27张图更新一次权重，subdivisions=1时占用GPU memory 15.6G左右&lt;br /&gt;
......&lt;br /&gt;
......&lt;br /&gt;
learning_rate=0.00001           学习率大了容易发散&lt;br /&gt;
max_batches = 500000&lt;br /&gt;
......&lt;br /&gt;
......&lt;br /&gt;
[convolutional]&lt;br /&gt;
......&lt;br /&gt;
......&lt;br /&gt;
filters=75                      最后一个卷积层输出的特征图数为5*(10+5)&lt;br /&gt;
......&lt;br /&gt;
......&lt;br /&gt;
[region]&lt;br /&gt;
......&lt;br /&gt;
......&lt;br /&gt;
classes=10                      训练十种物体&lt;br /&gt;
......&lt;br /&gt;
......&lt;/p&gt;

&lt;p&gt;在data目录下增加paul.names，内容如下：&lt;/p&gt;

&lt;p&gt;ambulance&lt;br /&gt;
apple&lt;br /&gt;
automat&lt;br /&gt;
backpack&lt;br /&gt;
baggage&lt;br /&gt;
banana&lt;br /&gt;
baseball&lt;br /&gt;
basketball&lt;br /&gt;
bed&lt;br /&gt;
bench&lt;/p&gt;

&lt;p&gt;修改Makefile&lt;/p&gt;

&lt;p&gt;GPU=1&lt;br /&gt;
CUDNN=1&lt;/p&gt;

&lt;p&gt;编译&lt;/p&gt;

&lt;p&gt;make clean&lt;br /&gt;
make -j8&lt;/p&gt;

&lt;p&gt;训练&lt;br /&gt;
首先准备ImageNet的预训练权重文件&lt;/p&gt;

&lt;p&gt;curl -O &lt;a href=&#34;https://pjreddie.com/media/files/darknet19.weights&#34; target=&#34;_blank&#34;&gt;https://pjreddie.com/media/files/darknet19.weights&lt;/a&gt;&lt;br /&gt;
使用前23层的权重&lt;/p&gt;

&lt;p&gt;./darknet partial cfg/darknet19_448.cfg darknet19_448.weights darknet19_448.conv.23 23&lt;br /&gt;
partial命令可以分割权重文件，fine-tune的时候也会用到。&lt;/p&gt;

&lt;p&gt;开始训练&lt;/p&gt;

&lt;p&gt;./darknet detector train cfg/paul.data cfg/yolo-paul.cfg darknet19_448.conv.23 2&amp;gt;1 | tee paul_train_log.txt&lt;br /&gt;
剩下的就是等待了。&lt;br /&gt;
需要注意的是，如果学习率设置的比较大，训练结果很容易发散，训练过程输出的log会有nan字样，需要减小学习率后再进行训练。&lt;/p&gt;

&lt;p&gt;多GPU训练技巧&lt;br /&gt;
darknet支持多GPU，使用多GPU训练可以极大加速训练速度。据我测试在DGX-1上使用8块Tesla P100同时训练的速度是在外星人上使用1块GTX1080的130多倍。&lt;/p&gt;

&lt;p&gt;单GPU与多GPU的切换技巧&lt;br /&gt;
在darknet上使用多GPU训练需要一定技巧，盲目使用多GPU训练会悲剧的发现损失一直在下降、recall在上升，然而Obj几乎为零,最终得到的权重文件无法预测出bounding box。&lt;/p&gt;

&lt;p&gt;使用多GPU训练前需要先用单GPU训练至Obj有稳定上升的趋势后（我一般在obj大于0.1后切换）再使用backup中备份的weights通过多GPU继续训练。一般情况下使用单GPU训练1000个迭代即可切换到多GPU。&lt;/p&gt;

&lt;p&gt;./darknet detector train cfg/paul.data cfg/yolo-paul.cfg backup/yolo-paul_1000.weights -gpus 0,1,2,3,4,5,6,7 2&amp;gt;1 | tee paul_train_log.txt&lt;br /&gt;
0,1,2,3,4,5,6,7是指定的GPU的ID，通过&lt;/p&gt;

&lt;p&gt;nvidia-smi&lt;br /&gt;
命令可以查询:&lt;/p&gt;

&lt;p&gt;+-----------------------------------------------------------------------------+&lt;br /&gt;
| NVIDIA-SMI 375.20                 Driver Version: 375.20                    |&lt;br /&gt;
|-------------------------------+----------------------+----------------------+&lt;br /&gt;
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |&lt;br /&gt;
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |&lt;br /&gt;
|===============================+======================+======================|&lt;br /&gt;
|   0  Tesla P100-SXM2...  On   | 0000:06:00.0     Off |                    0 |&lt;br /&gt;
| N/A   52C    P0   270W / 300W |  15887MiB / 16308MiB |     99%      Default |&lt;br /&gt;
+-------------------------------+----------------------+----------------------+&lt;br /&gt;
|   1  Tesla P100-SXM2...  On   | 0000:07:00.0     Off |                    0 |&lt;br /&gt;
| N/A   55C    P0   247W / 300W |  15887MiB / 16308MiB |     97%      Default |&lt;br /&gt;
+-------------------------------+----------------------+----------------------+&lt;br /&gt;
|   2  Tesla P100-SXM2...  On   | 0000:0A:00.0     Off |                    0 |&lt;br /&gt;
| N/A   54C    P0   252W / 300W |  15887MiB / 16308MiB |     98%      Default |&lt;br /&gt;
+-------------------------------+----------------------+----------------------+&lt;br /&gt;
|   3  Tesla P100-SXM2...  On   | 0000:0B:00.0     Off |                    0 |&lt;br /&gt;
| N/A   51C    P0   242W / 300W |  15887MiB / 16308MiB |     97%      Default |&lt;br /&gt;
+-------------------------------+----------------------+----------------------+&lt;br /&gt;
|   4  Tesla P100-SXM2...  On   | 0000:85:00.0     Off |                    0 |&lt;br /&gt;
| N/A   53C    P0   227W / 300W |  15887MiB / 16308MiB |     98%      Default |&lt;br /&gt;
+-------------------------------+----------------------+----------------------+&lt;br /&gt;
|   5  Tesla P100-SXM2...  On   | 0000:86:00.0     Off |                    0 |&lt;br /&gt;
| N/A   58C    P0   245W / 300W |  15887MiB / 16308MiB |     97%      Default |&lt;br /&gt;
+-------------------------------+----------------------+----------------------+&lt;br /&gt;
|   6  Tesla P100-SXM2...  On   | 0000:89:00.0     Off |                    0 |&lt;br /&gt;
| N/A   59C    P0   245W / 300W |  15887MiB / 16308MiB |     97%      Default |&lt;br /&gt;
+-------------------------------+----------------------+----------------------+&lt;br /&gt;
|   7  Tesla P100-SXM2...  On   | 0000:8A:00.0     Off |                    0 |&lt;br /&gt;
| N/A   52C    P0   228W / 300W |  15887MiB / 16308MiB |     97%      Default |&lt;br /&gt;
+-------------------------------+----------------------+----------------------+&lt;br /&gt;
+-----------------------------------------------------------------------------+&lt;br /&gt;
| Processes:                                                       GPU Memory |&lt;br /&gt;
|  GPU       PID  Type  Process name                               Usage      |&lt;br /&gt;
|=============================================================================|&lt;br /&gt;
|    0     50064    C   ./darknet                                    15887MiB |&lt;br /&gt;
|    1     50064    C   ./darknet                                    15887MiB |&lt;br /&gt;
|    2     50064    C   ./darknet                                    15887MiB |&lt;br /&gt;
|    3     50064    C   ./darknet                                    15887MiB |&lt;br /&gt;
|    4     50064    C   ./darknet                                    15887MiB |&lt;br /&gt;
|    5     50064    C   ./darknet                                    15887MiB |&lt;br /&gt;
|    6     50064    C   ./darknet                                    15887MiB |&lt;br /&gt;
|    7     50064    C   ./darknet                                    15887MiB |&lt;br /&gt;
+-----------------------------------------------------------------------------+&lt;br /&gt;
使用多GPU时的学习率&lt;br /&gt;
使用多GPU训练时，学习率是使用单GPU训练的n倍，n是使用GPU的个数&lt;/p&gt;

&lt;p&gt;可视化训练过程的中间参数&lt;br /&gt;
等待训练结束后（有时候没等结束我们的模型就开始发散了），我们需要检查各项指标（如loss）是否达到了我们期望的数值，如果没有，要分析为什么。可视化训练过程的中间参数可以帮助我们分析问题。&lt;/p&gt;

&lt;p&gt;可视化中间参数需要用到训练时保存的log文件paul_train_log.txt&lt;/p&gt;

&lt;p&gt;训练log中各参数的意义&lt;br /&gt;
Region Avg IOU：平均的IOU，代表预测的bounding box和ground truth的交集与并集之比，期望该值趋近于1。&lt;/p&gt;

&lt;p&gt;Class:是标注物体的概率，期望该值趋近于1.&lt;/p&gt;

&lt;p&gt;Obj：期望该值趋近于1.&lt;/p&gt;

&lt;p&gt;No Obj:期望该值越来越小但不为零.&lt;/p&gt;

&lt;p&gt;Avg Recall：期望该值趋近1&lt;/p&gt;

&lt;p&gt;avg：平均损失，期望该值趋近于0&lt;/p&gt;

&lt;p&gt;使用train_loss_visualization.py脚本可以绘制loss变化曲线。&lt;br /&gt;
脚本已上传至GitHub（使用前需安装依赖）：&lt;br /&gt;
&lt;a href=&#34;https://github.com/PaulChongPeng/darknet/blob/master/tools/train_loss_visualization.py&#34; target=&#34;_blank&#34;&gt;https://github.com/PaulChongPeng/darknet/blob/master/tools/train_loss_visualization.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;import pandas as pd&lt;br /&gt;
import numpy as np&lt;br /&gt;
import matplotlib.pyplot as plt&lt;/p&gt;

&lt;p&gt;lines =1878760&lt;br /&gt;
result = pd.read_csv(&#39;S:/Tools/Paul_YOLO/paul_train_log_new.txt&#39;, skiprows=[x for x in range(lines) if ((x%10!=9) |(x&amp;lt;1000))] ,error_bad_lines=False, names=[&#39;loss&#39;, &#39;avg&#39;, &#39;rate&#39;, &#39;seconds&#39;, &#39;images&#39;])&lt;br /&gt;
result.head()&lt;/p&gt;

&lt;p&gt;result[&#39;loss&#39;]=result[&#39;loss&#39;].str.split(&#39; &#39;).str.get(1)&lt;br /&gt;
result[&#39;avg&#39;]=result[&#39;avg&#39;].str.split(&#39; &#39;).str.get(1)&lt;br /&gt;
result[&#39;rate&#39;]=result[&#39;rate&#39;].str.split(&#39; &#39;).str.get(1)&lt;br /&gt;
result[&#39;seconds&#39;]=result[&#39;seconds&#39;].str.split(&#39; &#39;).str.get(1)&lt;br /&gt;
result[&#39;images&#39;]=result[&#39;images&#39;].str.split(&#39; &#39;).str.get(1)&lt;br /&gt;
result.head()&lt;br /&gt;
result.tail()&lt;/p&gt;

&lt;p&gt;#print(result.head())&lt;/p&gt;

&lt;h1 id=&#34;print-result-tail&#34;&gt;print(result.tail())&lt;/h1&gt;

&lt;h1 id=&#34;print-result-dtypes&#34;&gt;print(result.dtypes)&lt;/h1&gt;

&lt;p&gt;print(result[&#39;loss&#39;])&lt;br /&gt;
print(result[&#39;avg&#39;])&lt;br /&gt;
print(result[&#39;rate&#39;])&lt;br /&gt;
print(result[&#39;seconds&#39;])&lt;br /&gt;
print(result[&#39;images&#39;])&lt;/p&gt;

&lt;p&gt;result[&#39;loss&#39;]=pd.to_numeric(result[&#39;loss&#39;])&lt;br /&gt;
result[&#39;avg&#39;]=pd.to_numeric(result[&#39;avg&#39;])&lt;br /&gt;
result[&#39;rate&#39;]=pd.to_numeric(result[&#39;rate&#39;])&lt;br /&gt;
result[&#39;seconds&#39;]=pd.to_numeric(result[&#39;seconds&#39;])&lt;br /&gt;
result[&#39;images&#39;]=pd.to_numeric(result[&#39;images&#39;])&lt;br /&gt;
result.dtypes&lt;/p&gt;

&lt;p&gt;fig = plt.figure()&lt;br /&gt;
ax = fig.add_subplot(1, 1, 1)&lt;br /&gt;
ax.plot(result[&#39;avg&#39;].values,label=&#39;avg_loss&#39;)&lt;br /&gt;
#ax.plot(result[&#39;loss&#39;].values,label=&#39;loss&#39;)&lt;br /&gt;
ax.legend(loc=&#39;best&#39;)&lt;br /&gt;
ax.set_title(&#39;The loss curves&#39;)&lt;br /&gt;
ax.set_xlabel(&#39;batches&#39;)&lt;br /&gt;
fig.savefig(&#39;avg_loss&#39;)&lt;br /&gt;
#fig.savefig(&#39;loss&#39;)&lt;/p&gt;

&lt;p&gt;脚本使用说明：&lt;/p&gt;

&lt;p&gt;使用命令&lt;/p&gt;

&lt;p&gt;2&amp;gt;1 | tee paul_train_log.txt&lt;br /&gt;
保存log时会生成两个文件，文件1里保存的是网络加载信息和checkout点保存信息，paul_train_log.txt中保存的是训练信息。&lt;/p&gt;

&lt;p&gt;1、删除log开头的三行：&lt;/p&gt;

&lt;p&gt;0,1,2,3,4,5,6,7&lt;br /&gt;
yolo-paul&lt;br /&gt;
Learning Rate: 1e-05, Momentum: 0.9, Decay: 0.0005&lt;br /&gt;
2、删除log的结尾几行，使最后一行为batch的输出，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;497001: 0.863348, 0.863348 avg, 0.001200 rate, 5.422251 seconds, 107352216 images
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、执行extract_log.py脚本，格式化log。脚本代码见GitHub：&lt;br /&gt;
&lt;a href=&#34;https://github.com/PaulChongPeng/darknet/blob/master/tools/extract_log.py&#34; target=&#34;_blank&#34;&gt;https://github.com/PaulChongPeng/darknet/blob/master/tools/extract_log.py&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;coding-utf-8-3&#34;&gt;coding=utf-8&lt;/h1&gt;

&lt;h1 id=&#34;该文件用来提取训练log-去除不可解析的log后使log文件格式化-生成新的log文件供可视化工具绘图&#34;&gt;该文件用来提取训练log，去除不可解析的log后使log文件格式化，生成新的log文件供可视化工具绘图&lt;/h1&gt;

&lt;p&gt;import random&lt;/p&gt;

&lt;p&gt;f = open(&#39;paul_train_log.txt&#39;)&lt;br /&gt;
train_log = open(&#39;paul_train_log_new.txt&#39;, &#39;w&#39;)&lt;/p&gt;

&lt;p&gt;for line in f:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 去除多gpu的同步log
if &#39;Syncing&#39; in line:
    continue
# 去除除零错误的log
if &#39;nan&#39; in line:
    continue
train_log.write(line)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;f.close()&lt;br /&gt;
train_log.close()&lt;/p&gt;

&lt;p&gt;最终log格式：&lt;/p&gt;

&lt;p&gt;Loaded: 5.588888 seconds&lt;br /&gt;
Region Avg IOU: 0.649881, Class: 0.854394, Obj: 0.476559, No Obj: 0.007302, Avg Recall: 0.737705,  count: 61&lt;br /&gt;
Region Avg IOU: 0.671544, Class: 0.959081, Obj: 0.523326, No Obj: 0.006902, Avg Recall: 0.780000,  count: 50&lt;br /&gt;
Region Avg IOU: 0.525841, Class: 0.815314, Obj: 0.449031, No Obj: 0.006602, Avg Recall: 0.484375,  count: 64&lt;br /&gt;
Region Avg IOU: 0.583596, Class: 0.830763, Obj: 0.377681, No Obj: 0.007916, Avg Recall: 0.629214,  count: 89&lt;br /&gt;
Region Avg IOU: 0.651377, Class: 0.908635, Obj: 0.460094, No Obj: 0.008060, Avg Recall: 0.753425,  count: 73&lt;br /&gt;
Region Avg IOU: 0.571363, Class: 0.880554, Obj: 0.341659, No Obj: 0.007820, Avg Recall: 0.633663,  count: 101&lt;br /&gt;
Region Avg IOU: 0.585424, Class: 0.935552, Obj: 0.358635, No Obj: 0.008192, Avg Recall: 0.644860,  count: 107&lt;br /&gt;
Region Avg IOU: 0.599972, Class: 0.832793, Obj: 0.382910, No Obj: 0.009005, Avg Recall: 0.650602,  count: 83&lt;br /&gt;
497001: 0.863348, 0.863348 avg, 0.000012 rate, 5.422251 seconds, 107352216 images&lt;/p&gt;

&lt;p&gt;4、修改train_loss_visualization.py中lines为log行数，并根据需要修改要跳过的行数。&lt;/p&gt;

&lt;p&gt;skiprows=[x for x in range(lines) if ((x%10!=9) |(x&amp;lt;1000))]&lt;br /&gt;
运行train_loss_visualization.py会在脚本所在路径生成avg_loss.png。&lt;/p&gt;

&lt;p&gt;这里写图片描述&lt;/p&gt;

&lt;p&gt;从损失变化曲线可以看出，模型在100000万次迭代后损失下降速度非常慢，几乎没有下降。结合log和cfg文件发现，我自定义的学习率变化策略在十万次迭代时会减小十倍，十万次迭代后学习率下降到非常小的程度，导致损失下降速度降低。修改cfg中的学习率变化策略，10万次迭代时不改变学习率，30万次时再降低。&lt;/p&gt;

&lt;p&gt;我使用迭代97000次时的备份的checkout点来继续训练。&lt;/p&gt;

&lt;p&gt;./darknet detector train cfg/paul.data cfg/yolo-paul.cfg backup/yolo-paul_97000.weights 2&amp;gt;1 | tee paul_train_log.txt&lt;br /&gt;
除了可视化loss，还可以可视化Avg IOU，Avg Recall等参数。&lt;br /&gt;
可视化’Region Avg IOU’, ‘Class’, ‘Obj’, ‘No Obj’, ‘Avg Recall’,’count’这些参数可以使用脚本train_iou_visualization.py，使用方式和train_loss_visualization.py相同。脚本已上传至GitHub：&lt;a href=&#34;https://github.com/PaulChongPeng/darknet/blob/master/tools/train_iou_visualization.py&#34; target=&#34;_blank&#34;&gt;https://github.com/PaulChongPeng/darknet/blob/master/tools/train_iou_visualization.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;import pandas as pd&lt;br /&gt;
import numpy as np&lt;br /&gt;
import matplotlib.pyplot as plt&lt;/p&gt;

&lt;p&gt;lines =525990&lt;br /&gt;
result = pd.read_csv(&#39;S:/Tools/Paul_YOLO/paul_train_log_new.txt&#39;, skiprows=[x for x in range(lines) if (x%10==0 or x%10==9) ] ,error_bad_lines=False, names=[&#39;Region Avg IOU&#39;, &#39;Class&#39;, &#39;Obj&#39;, &#39;No Obj&#39;, &#39;Avg Recall&#39;,&#39;count&#39;])&lt;br /&gt;
result.head()&lt;/p&gt;

&lt;p&gt;result[&#39;Region Avg IOU&#39;]=result[&#39;Region Avg IOU&#39;].str.split(&#39;: &#39;).str.get(1)&lt;br /&gt;
result[&#39;Class&#39;]=result[&#39;Class&#39;].str.split(&#39;: &#39;).str.get(1)&lt;br /&gt;
result[&#39;Obj&#39;]=result[&#39;Obj&#39;].str.split(&#39;: &#39;).str.get(1)&lt;br /&gt;
result[&#39;No Obj&#39;]=result[&#39;No Obj&#39;].str.split(&#39;: &#39;).str.get(1)&lt;br /&gt;
result[&#39;Avg Recall&#39;]=result[&#39;Avg Recall&#39;].str.split(&#39;: &#39;).str.get(1)&lt;br /&gt;
result[&#39;count&#39;]=result[&#39;count&#39;].str.split(&#39;: &#39;).str.get(1)&lt;br /&gt;
result.head()&lt;br /&gt;
result.tail()&lt;/p&gt;

&lt;p&gt;#print(result.head())&lt;/p&gt;

&lt;h1 id=&#34;print-result-tail-1&#34;&gt;print(result.tail())&lt;/h1&gt;

&lt;h1 id=&#34;print-result-dtypes-1&#34;&gt;print(result.dtypes)&lt;/h1&gt;

&lt;p&gt;print(result[&#39;Region Avg IOU&#39;])&lt;/p&gt;

&lt;p&gt;result[&#39;Region Avg IOU&#39;]=pd.to_numeric(result[&#39;Region Avg IOU&#39;])&lt;br /&gt;
result[&#39;Class&#39;]=pd.to_numeric(result[&#39;Class&#39;])&lt;br /&gt;
result[&#39;Obj&#39;]=pd.to_numeric(result[&#39;Obj&#39;])&lt;br /&gt;
result[&#39;No Obj&#39;]=pd.to_numeric(result[&#39;No Obj&#39;])&lt;br /&gt;
result[&#39;Avg Recall&#39;]=pd.to_numeric(result[&#39;Avg Recall&#39;])&lt;br /&gt;
result[&#39;count&#39;]=pd.to_numeric(result[&#39;count&#39;])&lt;br /&gt;
result.dtypes&lt;/p&gt;

&lt;p&gt;fig = plt.figure()&lt;br /&gt;
ax = fig.add_subplot(1, 1, 1)&lt;br /&gt;
#ax.plot(result[&#39;Region Avg IOU&#39;].values,label=&#39;Region Avg IOU&#39;)&lt;br /&gt;
#ax.plot(result[&#39;Class&#39;].values,label=&#39;Class&#39;)&lt;br /&gt;
#ax.plot(result[&#39;Obj&#39;].values,label=&#39;Obj&#39;)&lt;br /&gt;
#ax.plot(result[&#39;No Obj&#39;].values,label=&#39;No Obj&#39;)&lt;br /&gt;
ax.plot(result[&#39;Avg Recall&#39;].values,label=&#39;Avg Recall&#39;)&lt;br /&gt;
#ax.plot(result[&#39;count&#39;].values,label=&#39;count&#39;)&lt;br /&gt;
ax.legend(loc=&#39;best&#39;)&lt;br /&gt;
#ax.set_title(&#39;The Region Avg IOU curves&#39;)&lt;br /&gt;
ax.set_title(&#39;The Avg Recall curves&#39;)&lt;br /&gt;
ax.set_xlabel(&#39;batches&#39;)&lt;br /&gt;
#fig.savefig(&#39;Avg IOU&#39;)&lt;br /&gt;
fig.savefig(&#39;Avg Recall&#39;)&lt;/p&gt;

&lt;p&gt;这里写图片描述&lt;/p&gt;

&lt;p&gt;使用验证集评估模型&lt;br /&gt;
评估模型可以使用命令valid（只有预测结果，没有评价预测是否正确）或recall，这两个命令都无法满足我的需求，我实现了category命令做性能评估。&lt;/p&gt;

&lt;p&gt;valid:&lt;br /&gt;
在paul.data末尾添加&lt;/p&gt;

&lt;p&gt;eval = imagenet #有voc、coco、imagenet三种模式&lt;br /&gt;
修改Detector.c文件validate_detector函数，修改阈值（默认.005）&lt;/p&gt;

&lt;p&gt;float thresh = .1;&lt;br /&gt;
重新编译然后执行命令&lt;/p&gt;

&lt;p&gt;./darknet detector valid cfg/paul.data cfg/yolo-paul.cfg backup/yolo-paul_final.weights&lt;br /&gt;
results目录下会生成预测结果，格式如下：&lt;/p&gt;

&lt;p&gt;1 1 0.431522 235.186066 77.746033 421.808258 348.950012&lt;br /&gt;
1 1 0.186538 161.324097 270.221497 187.429535 321.382141&lt;br /&gt;
1 14 0.166257 284.207947 364.423889 465.995056 454.305603&lt;br /&gt;
2 30 0.287718 274.455719 290.674194 343.506256 352.656433&lt;br /&gt;
2 30 0.582356 293.578918 294.799438 350.478088 327.216614&lt;br /&gt;
2 1 0.599921 138.686981 314.705231 352.362152 588.235962&lt;br /&gt;
3 59 0.251553 193.290497 183.707275 277.655273 349.782410&lt;br /&gt;
3 59 0.107120 209.172287 269.722626 330.998718 342.530914&lt;br /&gt;
3 62 0.162954 0.000000 278.525543 457.739563 480.000000&lt;br /&gt;
4 6 0.617184 38.155792 31.496445 434.091705 527.705811&lt;br /&gt;
4 1 0.101005 358.778351 238.540756 395.645050 289.902283&lt;br /&gt;
4 6 0.813770 75.790985 282.521210 459.018585 564.883545&lt;br /&gt;
4 3 0.114561 32.667072 407.288025 142.561798 506.885498&lt;br /&gt;
4 3 0.104120 87.489151 337.674896 446.883728 584.356689&lt;br /&gt;
5 1 0.106601 235.460571 0.707840 265.958740 34.851868&lt;br /&gt;
5 1 0.134753 310.776398 1.273307 344.392303 31.028347&lt;br /&gt;
5 1 0.146177 349.860596 0.445604 385.901550 29.931465&lt;br /&gt;
5 1 0.129790 388.831177 3.721551 419.852844 30.414955&lt;br /&gt;
5 1 0.146747 369.672150 0.000000 441.490387 45.012733&lt;br /&gt;
5 1 0.339233 7.567236 0.000000 53.692001 97.718735&lt;/p&gt;

&lt;p&gt;如果想要查看recall可以使用recall命令。&lt;br /&gt;
修改费Detector.c文件的validate_detector_recall函数：&lt;/p&gt;

&lt;p&gt;1、修改阈值：&lt;/p&gt;

&lt;p&gt;float thresh = .25;&lt;br /&gt;
2、修改验证集路径：&lt;/p&gt;

&lt;p&gt;list *plist = get_paths(&amp;quot;/mnt/large4t/pengchong_data/Data/Paul/filelist/val.txt&amp;quot;);&lt;br /&gt;
3、增加Precision&lt;/p&gt;

&lt;p&gt;//fprintf(stderr, &amp;quot;%5d %5d %5d\tRPs/Img: %.2f\tIOU: %.2f%%\tRecall:%.2f%%\n&amp;quot;, i, correct, total, (float)proposals/(i+1), avg_iou*100/total, 100.*correct/total);&lt;br /&gt;
fprintf(stderr, &amp;quot;ID:%5d Correct:%5d Total:%5d\tRPs/Img: %.2f\tIOU: %.2f%%\tRecall:%.2f%%\t&amp;quot;, i, correct, total, (float)proposals/(i+1), avg_iou*100/total, 100.*correct/total);&lt;br /&gt;
fprintf(stderr, &amp;quot;proposals:%5d\tPrecision:%.2f%%\n&amp;quot;,proposals,100.*correct/(float)proposals);&lt;br /&gt;
重新编译然后执行命令&lt;/p&gt;

&lt;p&gt;./darknet detector recall cfg/paul.data cfg/yolo-paul.cfg backup/yolo-paul_final.weights&lt;br /&gt;
结果格式如下：&lt;/p&gt;

&lt;p&gt;ID:    0 Correct:    1 Total:   22  RPs/Img: 2.00   IOU: 7.59%  Recall:4.55%    proposals:    2 Precision:50.00%&lt;br /&gt;
ID:    1 Correct:    2 Total:   28  RPs/Img: 2.00   IOU: 8.90%  Recall:7.14%    proposals:    4 Precision:50.00%&lt;br /&gt;
ID:    2 Correct:    3 Total:   39  RPs/Img: 1.67   IOU: 7.91%  Recall:7.69%    proposals:    5 Precision:60.00%&lt;br /&gt;
ID:    3 Correct:    3 Total:   42  RPs/Img: 2.00   IOU: 7.42%  Recall:7.14%    proposals:    8 Precision:37.50%&lt;br /&gt;
ID:    4 Correct:    9 Total:   58  RPs/Img: 5.00   IOU: 15.96% Recall:15.52%   proposals:   25 Precision:36.00%&lt;br /&gt;
ID:    5 Correct:   10 Total:   70  RPs/Img: 4.50   IOU: 14.99% Recall:14.29%   proposals:   27 Precision:37.04%&lt;br /&gt;
ID:    6 Correct:   12 Total:   72  RPs/Img: 4.00   IOU: 16.51% Recall:16.67%   proposals:   28 Precision:42.86%&lt;br /&gt;
ID:    7 Correct:   14 Total:   76  RPs/Img: 3.75   IOU: 17.60% Recall:18.42%   proposals:   30 Precision:46.67%&lt;br /&gt;
ID:    8 Correct:   16 Total:   81  RPs/Img: 3.78   IOU: 19.15% Recall:19.75%   proposals:   34 Precision:47.06%&lt;br /&gt;
ID:    9 Correct:   20 Total:   96  RPs/Img: 3.80   IOU: 20.40% Recall:20.83%   proposals:   38 Precision:52.63%&lt;br /&gt;
ID:   10 Correct:   22 Total:  103  RPs/Img: 3.82   IOU: 21.09% Recall:21.36%   proposals:   42 Precision:52.38%&lt;/p&gt;

&lt;p&gt;category命令评估模型针对每种物体检测的性能&lt;br /&gt;
代码已提交至GitHub：&lt;a href=&#34;https://github.com/PaulChongPeng/darknet/blob/master/src/detector.c&#34; target=&#34;_blank&#34;&gt;https://github.com/PaulChongPeng/darknet/blob/master/src/detector.c&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;void print_category(FILE **fps, char *path, box *boxes, float **probs, int total, int classes, int w, int h, float thresh, float iou_thresh)&lt;br /&gt;
{&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int i, j;

char labelpath[4096];
find_replace(path, &amp;quot;images&amp;quot;, &amp;quot;labels&amp;quot;, labelpath);
find_replace(labelpath, &amp;quot;JPEGImages&amp;quot;, &amp;quot;labels&amp;quot;, labelpath);
find_replace(labelpath, &amp;quot;.jpg&amp;quot;, &amp;quot;.txt&amp;quot;, labelpath);
find_replace(labelpath, &amp;quot;.JPEG&amp;quot;, &amp;quot;.txt&amp;quot;, labelpath);

int num_labels = 0;
box_label *truth = read_boxes(labelpath, &amp;amp;num_labels);

for(i = 0; i &amp;lt; total; ++i){
    int class_id = max_index(probs[i],classes);
    float prob = probs[i][class_id];
    if (prob &amp;lt; thresh)continue;

    float best_iou = 0;
    int best_iou_id = 0;
    int correct = 0;
    for (j = 0; j &amp;lt; num_labels; ++j) {
        box t = {truth[j].x*w, truth[j].y*h, truth[j].w*w, truth[j].h*h};
        float iou = box_iou(boxes[i], t);
        //fprintf(stderr, &amp;quot;box p: %f, %f, %f, %f\n&amp;quot;, boxes[i].x, boxes[i].y, boxes[i].w, boxes[i].h);
        //fprintf(stderr, &amp;quot;box t: %f, %f, %f, %f\n&amp;quot;, t.x, t.y, t.w, t.h);
        //fprintf(stderr, &amp;quot;iou : %f\n&amp;quot;, iou);
        if(iou &amp;gt; best_iou){
            best_iou = iou;
            best_iou_id = j;
        }
    }

    if(best_iou &amp;gt; iou_thresh &amp;amp;&amp;amp; truth[best_iou_id].id == class_id){
        correct = 1;
    }

    float xmin = boxes[i].x - boxes[i].w/2.;
    float xmax = boxes[i].x + boxes[i].w/2.;
    float ymin = boxes[i].y - boxes[i].h/2.;
    float ymax = boxes[i].y + boxes[i].h/2.;

    if (xmin &amp;lt; 0) xmin = 0;
    if (ymin &amp;lt; 0) ymin = 0;
    if (xmax &amp;gt; w) xmax = w;
    if (ymax &amp;gt; h) ymax = h;

    fprintf(fps[class_id], &amp;quot;%s, %d, %d, %f, %f, %f, %f, %f, %f\n&amp;quot;, path, class_id, correct, prob, best_iou, xmin, ymin, xmax, ymax);

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;void validate_detector_category(char *datacfg, char *cfgfile, char *weightfile, char *outfile)&lt;br /&gt;
{&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int j;
list *options = read_data_cfg(datacfg);
char *valid_images = option_find_str(options, &amp;quot;valid&amp;quot;, &amp;quot;data/train.list&amp;quot;);
char *name_list = option_find_str(options, &amp;quot;names&amp;quot;, &amp;quot;data/names.list&amp;quot;);
char *prefix = option_find_str(options, &amp;quot;results&amp;quot;, &amp;quot;results&amp;quot;);
char **names = get_labels(name_list);
char *mapf = option_find_str(options, &amp;quot;map&amp;quot;, 0);
int *map = 0;
if (mapf) map = read_map(mapf);

network net = parse_network_cfg(cfgfile);
if(weightfile){
    load_weights(&amp;amp;net, weightfile);
}
set_batch_network(&amp;amp;net, 1);
fprintf(stderr, &amp;quot;Learning Rate: %g, Momentum: %g, Decay: %g\n&amp;quot;, net.learning_rate, net.momentum, net.decay);
srand(time(0));

list *plist = get_paths(valid_images);
char **paths = (char **)list_to_array(plist);

layer l = net.layers[net.n-1];
int classes = l.classes;

char buff[1024];
FILE **fps = 0;
if(!outfile) outfile = &amp;quot;paul_&amp;quot;;
fps = calloc(classes, sizeof(FILE *));
for(j = 0; j &amp;lt; classes; ++j){
    snprintf(buff, 1024, &amp;quot;%s/%s%s.txt&amp;quot;, prefix, outfile, names[j]);
    fps[j] = fopen(buff, &amp;quot;w&amp;quot;);
}


box *boxes = calloc(l.w*l.h*l.n, sizeof(box));
float **probs = calloc(l.w*l.h*l.n, sizeof(float *));
for(j = 0; j &amp;lt; l.w*l.h*l.n; ++j) probs[j] = calloc(classes, sizeof(float *));

int m = plist-&amp;gt;size;
int i=0;
int t;

float thresh = .25;
float iou_thresh = .5;
float nms = .45;

int nthreads = 4;
image *val = calloc(nthreads, sizeof(image));
image *val_resized = calloc(nthreads, sizeof(image));
image *buf = calloc(nthreads, sizeof(image));
image *buf_resized = calloc(nthreads, sizeof(image));
pthread_t *thr = calloc(nthreads, sizeof(pthread_t));

load_args args = {0};
args.w = net.w;
args.h = net.h;
args.type = IMAGE_DATA;

for(t = 0; t &amp;lt; nthreads; ++t){
    args.path = paths[i+t];
    args.im = &amp;amp;buf[t];
    args.resized = &amp;amp;buf_resized[t];
    thr[t] = load_data_in_thread(args);
}
time_t start = time(0);
for(i = nthreads; i &amp;lt; m+nthreads; i += nthreads){
    fprintf(stderr, &amp;quot;%d\n&amp;quot;, i);
    for(t = 0; t &amp;lt; nthreads &amp;amp;&amp;amp; i+t-nthreads &amp;lt; m; ++t){
        pthread_join(thr[t], 0);
        val[t] = buf[t];
        val_resized[t] = buf_resized[t];
    }
    for(t = 0; t &amp;lt; nthreads &amp;amp;&amp;amp; i+t &amp;lt; m; ++t){
        args.path = paths[i+t];
        args.im = &amp;amp;buf[t];
        args.resized = &amp;amp;buf_resized[t];
        thr[t] = load_data_in_thread(args);
    }
    for(t = 0; t &amp;lt; nthreads &amp;amp;&amp;amp; i+t-nthreads &amp;lt; m; ++t){
        char *path = paths[i+t-nthreads];
        float *X = val_resized[t].data;
        network_predict(net, X);
        int w = val[t].w;
        int h = val[t].h;
        get_region_boxes(l, w, h, thresh, probs, boxes, 0, map, .5);
        if (nms) do_nms_sort(boxes, probs, l.w*l.h*l.n, classes, nms);
        print_category(fps, path, boxes, probs, l.w*l.h*l.n, classes, w, h, thresh, iou_thresh);
        free_image(val[t]);
        free_image(val_resized[t]);
    }
}
for(j = 0; j &amp;lt; classes; ++j){
    if(fps) fclose(fps[j]);
}
fprintf(stderr, &amp;quot;Total Detection Time: %f Seconds\n&amp;quot;, (double)(time(0) - start));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;void run_detector(int argc, char **argv)&lt;br /&gt;
{&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char *prefix = find_char_arg(argc, argv, &amp;quot;-prefix&amp;quot;, 0);
float thresh = find_float_arg(argc, argv, &amp;quot;-thresh&amp;quot;, .24);
float hier_thresh = find_float_arg(argc, argv, &amp;quot;-hier&amp;quot;, .5);
int cam_index = find_int_arg(argc, argv, &amp;quot;-c&amp;quot;, 0);
int frame_skip = find_int_arg(argc, argv, &amp;quot;-s&amp;quot;, 0);
if(argc &amp;lt; 4){
    fprintf(stderr, &amp;quot;usage: %s %s [train/test/valid] [cfg] [weights (optional)]\n&amp;quot;, argv[0], argv[1]);
    return;
}
char *gpu_list = find_char_arg(argc, argv, &amp;quot;-gpus&amp;quot;, 0);
char *outfile = find_char_arg(argc, argv, &amp;quot;-out&amp;quot;, 0);
int *gpus = 0;
int gpu = 0;
int ngpus = 0;
if(gpu_list){
    printf(&amp;quot;%s\n&amp;quot;, gpu_list);
    int len = strlen(gpu_list);
    ngpus = 1;
    int i;
    for(i = 0; i &amp;lt; len; ++i){
        if (gpu_list[i] == &#39;,&#39;) ++ngpus;
    }
    gpus = calloc(ngpus, sizeof(int));
    for(i = 0; i &amp;lt; ngpus; ++i){
        gpus[i] = atoi(gpu_list);
        gpu_list = strchr(gpu_list, &#39;,&#39;)+1;
    }
} else {
    gpu = gpu_index;
    gpus = &amp;amp;gpu;
    ngpus = 1;
}

int clear = find_arg(argc, argv, &amp;quot;-clear&amp;quot;);

char *datacfg = argv[3];
char *cfg = argv[4];
char *weights = (argc &amp;gt; 5) ? argv[5] : 0;
char *filename = (argc &amp;gt; 6) ? argv[6]: 0;
if(0==strcmp(argv[2], &amp;quot;test&amp;quot;)) test_detector(datacfg, cfg, weights, filename, thresh, hier_thresh);
else if(0==strcmp(argv[2], &amp;quot;train&amp;quot;)) train_detector(datacfg, cfg, weights, gpus, ngpus, clear);
else if(0==strcmp(argv[2], &amp;quot;valid&amp;quot;)) validate_detector(datacfg, cfg, weights, outfile);
else if(0==strcmp(argv[2], &amp;quot;recall&amp;quot;)) validate_detector_recall(cfg, weights);
else if(0==strcmp(argv[2], &amp;quot;category&amp;quot;))validate_detector_category(datacfg, cfg, weights, outfile);
else if(0==strcmp(argv[2], &amp;quot;demo&amp;quot;)) {
    list *options = read_data_cfg(datacfg);
    int classes = option_find_int(options, &amp;quot;classes&amp;quot;, 20);
    char *name_list = option_find_str(options, &amp;quot;names&amp;quot;, &amp;quot;data/names.list&amp;quot;);
    char **names = get_labels(name_list);
    demo(cfg, weights, thresh, cam_index, filename, names, classes, frame_skip, prefix, hier_thresh);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;执行命令&lt;/p&gt;

&lt;p&gt;./darknet detector category cfg/paul.data cfg/yolo-paul.cfg backup/yolo-paul_final.weights&lt;br /&gt;
result目录下会生成各类物体的val结果，有多少种物体，就会生成多少个txt文件，每个txt文件中有path, class_id, correct, prob, best_iou, xmin, ymin, xmax, ymax信息。&lt;/p&gt;

&lt;p&gt;使用evalute.py工具可以解析这些txt文件做一个总结性的评估。&lt;br /&gt;
工具已上传到GitHub：&lt;a href=&#34;https://github.com/PaulChongPeng/darknet/blob/master/tools/evalute.py&#34; target=&#34;_blank&#34;&gt;https://github.com/PaulChongPeng/darknet/blob/master/tools/evalute.py&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;coding-utf-8-4&#34;&gt;coding=utf-8&lt;/h1&gt;

&lt;h1 id=&#34;本工具和category命令结合使用&#34;&gt;本工具和category命令结合使用&lt;/h1&gt;

&lt;h1 id=&#34;category是在detector-c中新增的命令-主要作用是生成每类物体的evalute结果&#34;&gt;category是在detector.c中新增的命令，主要作用是生成每类物体的evalute结果&lt;/h1&gt;

&lt;h1 id=&#34;执行命令-darknet-detector-category-cfg-paul-data-cfg-yolo-paul-cfg-backup-yolo-paul-final-weights&#34;&gt;执行命令 ./darknet detector category cfg/paul.data cfg/yolo-paul.cfg backup/yolo-paul_final.weights&lt;/h1&gt;

&lt;h1 id=&#34;result目录下会生成各类物体的val结果-将本工具放在result目录下执行-会print出各种物体的evalute结果-包括&#34;&gt;result目录下会生成各类物体的val结果，将本工具放在result目录下执行，会print出各种物体的evalute结果，包括&lt;/h1&gt;

&lt;h1 id=&#34;id-avg-iou-avg-correct-iou-avg-precision-avg-recall-avg-score&#34;&gt;id,avg_iou,avg_correct_iou,avg_precision,avg_recall,avg_score&lt;/h1&gt;

&lt;h1 id=&#34;result目录下会生成low-list和high-list-内容分别为精度和recall未达标和达标的物体种类&#34;&gt;result目录下会生成low_list和high_list，内容分别为精度和recall未达标和达标的物体种类&lt;/h1&gt;

&lt;p&gt;import os&lt;br /&gt;
from os import listdir, getcwd&lt;br /&gt;
from os.path import join&lt;br /&gt;
import shutil&lt;/p&gt;

&lt;h1 id=&#34;共有多少类物体&#34;&gt;共有多少类物体&lt;/h1&gt;

&lt;p&gt;class_num = 97&lt;/p&gt;

&lt;h1 id=&#34;每类物体的验证结果&#34;&gt;每类物体的验证结果&lt;/h1&gt;

&lt;p&gt;class CategoryValidation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;id = 0  # Category id
path = &amp;quot;&amp;quot;  # path
total_num = 0  # 标注文件中该类bounding box的总数
proposals_num = 0  # validate结果中共预测了多少个该类的bounding box
correct_num = 0  # 预测正确的bounding box（与Ground-truth的IOU大于0.5且种类正确）的数量
iou_num = 0  # 所有大于0.5的IOU的数量
iou_sum = 0  # 所有大于0.5的IOU的IOU之和
correct_iou_sum = 0  # 预测正确的bounding box的IOU之和
score_sum = 0  # 所有正确预测的bounding box的概率之和
avg_iou = 0  # 无论预测的bounding box的object的种类是否正确，所有bounding box 与最吻合的Ground-truth求出IOU，对大于0.5的IOU求平均值：avg_iou = iou_sum/iou_num
avg_correct_iou = 0  # 对预测正确的bounding box的IOU求平均值：avg_correct_iou = correct_iou_sum/correct_num
avg_precision = 0  # avg_precision = correct_num/proposals_num
avg_recall = 0  # avg_recall = correct_num/total_num
avg_score = 0  # avg_score=score_sum/correct_num

def __init__(self, path, val_cat_num):
    self.path = path
    f = open(path)

    for line in f:
        temp = line.rstrip().replace(&#39; &#39;, &#39;&#39;).split(&#39;,&#39;, 9)
        temp[1] = int(temp[1])
        self.id = temp[1]
        self.total_num = val_cat_num[self.id]
        if (self.total_num):
            break

    for line in f:
        # path, class_id, correct, prob, best_iou, xmin, ymin, xmax, ymax
        temp = line.rstrip().split(&#39;, &#39;, 9)
        temp[1] = int(temp[1])
        temp[2] = int(temp[2])
        temp[3] = float(temp[3])
        temp[4] = float(temp[4])
        self.proposals_num = self.proposals_num + 1.00
        if (temp[2]):
            self.correct_num = self.correct_num + 1.00
            self.score_sum = self.score_sum + temp[3]
            self.correct_iou_sum = self.correct_iou_sum + temp[4]
        if (temp[4] &amp;gt; 0.5):
            self.iou_num = self.iou_num + 1
            self.iou_sum = self.iou_sum + temp[4]

    self.avg_iou = self.iou_sum / self.iou_num
    self.avg_correct_iou = self.correct_iou_sum / self.correct_num
    self.avg_precision = self.correct_num / self.proposals_num
    self.avg_recall = self.correct_num / self.total_num
    self.avg_score = self.score_sum / self.correct_num

    f.close()

# 导出识别正确的图片列表
def get_correct_list(self):
    f = open(self.path)
    new_f_name = &amp;quot;correct_list_&amp;quot; + self.id + &amp;quot;.txt&amp;quot;
    new_f = open(new_f_name, &#39;w&#39;)
    for line in f:
        temp = line.rstrip().split(&#39;, &#39;, 9)
        if (temp[2]):
            new_f.write(line)
    f.close()

# 导出识别错误的图片列表
def get_error_list(self):
    f = open(self.path)
    new_f_name = &amp;quot;error_list_&amp;quot; + self.id + &amp;quot;.txt&amp;quot;
    new_f = open(new_f_name, &#39;w&#39;)
    for line in f:
        temp = line.rstrip().split(&#39;, &#39;, 9)
        if (temp[2] == 0):
            new_f.write(line)
    f.close()

def print_eva(self):
    print(&amp;quot;id=%d, avg_iou=%f, avg_correct_iou=%f, avg_precision=%f, avg_recall=%f, avg_score=%f \n&amp;quot; % (self.id,
                                                                                                       self.avg_iou,
                                                                                                       self.avg_correct_iou,
                                                                                                       self.avg_precision,
                                                                                                       self.avg_recall,
                                                                                                       self.avg_score))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;def IsSubString(SubStrList, Str):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flag = True
for substr in SubStrList:
    if not (substr in Str):
        flag = False

return flag
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;获取findpath路径下指定格式-flagstr-的文件名列表&#34;&gt;获取FindPath路径下指定格式（FlagStr）的文件名列表&lt;/h1&gt;

&lt;p&gt;def GetFileList(FindPath, FlagStr=[]):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os
FileList = []
FileNames = os.listdir(FindPath)
if (len(FileNames) &amp;gt; 0):
    for fn in FileNames:
        if (len(FlagStr) &amp;gt; 0):
            if (IsSubString(FlagStr, fn)):
                FileList.append(fn)
        else:
            FileList.append(fn)

if (len(FileList) &amp;gt; 0):
    FileList.sort()

return FileList
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;获取所有物体种类的roi数目&#34;&gt;获取所有物体种类的ROI数目&lt;/h1&gt;

&lt;h1 id=&#34;path是图片列表的地址&#34;&gt;path是图片列表的地址&lt;/h1&gt;

&lt;h1 id=&#34;返回值是一个list-list的索引是物体种类在yolo中的id-值是该种物体的roi数量&#34;&gt;返回值是一个list，list的索引是物体种类在yolo中的id，值是该种物体的ROI数量&lt;/h1&gt;

&lt;p&gt;def get_val_cat_num(path):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val_cat_num = []
for i in range(0, class_num):
    val_cat_num.append(0)

f = open(path)
for line in f:
    label_path = line.rstrip().replace(&#39;images&#39;, &#39;labels&#39;)
    label_path = label_path.replace(&#39;JPEGImages&#39;, &#39;labels&#39;)
    label_path = label_path.replace(&#39;.jpg&#39;, &#39;.txt&#39;)
    label_path = label_path.replace(&#39;.JPEG&#39;, &#39;.txt&#39;)
    label_list = open(label_path)
    for label in label_list:
        temp = label.rstrip().split(&amp;quot; &amp;quot;, 4)
        id = int(temp[0])
        val_cat_num[id] = val_cat_num[id] + 1.00
    label_list.close()
f.close()
return val_cat_num
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;获取物体名list&#34;&gt;获取物体名list&lt;/h1&gt;

&lt;h1 id=&#34;path是物体名list文件地址&#34;&gt;path是物体名list文件地址&lt;/h1&gt;

&lt;h1 id=&#34;返回值是一个列表-列表的索引是类的id-值为该类物体的名字&#34;&gt;返回值是一个列表，列表的索引是类的id，值为该类物体的名字&lt;/h1&gt;

&lt;p&gt;def get_name_list(path):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name_list = []
f = open(path)
for line in f:
    temp = line.rstrip().split(&#39;,&#39;, 2)
    name_list.append(temp[1])
return name_list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;wd = getcwd()&lt;br /&gt;
val_result_list = GetFileList(wd, [&#39;txt&#39;])&lt;br /&gt;
val_cat_num = get_val_cat_num(&amp;quot;/raid/pengchong_data/Data/filelists/val.txt&amp;quot;)&lt;br /&gt;
name_list = get_name_list(&amp;quot;/raid/pengchong_data/Tools/Paul_YOLO/data/paul_list.txt&amp;quot;)&lt;br /&gt;
low_list = open(&amp;quot;low_list.log&amp;quot;, &#39;w&#39;)&lt;br /&gt;
high_list = open(&amp;quot;high_list.log&amp;quot;, &#39;w&#39;)&lt;br /&gt;
for result in val_result_list:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat = CategoryValidation(result, val_cat_num)
cat.print_eva()
if ((cat.avg_precision &amp;lt; 0.3) | (cat.avg_recall &amp;lt; 0.3)):
    low_list.write(&amp;quot;id=%d, name=%s, avg_precision=%f, avg_recall=%f \n&amp;quot; % (cat.id, name_list[cat.id], cat.avg_precision, cat.avg_recall))
if ((cat.avg_precision &amp;gt; 0.6) &amp;amp; (cat.avg_recall &amp;gt; 0.6)):
    high_list.write(&amp;quot;id=%d, name=%s, avg_precision=%f, avg_recall=%f \n&amp;quot; % (cat.id, name_list[cat.id], cat.avg_precision, cat.avg_recall))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;low_list.close()&lt;br /&gt;
high_list.close()&lt;/p&gt;

&lt;p&gt;将本工具放在result目录下执行，会print出各种物体的evalute结果，包括&lt;br /&gt;
id,avg_iou,avg_correct_iou,avg_precision,avg_recall,avg_score。&lt;/p&gt;

&lt;p&gt;id=1, avg_iou=0.807394, avg_correct_iou=0.810435, avg_precision=0.473983, avg_recall=0.283531, avg_score=0.661014&lt;/p&gt;

&lt;p&gt;id=2, avg_iou=0.824890, avg_correct_iou=0.826227, avg_precision=0.812950, avg_recall=0.824818, avg_score=0.772828&lt;/p&gt;

&lt;p&gt;id=3, avg_iou=0.748561, avg_correct_iou=0.756006, avg_precision=0.401891, avg_recall=0.146048, avg_score=0.568196&lt;/p&gt;

&lt;p&gt;id=4, avg_iou=0.821225, avg_correct_iou=0.822419, avg_precision=0.779621, avg_recall=0.798544, avg_score=0.773700&lt;/p&gt;

&lt;p&gt;id=5, avg_iou=0.722905, avg_correct_iou=0.721078, avg_precision=0.391119, avg_recall=0.255361, avg_score=0.552248&lt;/p&gt;

&lt;p&gt;id=6, avg_iou=0.814797, avg_correct_iou=0.814427, avg_precision=0.731707, avg_recall=0.612245, avg_score=0.833531&lt;/p&gt;

&lt;p&gt;id=7, avg_iou=0.713375, avg_correct_iou=0.702796, avg_precision=0.739336, avg_recall=0.715596, avg_score=0.691065&lt;/p&gt;

&lt;p&gt;id=8, avg_iou=0.785120, avg_correct_iou=0.797686, avg_precision=0.582267, avg_recall=0.594216, avg_score=0.734099&lt;/p&gt;

&lt;p&gt;id=9, avg_iou=0.744355, avg_correct_iou=0.752729, avg_precision=0.523982, avg_recall=0.241049, avg_score=0.650683&lt;/p&gt;

&lt;p&gt;id=10, avg_iou=0.736755, avg_correct_iou=0.744951, avg_precision=0.621368, avg_recall=0.382028, avg_score=0.651450&lt;/p&gt;

&lt;p&gt;同时result目录下会生成low_list和high_list，内容分别为精度和recall未达标和达标的物体种类。&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>nnie yolov3 code</title>
            <link>/hardware/hisilicon/nnie/nnie-yolov3-code/</link>
            <pubDate>Tue, 10 Dec 2019 16:14:12 CST</pubDate>
            <author>rinetd</author>
            <guid>/hardware/hisilicon/nnie/nnie-yolov3-code/</guid>
            <description>&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/zh8706/article/details/98031231&#34; target=&#34;_blank&#34;&gt;海思NNIE开发（三）：FasterRCNN在海思NNIE平台上的执行流程（二） - 夜风里唱的专栏&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;本篇文章我们接着上一篇（海思NNIE开发二），继续分析FasterRCNN在海思NNIE平台上的执行流程。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;解析网络模型信息&lt;br /&gt;
首先我们来看以下加载模型网络信息的函数：&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;s32Ret = SAMPLE_COMM_SVP_NNIE_LoadModel(pcModelName,&amp;amp;s_stFasterRcnnModel);&lt;br /&gt;
这里的pcModelName就是wk文件的路径，s_stFasterRcnnModel是SAMPLE_SVP_NNIE_MODEL_S结构体，我们在上一篇（海思NNIE开发二）文章中已对该结构体做分析。我们进入该函数分析：&lt;/p&gt;

&lt;p&gt;HI_S32 SAMPLE_COMM_SVP_NNIE_LoadModel(HI_CHAR * pszModelFile,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SAMPLE_SVP_NNIE_MODEL_S *pstNnieModel)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HI_S32 s32Ret = HI_INVALID_VALUE;
HI_U64 u64PhyAddr = 0;
HI_U8 *pu8VirAddr = NULL;
HI_SL slFileSize = 0;

/*打开网络模型文件，即*.wk文件， 再获取文件大小*/   
FILE *fp=fopen(pszModelFile,&amp;quot;rb&amp;quot;);  
s32Ret = fseek(fp,0L,SEEK_END); // 文件指针指向文件尾
slFileSize = ftell(fp);  // 获取文件字节大小
s32Ret = fseek(fp,0L,SEEK_SET);// 再将文件指针指向文件头

/*malloc model file mem，根据文件大小计算需分配的物理地址及虚拟地址大小*/
s32Ret = SAMPLE_COMM_SVP_MallocMem(&amp;quot;SAMPLE_NNIE_MODEL&amp;quot;,NULL,(HI_U64*)&amp;amp;u64PhyAddr,(void**)&amp;amp;pu8VirAddr,slFileSize);

pstNnieModel-&amp;gt;stModelBuf.u32Size = (HI_U32)slFileSize;  /*文件大小*/
pstNnieModel-&amp;gt;stModelBuf.u64PhyAddr = u64PhyAddr;       /*物理地址*/
pstNnieModel-&amp;gt;stModelBuf.u64VirAddr = (HI_U64)pu8VirAddr;/*虚拟地址*/

/*读取整个wk文件到虚拟地址*/
s32Ret = fread(pu8VirAddr, slFileSize, 1, fp);
SAMPLE_SVP_CHECK_EXPR_GOTO(1 != s32Ret,FAIL_1,SAMPLE_SVP_ERR_LEVEL_ERROR,
    &amp;quot;Error,read model file failed!\n&amp;quot;);

/*load model，从wk文件数据buf 中的模型中解析出网络模型*/
s32Ret = HI_MPI_SVP_NNIE_LoadModel(&amp;amp;pstNnieModel-&amp;gt;stModelBuf/*输入：模型数据buf*/,
&amp;amp;pstNnieModel-&amp;gt;stModel/*输出：网络模型结构体*/
);

fclose(fp);
return HI_SUCCESS;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;FAIL_1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SAMPLE_SVP_MMZ_FREE(pstNnieModel-&amp;gt;stModelBuf.u64PhyAddr,pstNnieModel-&amp;gt;stModelBuf.u64VirAddr);
pstNnieModel-&amp;gt;stModelBuf.u32Size  = 0;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;FAIL_0:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if (NULL != fp)
{
    fclose(fp);
}

return HI_FAILURE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;br /&gt;
这个函数执行以下步骤：&lt;/p&gt;

&lt;p&gt;获取wk文件字节大小&lt;br /&gt;
分配存储wk文件的内存空间&lt;br /&gt;
读取wk文件到内存空间&lt;br /&gt;
从wk文件的内存buf中解析出网络模型信息&lt;br /&gt;
执行完后，模型信息在s_stFasterRcnnModel.stModel结构体里，这个结构体里存储的是什么信息，可参考我上一篇文章（海思NNIE开发二），这里简单罗列各个段、输入输出节点的信息如下：&lt;/p&gt;

&lt;p&gt;段   段类型/段类型值    输入/输出   节点名 节点类型/节点类型值&lt;br /&gt;
第1段&lt;/p&gt;

&lt;p&gt;SVP_NNIE_NET_TYPE_CNN/0 输入  data    SVP_BLOB_TYPE_S32/0&lt;br /&gt;
输出  conv5   SVP_BLOB_TYPE_S32/0&lt;br /&gt;
rpn_cls_score   SVP_BLOB_TYPE_S32/0&lt;br /&gt;
rpn_bbox_pred   SVP_BLOB_TYPE_S32/0&lt;br /&gt;
rpn_cls_prob_reshape    SVP_BLOB_TYPE_S32/0&lt;br /&gt;
第2段&lt;/p&gt;

&lt;p&gt;SVP_NNIE_NET_TYPE_ROI/1 输入  conv5   SVP_BLOB_TYPE_S32/0&lt;br /&gt;
输出  bbox_pred   SVP_BLOB_TYPE_VEC_S32/4&lt;br /&gt;
cls_prob    SVP_BLOB_TYPE_VEC_S32/4&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;初始化&lt;br /&gt;
解析完网络模型信息之后，结构体指针给到 SAMPLE_SVP_NNIE_PARAM_S s_stFasterRcnnNnieParam这个结构体中，如下：&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;s_stFasterRcnnNnieParam.pstModel = &amp;amp;s_stFasterRcnnModel.stModel;&lt;br /&gt;
我们接着看以下初始化函数：&lt;/p&gt;

&lt;p&gt;s32Ret = SAMPLE_SVP_NNIE_FasterRcnn_ParamInit(&lt;br /&gt;
&amp;amp;stNnieCfg,&lt;br /&gt;
&amp;amp;s_stFasterRcnnNnieParam,&lt;br /&gt;
&amp;amp;s_stFasterRcnnSoftwareParam);&lt;br /&gt;
这个函数里面执行稍复杂，简单来说就是使用stNnieCfg等信息来初始化s_stFasterRcnnNnieParam，再使用s_stFasterRcnnNnieParam等来初始化s_stFasterRcnnSoftwareParam。该函数的实现如下：&lt;/p&gt;

&lt;p&gt;static HI_S32 SAMPLE_SVP_NNIE_FasterRcnn_ParamInit(SAMPLE_SVP_NNIE_CFG_S* pstFasterRcnnCfg/&lt;em&gt;图片及框等信息&lt;/em&gt;/,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SAMPLE_SVP_NNIE_PARAM_S *pstNnieParam/*模型信息*/, SAMPLE_SVP_NNIE_FASTERRCNN_SOFTWARE_PARAM_S* pstSoftWareParam)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HI_S32 s32Ret = HI_SUCCESS;
/*init hardware parameter*/
s32Ret = SAMPLE_COMM_SVP_NNIE_ParamInit(pstFasterRcnnCfg,pstNnieParam);

/*init software parameter*/
s32Ret = SAMPLE_SVP_NNIE_FasterRcnn_SoftwareInit(
pstFasterRcnnCfg,
pstNnieParam,
    pstSoftWareParam);

return s32Ret;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;INIT_FAIL_0:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s32Ret = SAMPLE_SVP_NNIE_FasterRcnn_Deinit(pstNnieParam,pstSoftWareParam,NULL);

return HI_FAILURE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;br /&gt;
分为SAMPLE_SVP_NNIE_ParamInit与SAMPLE_SVP_NNIE_FasterRcnn_SoftwareInit两个函数。我们首先看SAMPLE_COMM_SVP_NNIE_ParamInit，这个函数的实现里做了一些输入参数的有效判断后，就直接调用SAMPLE_SVP_NNIE_ParamInit，因此我们就直接看SAMPLE_SVP_NNIE_ParamInit的实现，在这个函数里首先调用：&lt;/p&gt;

&lt;p&gt;s32Ret = SAMPLE_SVP_NNIE_FillForwardInfo(pstNnieCfg,pstNnieParam);&lt;br /&gt;
这个函数的实质就是使用pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg的信息来初始化pstNnieParam-&amp;gt;astForwardWithBboxCtrl与pstNnieParam-&amp;gt;astSegData这两个结构体，其实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;static HI_S32 SAMPLE_SVP_NNIE_FillForwardInfo(
	SAMPLE_SVP_NNIE_CFG_S *pstNnieCfg/*图片及框等信息*/,
	SAMPLE_SVP_NNIE_PARAM_S *pstNnieParam/*模型信息*/)
{
	HI_U32 i = 0, j = 0;
	HI_U32 u32Offset = 0;
	HI_U32 u32Num = 0;
	/*u32NetSegNum:网络模型中 NNIE 执行的网络分段数，在FasterRCNN中为2*/	
	for(i = 0; i &amp;lt; pstNnieParam-&amp;gt;pstModel-&amp;gt;u32NetSegNum; i++)
	{       
		if(SVP_NNIE_NET_TYPE_ROI == pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].enNetType)/*网络段的类型,SVP_NNIE_NET_TYPE_ROI为1*/
		{
			/*astForwardWithBboxCtrl:有 Bbox 输入的目标检测网络预测控制参数*/
			pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].enNnieId = pstNnieCfg-&amp;gt;aenNnieCoreId[i];//网络段的段序号,初始化时设置为0
			pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].u32SrcNum = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].u16SrcNum;//网络段的输入节点数，这里为1
			pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].u32DstNum = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].u16DstNum;//网络段的输出节点数，这里为2
			pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].u32ProposalNum = 1;
			pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].u32NetSegId = i;//网络段的段序号，这里为1
			pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].stTmpBuf = pstNnieParam-&amp;gt;stTmpBuf;// 辅助内存
			pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].stTskBuf.u64PhyAddr= pstNnieParam-&amp;gt;stTaskBuf.u64PhyAddr+u32Offset;// 内存块物理地址
			pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].stTskBuf.u64VirAddr= pstNnieParam-&amp;gt;stTaskBuf.u64VirAddr+u32Offset; // 内存块虚拟地址
			pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].stTskBuf.u32Size= pstNnieParam-&amp;gt;au32TaskBufSize[i];  // 内存块字节数
		}
		else if(SVP_NNIE_NET_TYPE_CNN == pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].enNetType ||
            SVP_NNIE_NET_TYPE_RECURRENT== pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].enNetType)
		{
			pstNnieParam-&amp;gt;astForwardCtrl[i].enNnieId = pstNnieCfg-&amp;gt;aenNnieCoreId[i];//网络段的段序号,初始化时设置为0
			pstNnieParam-&amp;gt;astForwardCtrl[i].u32SrcNum = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].u16SrcNum;//网络段的输入节点数，这里为1
			pstNnieParam-&amp;gt;astForwardCtrl[i].u32DstNum = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].u16DstNum;//网络段的输出节点数，这里为4
			pstNnieParam-&amp;gt;astForwardCtrl[i].u32NetSegId = i;//网络段的段序号，这里为0
			pstNnieParam-&amp;gt;astForwardCtrl[i].stTmpBuf = pstNnieParam-&amp;gt;stTmpBuf; // 辅助内存，这里为0
			pstNnieParam-&amp;gt;astForwardCtrl[i].stTskBuf.u64PhyAddr= pstNnieParam-&amp;gt;stTaskBuf.u64PhyAddr+u32Offset; // 内存块物理地址，这里为0
			pstNnieParam-&amp;gt;astForwardCtrl[i].stTskBuf.u64VirAddr= pstNnieParam-&amp;gt;stTaskBuf.u64VirAddr+u32Offset; // 内存块虚拟地址，这里为0
			pstNnieParam-&amp;gt;astForwardCtrl[i].stTskBuf.u32Size= pstNnieParam-&amp;gt;au32TaskBufSize[i]; // 内存块字节数，这里为0
		}
		u32Offset += pstNnieParam-&amp;gt;au32TaskBufSize[i];// 网络任务各段辅助内存，这里为0
 
        /*fill src blob info，从pstModel中获取每一段的输入节点信息，存储于astSegData[i].astSrc*/
		for(j = 0; j &amp;lt; pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].u16SrcNum; j++)// 第i段的第j个输入节点，FasterRCNN中每段都只有1个输入节点
	    {
            /*FasterRCNN中第1段与第2段的第1个输入节点类型为SVP_BLOB_TYPE_U8*/
            if(SVP_BLOB_TYPE_SEQ_S32 == pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astSrcNode[j].enType) // 0x5类型
            {
                pstNnieParam-&amp;gt;astSegData[i].astSrc[j].enType = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astSrcNode[j].enType;
                pstNnieParam-&amp;gt;astSegData[i].astSrc[j].unShape.stSeq.u32Dim = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astSrcNode[j].unShape.u32Dim;
                pstNnieParam-&amp;gt;astSegData[i].astSrc[j].u32Num = pstNnieCfg-&amp;gt;u32MaxInputNum; // 1
                pstNnieParam-&amp;gt;astSegData[i].astSrc[j].unShape.stSeq.u64VirAddrStep = pstNnieCfg-&amp;gt;au64StepVirAddr[i*SAMPLE_SVP_NNIE_EACH_SEG_STEP_ADDR_NUM];
            }
            else
            {
    		    pstNnieParam-&amp;gt;astSegData[i].astSrc[j].enType = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astSrcNode[j].enType; // 节点类型，这里为SVP_BLOB_TYPE_U8
    	        pstNnieParam-&amp;gt;astSegData[i].astSrc[j].unShape.stWhc.u32Chn = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astSrcNode[j].unShape.stWhc.u32Chn; //节点输入通道数
    	        pstNnieParam-&amp;gt;astSegData[i].astSrc[j].unShape.stWhc.u32Height = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astSrcNode[j].unShape.stWhc.u32Height; // 节点输入的高度
    	        pstNnieParam-&amp;gt;astSegData[i].astSrc[j].unShape.stWhc.u32Width = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astSrcNode[j].unShape.stWhc.u32Width; // 节点输入的宽度
    	        pstNnieParam-&amp;gt;astSegData[i].astSrc[j].u32Num = pstNnieCfg-&amp;gt;u32MaxInputNum; // 1
            }
	    }
        /* FasterRCNN中第1段的类型为SVP_NNIE_NET_TYPE_CNN， 第2段的类型为SVP_NNIE_NET_TYPE_ROI
		* u32MaxRoiNum为300，u32MaxInputNum为1 */
		if(SVP_NNIE_NET_TYPE_ROI == pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].enNetType) // 0x1
		{
			u32Num = pstNnieCfg-&amp;gt;u32MaxRoiNum*pstNnieCfg-&amp;gt;u32MaxInputNum; // 300
		}
		else
		{
			u32Num = pstNnieCfg-&amp;gt;u32MaxInputNum; // 这里为1
		}
		// FasterRcnn第1段有4个输出节点，都是SVP_BLOB_TYPE_S32类型；第2段有2个输出节点，都是SVP_BLOB_TYPE_VEC_S32类型
		for(j = 0; j &amp;lt; pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].u16DstNum; j++)// 第i段的第j个输出节点
		{
            if(SVP_BLOB_TYPE_SEQ_S32 == pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astDstNode[j].enType)// 0x5类型
            {
    			pstNnieParam-&amp;gt;astSegData[i].astDst[j].enType = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astDstNode[j].enType;
    			pstNnieParam-&amp;gt;astSegData[i].astDst[j].unShape.stSeq.u32Dim =
                    pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astDstNode[j].unShape.u32Dim;
                pstNnieParam-&amp;gt;astSegData[i].astDst[j].u32Num = u32Num;
                pstNnieParam-&amp;gt;astSegData[i].astDst[j].unShape.stSeq.u64VirAddrStep =
                    pstNnieCfg-&amp;gt;au64StepVirAddr[i*SAMPLE_SVP_NNIE_EACH_SEG_STEP_ADDR_NUM+1];
            }
            else
            {
    		    pstNnieParam-&amp;gt;astSegData[i].astDst[j].enType = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astDstNode[j].enType;// 节点类型，
    		    pstNnieParam-&amp;gt;astSegData[i].astDst[j].unShape.stWhc.u32Chn = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astDstNode[j].unShape.stWhc.u32Chn; // 通道
    		    pstNnieParam-&amp;gt;astSegData[i].astDst[j].unShape.stWhc.u32Height = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astDstNode[j].unShape.stWhc.u32Height;// 高度
    		    pstNnieParam-&amp;gt;astSegData[i].astDst[j].unShape.stWhc.u32Width = pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astDstNode[j].unShape.stWhc.u32Width;// 宽度
    		    pstNnieParam-&amp;gt;astSegData[i].astDst[j].u32Num = u32Num; // 第1段NNIE网络为1，第2段NNIE网络为300
            }
		}
	}
	return HI_SUCCESS;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个函数还对pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].stTskBuf、pstNnieParam-&amp;gt;astForwardCtrl[i].stTskBuf、pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].stTmpBuf、pstNnieParam-&amp;gt;astForwardCtrl[i].stTmpBuf等参数进行初始化，其实这里没有必要，因为pstNnieParam-&amp;gt;stTaskBuf、pstNnieParam-&amp;gt;stTmpBuf等结构体的值也是空的，并没有做过申请内存的操作。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;我们再来看第2个关键函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;/*1. 计算网络各段的辅助内存大小
2. 计算第1段第1个输入节点的Blob的辅助内存大小
3. 计算各段第1个输出节点的Blob的辅助内存大小
*/
	s32Ret = SAMPLE_SVP_NNIE_GetTaskAndBlobBufSize(pstNnieCfg,pstNnieParam,
	&amp;amp;u32TotalTaskBufSize,    /*输入&amp;amp;输出：输入值为0; 输出：网络各段辅助内存的总和*/
		&amp;amp;u32TmpBufSize,      /*输入&amp;amp;输出，输入值为0; 输出：模型辅助内存大小*/
		astBlobSize,        /*输入&amp;amp;输出：输入为空;  输出：各段第1个输入、输出节点辅助内存*/
		&amp;amp;u32TotalSize        /*输入&amp;amp;输出：输入值为0; 输出为：段辅助内存+模型辅助内存+第1段第1个输入节点辅助内存+各段第1个输出节点辅助内存大小*/);
这个函数是计算各个段、各个段中的各个节点的的辅助内存大小。我们知道，在之前的load模型的步骤中，是已经获取到模型的辅助内存（pstNnieParam-&amp;gt;pstModel-&amp;gt;u32TmpBufSize），但各段、段中各个节点的辅助内存是不知道的，因此该函数就是获取这些辅助内存。该函数的实现如下：

static HI_S32 SAMPLE_SVP_NNIE_GetTaskAndBlobBufSize(SAMPLE_SVP_NNIE_CFG_S *pstNnieCfg,
    SAMPLE_SVP_NNIE_PARAM_S *pstNnieParam,
	HI_U32*pu32TotalTaskBufSize,/*输入&amp;amp;输出：输入值为0，输出：网络各段辅助内存的总和*/
	HI_U32*pu32TmpBufSize,/*输入&amp;amp;输出， 输入值为0， 输出：模型辅助内存大小*/
    SAMPLE_SVP_NNIE_BLOB_SIZE_S astBlobSize[], /*输入&amp;amp;输出：输入为空；输出：各段第1个输入、输出节点辅助内存*/
	HI_U32*pu32TotalSize/*输入&amp;amp;输出：输入值为0， 输出为：段辅助内存+模型辅助内存+第1段第1个输入节点辅助内存+各段第1个输出节点辅助内存大小*/)
{
	HI_S32 s32Ret = HI_SUCCESS;
	HI_U32 i = 0, j = 0;
    HI_U32 u32TotalStep = 0;
 
	/*Get each seg&#39;s task buf size*//*获取给定网络任务各段辅助内存大小*/
	s32Ret = HI_MPI_SVP_NNIE_GetTskBufSize(pstNnieCfg-&amp;gt;u32MaxInputNum/*图片数量：1*/,
		pstNnieCfg-&amp;gt;u32MaxRoiNum,// 输入，300
		pstNnieParam-&amp;gt;pstModel,// 输入
		pstNnieParam-&amp;gt;au32TaskBufSize,// 输出：网络任务各段辅助内存
		pstNnieParam-&amp;gt;pstModel-&amp;gt;u32NetSegNum);// 输入：网络任务的段数
	SAMPLE_SVP_CHECK_EXPR_RET(HI_SUCCESS != s32Ret,s32Ret,SAMPLE_SVP_ERR_LEVEL_ERROR,
		&amp;quot;Error,HI_MPI_SVP_NNIE_GetTaskSize failed!\n&amp;quot;);
 
    /*Get total task buf size*/
	*pu32TotalTaskBufSize = 0;
	for(i = 0; i &amp;lt; pstNnieParam-&amp;gt;pstModel-&amp;gt;u32NetSegNum; i++)
	{
		*pu32TotalTaskBufSize += pstNnieParam-&amp;gt;au32TaskBufSize[i]; /*累加网络任务各段辅助内存*/
	}
 
	/*Get tmp buf size*/
	*pu32TmpBufSize = pstNnieParam-&amp;gt;pstModel-&amp;gt;u32TmpBufSize; // 模型辅助内存大小
	*pu32TotalSize += *pu32TotalTaskBufSize + *pu32TmpBufSize;// 段辅助内存+模型辅助内存
 
	/*calculate Blob mem size*/
	for(i = 0; i &amp;lt; pstNnieParam-&amp;gt;pstModel-&amp;gt;u32NetSegNum; i++)
	{
        if(SVP_NNIE_NET_TYPE_RECURRENT == pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].enNetType)
        {
            for(j = 0; j &amp;lt; pstNnieParam-&amp;gt;astSegData[i].astSrc[0].u32Num; j++)
            {
                u32TotalStep += *((HI_S32*)pstNnieParam-&amp;gt;astSegData[i].astSrc[0].unShape.stSeq.u64VirAddrStep+j);
            }
        }
		/*the first seg&#39;s Src Blob mem size, other seg&#39;s src blobs from the output blobs of
		those segs before it or from software output results*/
		if(i == 0)
		{
			/*计算第1段第1个输入节点的Blob的辅助内存大小*/
			SAMPLE_SVP_NNIE_GetBlobMemSize(
			&amp;amp;(pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astSrcNode[0]), /*输入，第i段的第1个输入节点信息*/
				pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].u16SrcNum,  /*输入，这里是1*/
				u32TotalStep,/*输入，这里是0*/
				&amp;amp;(pstNnieParam-&amp;gt;astSegData[i].astSrc[0]),/*第i段的第1个节点信息，在SAMPLE_SVP_NNIE_FillForwardInfo中已填充部分该结构体部分信息*/
				SAMPLE_SVP_NNIE_ALIGN_16, /*输入：内存对齐方式*/
				pu32TotalSize,/*输入&amp;amp;输出：输入为：段辅助内存+模型辅助内存；输出为：段辅助内存+模型辅助内存+输入节点辅助内存*/
				&amp;amp;(astBlobSize[i].au32SrcSize[0])/*输入&amp;amp;输出：输入为空；输出为：各个节点的Blob的辅助内存大小*/));
		}
 
		/*Get each seg&#39;s Dst Blob mem size*/
		/*计算第1个输出节点的Blob的辅助内存大小*/
		SAMPLE_SVP_NNIE_GetBlobMemSize(&amp;amp;(pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].astDstNode[0]),
			pstNnieParam-&amp;gt;pstModel-&amp;gt;astSeg[i].u16DstNum,u32TotalStep,&amp;amp;(pstNnieParam-&amp;gt;astSegData[i].astDst[0]),
			SAMPLE_SVP_NNIE_ALIGN_16, pu32TotalSize, &amp;amp;(astBlobSize[i].au32DstSize[0]));
	}
	return s32Ret;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个函数中，首先调用底层API  HI_MPI_SVP_NNIE_GetTskBufSize获取到网络任务的各段的辅助内存pstNnieParam-&amp;gt;au32TaskBufSize，然后再调用SAMPLE_SVP_NNIE_GetBlobMemSize计算第1段的第1个输入节点Blob的辅助内存，以及每段的第1个输出节点的Blob辅助内存。&lt;/p&gt;

&lt;p&gt;回到SAMPLE_SVP_NNIE_ParamInit函数中，SAMPLE_SVP_NNIE_GetTaskAndBlobBufSize执行完后，u32TotalSize为总的辅助内存大小（含模型、段、节点），此时调用：&lt;/p&gt;

&lt;p&gt;s32Ret = SAMPLE_COMM_SVP_MallocCached(&amp;quot;SAMPLE_NNIE_TASK&amp;quot;,NULL,(HI_U64*)&amp;amp;u64PhyAddr,(void**)&amp;amp;pu8VirAddr,u32TotalSize);&lt;br /&gt;
分配内存空间。接着后面，再根据得到的虚拟内存地址、物理内存地址来初始化pstNnieParam-&amp;gt;stTaskBuf、pstNnieParam-&amp;gt;stTmpBuf、pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].stTmpBuf、pstNnieParam-&amp;gt;astForwardWithBboxCtrl[i].stTskBuf、stNnieParam-&amp;gt;astForwardCtrl[i].stTskBuf、stNnieParam-&amp;gt;astSegData[i].astSrc[j]这些结构体中的内存地址值，这个才是真正的初始化，之前在SAMPLE_SVP_NNIE_FillForwardInfo函数中也有对这些结构体做初始化，但那是“假初始化”。&lt;/p&gt;

&lt;p&gt;此致，SAMPLE_SVP_NNIE_ParamInit函数执行完毕。&lt;br /&gt;
————————————————&lt;br /&gt;
版权声明：本文为CSDN博主「夜风里唱」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。&lt;br /&gt;
原文链接：&lt;a href=&#34;https://blog.csdn.net/zh8706/article/details/98031231&#34; target=&#34;_blank&#34;&gt;https://blog.csdn.net/zh8706/article/details/98031231&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>nnie yolov3 svp</title>
            <link>/hardware/hisilicon/nnie/nnie-yolov3-svp/</link>
            <pubDate>Tue, 10 Dec 2019 16:11:29 CST</pubDate>
            <author>rinetd</author>
            <guid>/hardware/hisilicon/nnie/nnie-yolov3-svp/</guid>
            <description>&lt;p&gt;将自己的网络放到HI3559A上运行&lt;br /&gt;
 &lt;/p&gt;

&lt;p&gt;前言：在上一篇中我们已经可以根据仿真的结果看到自己转化后的模型的效果是什么样的，此次我们来将这个结果在海思处理器上复现出来&lt;/p&gt;

&lt;p&gt;step1.&lt;br /&gt;
在ubuntu上进入SDK目录下的mpp/sample/svp/big-little，并将自己的wk文件与bgr图像放入nnie/data中&lt;/p&gt;

&lt;p&gt;注意可以在板子上运行的wk文件只能是指令仿真的wk文件（inst），功能性仿真的文件（func）不能被板子载入&lt;/p&gt;

&lt;p&gt;step2.&lt;br /&gt;
进入nnie/sample文件夹，打开并编辑sample_nnie.c 文件,找到下面这两部分，按照自己的需求进行更改&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;    pstSoftWareParam-&amp;gt;u32OriImHeight = pstNnieParam-&amp;gt;astSegData[0].astSrc[0].unShape.stWhc.u32Height;
    pstSoftWareParam-&amp;gt;u32OriImWidth = pstNnieParam-&amp;gt;astSegData[0].astSrc[0].unShape.stWhc.u32Width;
    pstSoftWareParam-&amp;gt;u32BboxNumEachGrid = 3;
    pstSoftWareParam-&amp;gt;u32ClassNum = 5;       //类的数目
    pstSoftWareParam-&amp;gt;au32GridNumHeight[0] = 19;  //图片大小/32 (例：608/32)
    pstSoftWareParam-&amp;gt;au32GridNumHeight[1] = 38;  //图片大小/16
    pstSoftWareParam-&amp;gt;au32GridNumHeight[2] = 76;  //图片大小/8
    pstSoftWareParam-&amp;gt;au32GridNumWidth[0] = 19;
    pstSoftWareParam-&amp;gt;au32GridNumWidth[1] = 38;
    pstSoftWareParam-&amp;gt;au32GridNumWidth[2] = 76;
    pstSoftWareParam-&amp;gt;u32NmsThresh = (HI_U32)(0.3f*SAMPLE_SVP_NNIE_QUANT_BASE);
    pstSoftWareParam-&amp;gt;u32ConfThresh = (HI_U32)(0.5f*SAMPLE_SVP_NNIE_QUANT_BASE);
    pstSoftWareParam-&amp;gt;u32MaxRoiNum = 15;     //每个图片最多由多少个目标
    /*以下为模型的anchor,具体计算方法请自行查询，不更改也可*/
    pstSoftWareParam-&amp;gt;af32Bias[0][0] = 116; 
    pstSoftWareParam-&amp;gt;af32Bias[0][1] = 90;
    pstSoftWareParam-&amp;gt;af32Bias[0][2] = 156;
    pstSoftWareParam-&amp;gt;af32Bias[0][3] = 198;
    pstSoftWareParam-&amp;gt;af32Bias[0][4] = 373;
    pstSoftWareParam-&amp;gt;af32Bias[0][5] = 326;
    pstSoftWareParam-&amp;gt;af32Bias[1][0] = 30;
    pstSoftWareParam-&amp;gt;af32Bias[1][1] = 61;
    pstSoftWareParam-&amp;gt;af32Bias[1][2] = 62;
    pstSoftWareParam-&amp;gt;af32Bias[1][3] = 45;
    pstSoftWareParam-&amp;gt;af32Bias[1][4] = 59;
    pstSoftWareParam-&amp;gt;af32Bias[1][5] = 119;
    pstSoftWareParam-&amp;gt;af32Bias[2][0] = 10;
    pstSoftWareParam-&amp;gt;af32Bias[2][1] = 13;
    pstSoftWareParam-&amp;gt;af32Bias[2][2] = 16;
    pstSoftWareParam-&amp;gt;af32Bias[2][3] = 30;
    pstSoftWareParam-&amp;gt;af32Bias[2][4] = 33;
    pstSoftWareParam-&amp;gt;af32Bias[2][5] = 23;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;step3.&lt;br /&gt;
重新返回到big-little 目录下，进行make，make完成后会在nnie中生成一个可执行的文件，就是我们有个nnie编译出来的结果。&lt;/p&gt;

&lt;p&gt;将整个big-little文件拷贝到板子上，最好是使用nfs挂载上硬盘，也可以使用其他方式拷贝过去。&lt;/p&gt;

&lt;p&gt;板子上电后需要先加载各部分的驱动文件，也就是load  ko文件，具体上电流程看此贴：&lt;/p&gt;

&lt;p&gt;3559A上电后需要做什么：&lt;a href=&#34;https://blog.csdn.net/qq_34533248/article/details/102502038&#34; target=&#34;_blank&#34;&gt;https://blog.csdn.net/qq_34533248/article/details/102502038&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;将文件拷贝过去之后，进去nnie文件，使用以下命令完成首次推理。&lt;/p&gt;

&lt;p&gt;./sample_nnie_main 8&lt;br /&gt;
————————————————&lt;br /&gt;
版权声明：本文为CSDN博主「他们叫我高老师」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。&lt;br /&gt;
原文链接：&lt;a href=&#34;https://blog.csdn.net/qq_34533248/article/details/102498143&#34; target=&#34;_blank&#34;&gt;https://blog.csdn.net/qq_34533248/article/details/102498143&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>nnie yolov3 simulator</title>
            <link>/hardware/hisilicon/nnie/nnie-yolov3-simulator/</link>
            <pubDate>Tue, 10 Dec 2019 16:07:32 CST</pubDate>
            <author>rinetd</author>
            <guid>/hardware/hisilicon/nnie/nnie-yolov3-simulator/</guid>
            <description>&lt;p&gt;完成网络的仿真与仿真代码简要分析&lt;br /&gt;
前言：&lt;/p&gt;

&lt;p&gt;上一篇我们已经得到了转化完成的.wk网络结构文件，此次我们来简要讲述一下仿真的过程&lt;/p&gt;

&lt;p&gt;Ruyi Stdio的仿真过程分为两种，一种是功能性的仿真，一种是指令仿真。顾名思义，功能性的仿真只在乎结果对不对，而指令性的仿真在乎的是每个过程，每次卷积的结果都是符合要求的。最终我们要在海思处理器上跑起来的就是指令性仿真过程中所用到的权重文件。&lt;/p&gt;

&lt;p&gt;注：在转化的过程中选择功能性仿真与指令仿真最后生成的权重是有差别的，如果要进行功能性仿真，那在转化的时候就要选择功能性的仿真。&lt;/p&gt;

&lt;p&gt;本文章以功能性的仿真为例。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;step1.&lt;br /&gt;
首先，查看scr/main.cpp,可以发现里面包含多种网络的推理过程，可以将其他网络进行屏蔽，如下所示：&lt;/p&gt;

&lt;p&gt; &lt;br /&gt;
step2.&lt;br /&gt;
打开SvpSampleDetectionOneSeg.cpp文件，可以看到里面包含了各种文件所在的地址，按照自己的需求选择各种文件。如下&lt;/p&gt;

&lt;p&gt;由于海思处理器所需要的图片文件类型不是标准格式，而是按照BBB...GGG...RRR格式排列的二进制文件。此地博主写了一个简单的python脚本完成jpg到bgr的转化，&lt;/p&gt;

&lt;p&gt;import cv2&lt;/p&gt;

&lt;p&gt;imgpath = &amp;quot;./BGR_img/917.jpg&amp;quot;&lt;br /&gt;
saveimg = r&amp;quot;./BGR_img/917_608x608.bgr&amp;quot;&lt;/p&gt;

&lt;p&gt;img = cv2.imread(imgpath)&lt;br /&gt;
save_img_size = 608&lt;/p&gt;

&lt;p&gt;if img is None:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print(&amp;quot;img is none&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;else:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;img = cv2.resize(img,(save_img_size,save_img_size))
(B, G, R) = cv2.split(img)
with open(saveimg,&#39;wb&#39;)as fp:
    for i in range(save_img_size):
        for j in range(save_img_size):
            fp.write(B[i, j])
    for i in range(save_img_size):
        for j in range(save_img_size):
            fp.write(G[i, j])
    for i in range(save_img_size):
        for j in range(save_img_size):
            fp.write(R[i, j])

print(&amp;quot;save success&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;博主在下载频道也上传了相应的bgr转jpg与jpg 转bgr的文件，需要请自提&lt;/p&gt;

&lt;p&gt;step3.&lt;br /&gt;
打开文件SvpSampleYolo.h 找到其中有关yolo3的部分，按以下文件注释修改各个参数，&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;//#define SVP_SAMPLE_YOLOV3_SRC_WIDTH                (416)
//#define SVP_SAMPLE_YOLOV3_SRC_HEIGHT               (416)
//
//#define SVP_SAMPLE_YOLOV3_GRIDNUM_CONV_82          (13)
//#define SVP_SAMPLE_YOLOV3_GRIDNUM_CONV_94          (26)
//#define SVP_SAMPLE_YOLOV3_GRIDNUM_CONV_106         (52)
//#define SVP_SAMPLE_YOLOV3_RESULT_BLOB_NUM          (3)
//#define SVP_SAMPLE_YOLOV3_CHANNLENUM               (255)
//#define SVP_SAMPLE_YOLOV3_PARAMNUM                 (85)
//#define SVP_SAMPLE_YOLOV3_BOXNUM                   (3)
//#define SVP_SAMPLE_YOLOV3_CLASSNUM                 (80)
//#define SVP_SAMPLE_YOLOV3_MAX_BOX_NUM              (10)

#define SVP_SAMPLE_YOLOV3_SRC_WIDTH                (608)
#define SVP_SAMPLE_YOLOV3_SRC_HEIGHT               (608)
 
#define SVP_SAMPLE_YOLOV3_GRIDNUM_CONV_82          (19) //a = 608÷32 //参数值为图片大小除以32
#define SVP_SAMPLE_YOLOV3_GRIDNUM_CONV_94          (38) //b = a*2  //参数值为上一行参数乘2
#define SVP_SAMPLE_YOLOV3_GRIDNUM_CONV_106         (76)	//c = b*2  //值为上一行参数乘以2
#define SVP_SAMPLE_YOLOV3_RESULT_BLOB_NUM          (3)
#define SVP_SAMPLE_YOLOV3_CHANNLENUM               (30) //值为  3x(5+ClassNum)
#define SVP_SAMPLE_YOLOV3_PARAMNUM                 (10) //值为  (5+ClassNum)
#define SVP_SAMPLE_YOLOV3_BOXNUM                   (3)
#define SVP_SAMPLE_YOLOV3_CLASSNUM                 (5)
#define SVP_SAMPLE_YOLOV3_MAX_BOX_NUM              (15)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;step4.&lt;br /&gt;
以上全部修改并保存后，右键单击项目名称，切换为功能仿真，如下&lt;/p&gt;

&lt;p&gt;随后点击build按钮，选择release&lt;/p&gt;

&lt;p&gt;等待完成，大概耗时一分钟左右。&lt;/p&gt;

&lt;p&gt; &lt;br /&gt;
step5.&lt;br /&gt;
打开release文件夹，右键单击可执行文件，选择run as -- local&lt;/p&gt;

&lt;p&gt;开始进行仿真，速度取决于电脑性能，完成之后可以在sim_out -- result文件夹看到输出的图片。&lt;/p&gt;

&lt;p&gt;到此，就完成了一次功能性的仿真，如果RuyiStdio用不舒服的话可以选择使用VS进行上面的修改与运行，原sample也提供了vs的解决方案&lt;/p&gt;

&lt;p&gt; &lt;br /&gt;
————————————————&lt;br /&gt;
版权声明：本文为CSDN博主「他们叫我高老师」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。&lt;br /&gt;
原文链接：&lt;a href=&#34;https://blog.csdn.net/qq_34533248/article/details/102497297&#34; target=&#34;_blank&#34;&gt;https://blog.csdn.net/qq_34533248/article/details/102497297&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>nnie yolov3</title>
            <link>/hardware/hisilicon/nnie/nnie-yolov3/</link>
            <pubDate>Tue, 10 Dec 2019 15:46:04 CST</pubDate>
            <author>rinetd</author>
            <guid>/hardware/hisilicon/nnie/nnie-yolov3/</guid>
            <description>&lt;p&gt;开发板中新更新了yolov3的demo，那么从demo中把NNIE相关的截取出来和大家分享一下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;/******************************************************************************
* function : ive sample
******************************************************************************/
#ifdef __HuaweiLite__
int app_main(int argc, char *argv[])
#else
int main(int argc, char *argv[])
#endif
{
    int s32Ret = HI_SUCCESS;
 
 
    s_ppChCmdArgv = argv;
 
 
    SAMPLE_SVP_NNIE_Yolov3();
 
    return s32Ret;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主函数很简单，直接跑了demo，读取图片后进行物体识别。&lt;/p&gt;

&lt;p&gt;具体函数放在 ../深度学习demo\yolov3\yolov3\sample 中  sample_nnie.c。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;/******************************************************************************
* function : show YOLOV3 sample(image 416x416 U8_C3)
******************************************************************************/
void SAMPLE_SVP_NNIE_Yolov3(void)
{
    HI_CHAR *pcSrcFile = &amp;quot;./data/nnie_image/rgb_planar/dog_bike_car_416x416.bgr&amp;quot;;
    HI_CHAR *pcModelName = &amp;quot;./data/nnie_model/detection/inst_yolov3_cycle.wk&amp;quot;;
    HI_U32 u32PicNum = 1;
    HI_FLOAT f32PrintResultThresh = 0.0f;
    HI_S32 s32Ret = HI_SUCCESS;
    SAMPLE_SVP_NNIE_CFG_S   stNnieCfg = {0};
    SAMPLE_SVP_NNIE_INPUT_DATA_INDEX_S stInputDataIdx = {0};
    SAMPLE_SVP_NNIE_PROCESS_SEG_INDEX_S stProcSegIdx = {0};
 
    /*Set configuration parameter*/
    f32PrintResultThresh = 0.15f;
    stNnieCfg.pszPic= pcSrcFile;
    stNnieCfg.u32MaxInputNum = u32PicNum; //max input image num in each batch
    stNnieCfg.u32MaxRoiNum = 0;
    stNnieCfg.aenNnieCoreId[0] = SVP_NNIE_ID_0;//set NNIE core
 
    /*Sys init*/
    SAMPLE_COMM_SVP_CheckSysInit();
 
    /*Yolov3 Load model*/
    SAMPLE_SVP_TRACE_INFO(&amp;quot;Yolov3 Load model!\n&amp;quot;);
    s32Ret = SAMPLE_COMM_SVP_NNIE_LoadModel(pcModelName,&amp;amp;s_stYolov3Model);
    SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,YOLOV3_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR,
        &amp;quot;Error,SAMPLE_COMM_SVP_NNIE_LoadModel failed!\n&amp;quot;);
 
    /*Yolov3 parameter initialization*/
    /*Yolov3 software parameters are set in SAMPLE_SVP_NNIE_Yolov3_SoftwareInit,
      if user has changed net struct, please make sure the parameter settings in
      SAMPLE_SVP_NNIE_Yolov3_SoftwareInit function are correct*/
    SAMPLE_SVP_TRACE_INFO(&amp;quot;Yolov3 parameter initialization!\n&amp;quot;);
    s_stYolov3NnieParam.pstModel = &amp;amp;s_stYolov3Model.stModel;
    s32Ret = SAMPLE_SVP_NNIE_Yolov3_ParamInit(&amp;amp;stNnieCfg,&amp;amp;s_stYolov3NnieParam,&amp;amp;s_stYolov3SoftwareParam);
    SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,YOLOV3_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR,
        &amp;quot;Error,SAMPLE_SVP_NNIE_Yolov3_ParamInit failed!\n&amp;quot;);
 
    /*Fill src data*/
    SAMPLE_SVP_TRACE_INFO(&amp;quot;Yolov3 start!\n&amp;quot;);
    stInputDataIdx.u32SegIdx = 0;
    stInputDataIdx.u32NodeIdx = 0;
    s32Ret = SAMPLE_SVP_NNIE_FillSrcData(&amp;amp;stNnieCfg,&amp;amp;s_stYolov3NnieParam,&amp;amp;stInputDataIdx);
    SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,YOLOV3_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR,
        &amp;quot;Error,SAMPLE_SVP_NNIE_FillSrcData failed!\n&amp;quot;);
 
    gettimeofday(&amp;amp;tp,NULL);
    g_time_start = tp.tv_sec * 1000 + tp.tv_usec/1000;
    /*NNIE process(process the 0-th segment)*/
    stProcSegIdx.u32SegIdx = 0;
    s32Ret = SAMPLE_SVP_NNIE_Forward(&amp;amp;s_stYolov3NnieParam,&amp;amp;stInputDataIdx,&amp;amp;stProcSegIdx,HI_TRUE);
    SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,YOLOV3_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR,
        &amp;quot;Error,SAMPLE_SVP_NNIE_Forward failed!\n&amp;quot;);
 
 
    /*Software process*/
    /*if user has changed net struct, please make sure SAMPLE_SVP_NNIE_Yolov3_GetResult
     function input datas are correct*/
    s32Ret = SAMPLE_SVP_NNIE_Yolov3_GetResult(&amp;amp;s_stYolov3NnieParam,&amp;amp;s_stYolov3SoftwareParam);
    SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,YOLOV3_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR,
        &amp;quot;Error,SAMPLE_SVP_NNIE_Yolov3_GetResult failed!\n&amp;quot;);
 
 
     /*print result, this sample has 81 classes:
      class 0:background      class 1:person       class 2:bicycle         class 3:car            class 4:motorbike      class 5:aeroplane
      class 6:bus             class 7:train        class 8:truck           class 9:boat           class 10:traffic light
      class 11:fire hydrant   class 12:stop sign   class 13:parking meter  class 14:bench         class 15:bird
      class 16:cat            class 17:dog         class 18:horse          class 19:sheep         class 20:cow
      class 21:elephant       class 22:bear        class 23:zebra          class 24:giraffe       class 25:backpack
      class 26:umbrella       class 27:handbag     class 28:tie            class 29:suitcase      class 30:frisbee
      class 31:skis           class 32:snowboard   class 33:sports ball    class 34:kite          class 35:baseball bat
      class 36:baseball glove class 37:skateboard  class 38:surfboard      class 39:tennis racket class 40bottle
      class 41:wine glass     class 42:cup         class 43:fork           class 44:knife         class 45:spoon
      class 46:bowl           class 47:banana      class 48:apple          class 49:sandwich      class 50orange
      class 51:broccoli       class 52:carrot      class 53:hot dog        class 54:pizza         class 55:donut
      class 56:cake           class 57:chair       class 58:sofa           class 59:pottedplant   class 60bed
      class 61:diningtable    class 62:toilet      class 63:vmonitor       class 64:laptop        class 65:mouse
      class 66:remote         class 67:keyboard    class 68:cell phone     class 69:microwave     class 70:oven
      class 71:toaster        class 72:sink        class 73:refrigerator   class 74:book          class 75:clock
      class 76:vase           class 77:scissors    class 78:teddy bear     class 79:hair drier    class 80:toothbrush*/
    SAMPLE_SVP_TRACE_INFO(&amp;quot;Yolov3 result:\n&amp;quot;);
    (void)SAMPLE_SVP_NNIE_Detection_PrintResult(&amp;amp;s_stYolov3SoftwareParam.stDstScore,
        &amp;amp;s_stYolov3SoftwareParam.stDstRoi, &amp;amp;s_stYolov3SoftwareParam.stClassRoiNum,f32PrintResultThresh);
 
    gettimeofday(&amp;amp;tp1,NULL);
    g_time_end = tp1.tv_sec * 1000 + tp1.tv_usec/1000;
 
    printf(&amp;quot;yolov3 time : %d ms .\n&amp;quot;, g_time_end-g_time_start);
 
YOLOV3_FAIL_0:
    SAMPLE_SVP_NNIE_Yolov3_Deinit(&amp;amp;s_stYolov3NnieParam,&amp;amp;s_stYolov3SoftwareParam,&amp;amp;s_stYolov3Model);
    SAMPLE_COMM_SVP_CheckSysExit();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;是采用完整版YOLOV3的416x416大小输入的模型。从中截取几个函数看一下。&lt;/p&gt;

&lt;p&gt;首先是初始化函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;/******************************************************************************
* function : Yolov3 init
******************************************************************************/
static HI_S32 SAMPLE_SVP_NNIE_Yolov3_ParamInit(SAMPLE_SVP_NNIE_CFG_S* pstCfg,
    SAMPLE_SVP_NNIE_PARAM_S *pstNnieParam, SAMPLE_SVP_NNIE_YOLOV3_SOFTWARE_PARAM_S* pstSoftWareParam)
{
    HI_S32 s32Ret = HI_SUCCESS;
    /*init hardware para*/
    s32Ret = SAMPLE_COMM_SVP_NNIE_ParamInit(pstCfg,pstNnieParam);
    SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,INIT_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR,
        &amp;quot;Error(%#x),SAMPLE_COMM_SVP_NNIE_ParamInit failed!\n&amp;quot;,s32Ret);
 
    /*init software para*/
    s32Ret = SAMPLE_SVP_NNIE_Yolov3_SoftwareInit(pstCfg,pstNnieParam,
        pstSoftWareParam);
    SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,INIT_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR,
        &amp;quot;Error(%#x),SAMPLE_SVP_NNIE_Yolov3_SoftwareInit failed!\n&amp;quot;,s32Ret);
 
    return s32Ret;
INIT_FAIL_0:
    s32Ret = SAMPLE_SVP_NNIE_Yolov3_Deinit(pstNnieParam,pstSoftWareParam,NULL);
    SAMPLE_SVP_CHECK_EXPR_RET(HI_SUCCESS != s32Ret,s32Ret,SAMPLE_SVP_ERR_LEVEL_ERROR,
            &amp;quot;Error(%#x),SAMPLE_SVP_NNIE_Yolov3_Deinit failed!\n&amp;quot;,s32Ret);
    return HI_FAILURE;
 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;YOLOV3的相关参数在 SAMPLE_SVP_NNIE_Yolov3_SoftwareInit 函数中进行设置：&lt;br /&gt;
```cpp&lt;br /&gt;
/******************************************************************************&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;function : Yolov3 software para init&lt;br /&gt;
*****************************************************************************&lt;em&gt;/&lt;br /&gt;
static HI_S32 SAMPLE_SVP_NNIE_Yolov3_SoftwareInit(SAMPLE_SVP_NNIE_CFG_S&lt;/em&gt; pstCfg,&lt;br /&gt;
SAMPLE_SVP_NNIE_PARAM_S &lt;em&gt;pstNnieParam, SAMPLE_SVP_NNIE_YOLOV3_SOFTWARE_PARAM_S&lt;/em&gt; pstSoftWareParam)&lt;br /&gt;
{&lt;br /&gt;
HI_S32 s32Ret = HI_SUCCESS;&lt;br /&gt;
HI_U32 u32ClassNum = 0;&lt;br /&gt;
HI_U32 u32TotalSize = 0;&lt;br /&gt;
HI_U32 u32DstRoiSize = 0;&lt;br /&gt;
HI_U32 u32DstScoreSize = 0;&lt;br /&gt;
HI_U32 u32ClassRoiNumSize = 0;&lt;br /&gt;
HI_U32 u32TmpBufTotalSize = 0;&lt;br /&gt;
HI_U64 u64PhyAddr = 0;&lt;br /&gt;
HI_U8* pu8VirAddr = NULL;&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;//  #define SVP_SAMPLE_YOLOV3_SRC_WIDTH                (608)&lt;br /&gt;
// #define SVP_SAMPLE_YOLOV3_SRC_HEIGHT               (608)&lt;/p&gt;

&lt;p&gt;// #define SVP_SAMPLE_YOLOV3_GRIDNUM_CONV_82          (19) //a = 608÷32 //参数值为图片大小除以32&lt;br /&gt;
// #define SVP_SAMPLE_YOLOV3_GRIDNUM_CONV_94          (38) //b = a*2  //参数值为上一行参数乘2&lt;br /&gt;
// #define SVP_SAMPLE_YOLOV3_GRIDNUM_CONV_106         (76)  //c = b*2  //值为上一行参数乘以2&lt;br /&gt;
// #define SVP_SAMPLE_YOLOV3_RESULT_BLOB_NUM          (3)&lt;br /&gt;
// #define SVP_SAMPLE_YOLOV3_CHANNLENUM               (30) //值为  3x(5+ClassNum)&lt;br /&gt;
// #define SVP_SAMPLE_YOLOV3_PARAMNUM                 (10) //值为  (5+ClassNum)&lt;br /&gt;
// #define SVP_SAMPLE_YOLOV3_BOXNUM                   (3)&lt;br /&gt;
// #define SVP_SAMPLE_YOLOV3_CLASSNUM                 (5)&lt;br /&gt;
// #define SVP_SAMPLE_YOLOV3_MAX_BOX_NUM              (15)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pstSoftWareParam-&amp;gt;u32OriImHeight = pstNnieParam-&amp;gt;astSegData[0].astSrc[0].unShape.stWhc.u32Height;
pstSoftWareParam-&amp;gt;u32OriImWidth = pstNnieParam-&amp;gt;astSegData[0].astSrc[0].unShape.stWhc.u32Width;
pstSoftWareParam-&amp;gt;u32BboxNumEachGrid = 3;
pstSoftWareParam-&amp;gt;u32ClassNum = 5;       //类的数目
pstSoftWareParam-&amp;gt;au32GridNumHeight[0] = 19;  //图片大小/32 (例：608/32)
pstSoftWareParam-&amp;gt;au32GridNumHeight[1] = 38;  //图片大小/16
pstSoftWareParam-&amp;gt;au32GridNumHeight[2] = 76;  //图片大小/8
pstSoftWareParam-&amp;gt;au32GridNumWidth[0] = 19;
pstSoftWareParam-&amp;gt;au32GridNumWidth[1] = 38;
pstSoftWareParam-&amp;gt;au32GridNumWidth[2] = 76;
pstSoftWareParam-&amp;gt;u32NmsThresh = (HI_U32)(0.3f*SAMPLE_SVP_NNIE_QUANT_BASE);
pstSoftWareParam-&amp;gt;u32ConfThresh = (HI_U32)(0.5f*SAMPLE_SVP_NNIE_QUANT_BASE);
pstSoftWareParam-&amp;gt;u32MaxRoiNum = 15;     //每个图片最多由多少个目标
/*以下为模型的anchor,具体计算方法请自行查询，不更改也可*/
pstSoftWareParam-&amp;gt;af32Bias[0][0] = 116; 
pstSoftWareParam-&amp;gt;af32Bias[0][1] = 90;



pstSoftWareParam-&amp;gt;u32OriImHeight = pstNnieParam-&amp;gt;astSegData[0].astSrc[0].unShape.stWhc.u32Height;
pstSoftWareParam-&amp;gt;u32OriImWidth = pstNnieParam-&amp;gt;astSegData[0].astSrc[0].unShape.stWhc.u32Width;
pstSoftWareParam-&amp;gt;u32BboxNumEachGrid = 3;
pstSoftWareParam-&amp;gt;u32ClassNum = 80;
pstSoftWareParam-&amp;gt;au32GridNumHeight[0] = 13;
pstSoftWareParam-&amp;gt;au32GridNumHeight[1] = 26;
pstSoftWareParam-&amp;gt;au32GridNumHeight[2] = 52;
pstSoftWareParam-&amp;gt;au32GridNumWidth[0] = 13;
pstSoftWareParam-&amp;gt;au32GridNumWidth[1] = 26;
pstSoftWareParam-&amp;gt;au32GridNumWidth[2] = 52;
pstSoftWareParam-&amp;gt;u32NmsThresh = (HI_U32)(0.15f*SAMPLE_SVP_NNIE_QUANT_BASE);
pstSoftWareParam-&amp;gt;u32ConfThresh = (HI_U32)(0.25f*SAMPLE_SVP_NNIE_QUANT_BASE);
pstSoftWareParam-&amp;gt;u32MaxRoiNum = 10;

pstSoftWareParam-&amp;gt;af32Bias[0][0] = 116;
pstSoftWareParam-&amp;gt;af32Bias[0][1] = 90;
pstSoftWareParam-&amp;gt;af32Bias[0][2] = 156;
pstSoftWareParam-&amp;gt;af32Bias[0][3] = 198;
pstSoftWareParam-&amp;gt;af32Bias[0][4] = 373;
pstSoftWareParam-&amp;gt;af32Bias[0][5] = 326;
pstSoftWareParam-&amp;gt;af32Bias[1][0] = 30;
pstSoftWareParam-&amp;gt;af32Bias[1][1] = 61;
pstSoftWareParam-&amp;gt;af32Bias[1][2] = 62;
pstSoftWareParam-&amp;gt;af32Bias[1][3] = 45;
pstSoftWareParam-&amp;gt;af32Bias[1][4] = 59;
pstSoftWareParam-&amp;gt;af32Bias[1][5] = 119;
pstSoftWareParam-&amp;gt;af32Bias[2][0] = 10;
pstSoftWareParam-&amp;gt;af32Bias[2][1] = 13;
pstSoftWareParam-&amp;gt;af32Bias[2][2] = 16;
pstSoftWareParam-&amp;gt;af32Bias[2][3] = 30;
pstSoftWareParam-&amp;gt;af32Bias[2][4] = 33;
pstSoftWareParam-&amp;gt;af32Bias[2][5] = 23;

/*Malloc assist buffer memory*/
u32ClassNum = pstSoftWareParam-&amp;gt;u32ClassNum+1;

... /*部分省略*/
...

return s32Ret;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;br /&gt;
这里面涉及到的数字参数都是在Ruyi中模型量化的时候对应的参数，如果默认用80类yolov3的话基本不用修改。&lt;/p&gt;

&lt;p&gt;u32ClassNum代表识别的类别数，这里是默认80类。&lt;/p&gt;

&lt;p&gt;主程序里面 f32PrintResultThresh 参数是识别为物体的阈值，这里配置的值是 0.15f，即大于15%的概率时是真实物体。&lt;/p&gt;

&lt;p&gt;由于海思模型量化后概率值量级分化很严重，所以阈值可以设置的比较低，与原始模型的阈值设置有些区别。&lt;/p&gt;

&lt;p&gt;/******************************************************************************&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;function : print detection result&lt;br /&gt;
******************************************************************************/&lt;br /&gt;
static HI_S32 SAMPLE_SVP_NNIE_Detection_PrintResult(SVP_BLOB_S *pstDstScore,&lt;br /&gt;
SVP_BLOB_S *pstDstRoi, SVP_BLOB_S &lt;em&gt;pstClassRoiNum, HI_FLOAT f32PrintResultThresh)&lt;br /&gt;
{&lt;br /&gt;
HI_U32 i = 0, j = 0;&lt;br /&gt;
HI_U32 u32RoiNumBias = 0;&lt;br /&gt;
HI_U32 u32ScoreBias = 0;&lt;br /&gt;
HI_U32 u32BboxBias = 0;&lt;br /&gt;
HI_FLOAT f32Score = 0.0f;&lt;br /&gt;
HI_S32&lt;/em&gt; ps32Score = SAMPLE_SVP_NNIE_CONVERT_64BIT_ADDR(HI_S32,pstDstScore-&amp;gt;u64VirAddr);&lt;br /&gt;
HI_S32* ps32Roi = SAMPLE_SVP_NNIE_CONVERT_64BIT_ADDR(HI_S32,pstDstRoi-&amp;gt;u64VirAddr);&lt;br /&gt;
HI_S32* ps32ClassRoiNum = SAMPLE_SVP_NNIE_CONVERT_64BIT_ADDR(HI_S32,pstClassRoiNum-&amp;gt;u64VirAddr);&lt;br /&gt;
HI_U32 u32ClassNum = pstClassRoiNum-&amp;gt;unShape.stWhc.u32Width;&lt;br /&gt;
HI_S32 s32XMin = 0,s32YMin= 0,s32XMax = 0,s32YMax = 0;&lt;/p&gt;

&lt;p&gt;u32RoiNumBias += ps32ClassRoiNum[0];&lt;br /&gt;
for (i = 1; i &amp;lt; u32ClassNum; i++)&lt;br /&gt;
{&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;u32ScoreBias = u32RoiNumBias;
u32BboxBias = u32RoiNumBias * SAMPLE_SVP_NNIE_COORDI_NUM;
/*if the confidence score greater than result threshold, the result will be printed*/
if((HI_FLOAT)ps32Score[u32ScoreBias] / SAMPLE_SVP_NNIE_QUANT_BASE &amp;gt;=
    f32PrintResultThresh &amp;amp;&amp;amp; ps32ClassRoiNum[i]!=0)
{
    SAMPLE_SVP_TRACE_INFO(&amp;quot;==== The %dth class box info====\n&amp;quot;, i);
}
for (j = 0; j &amp;lt; (HI_U32)ps32ClassRoiNum[i]; j++)
{
    f32Score = (HI_FLOAT)ps32Score[u32ScoreBias + j] / SAMPLE_SVP_NNIE_QUANT_BASE;
    if (f32Score &amp;lt; f32PrintResultThresh)
    {
        break;
    }
    s32XMin = ps32Roi[u32BboxBias + j*SAMPLE_SVP_NNIE_COORDI_NUM];
    s32YMin = ps32Roi[u32BboxBias + j*SAMPLE_SVP_NNIE_COORDI_NUM + 1];
    s32XMax = ps32Roi[u32BboxBias + j*SAMPLE_SVP_NNIE_COORDI_NUM + 2];
    s32YMax = ps32Roi[u32BboxBias + j*SAMPLE_SVP_NNIE_COORDI_NUM + 3];
    SAMPLE_SVP_TRACE_INFO(&amp;quot;%d %d %d %d %f\n&amp;quot;, s32XMin, s32YMin, s32XMax, s32YMax, f32Score);
}
u32RoiNumBias += ps32ClassRoiNum[i];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;br /&gt;
return HI_SUCCESS;&lt;br /&gt;
}&lt;br /&gt;
上面部分是打印识别到物体类别信息。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后打印从图片输入到识别输出的时间延迟。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;g_time_end = tp1.tv_sec * 1000 + tp1.tv_usec/1000;

printf(&amp;quot;yolov3 time : %d ms .\n&amp;quot;, g_time_end-g_time_start);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前运行未做修改的完整版 yolov3时间在70ms左右一张是比较正常的。&lt;/p&gt;

&lt;p&gt;开发板资料中 ..深度学习demo\yolov3\data\nnie_model\detection 中提供了已经量化后的 YOLOV3模型，&lt;/p&gt;

&lt;p&gt;是从Darknet YOLOV3模型上转成caffe下的yolov3模型的。&lt;/p&gt;

&lt;p&gt;..深度学习demo\yolov3\data\nnie_image\rgb_planar 中提供了可供测试的 dog_bike_car.jpg和转换后的.bgr图片。&lt;br /&gt;
————————————————&lt;br /&gt;
版权声明：本文为CSDN博主「LizardHan」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。&lt;br /&gt;
原文链接：&lt;a href=&#34;https://blog.csdn.net/kwdx2/article/details/100883764&#34; target=&#34;_blank&#34;&gt;https://blog.csdn.net/kwdx2/article/details/100883764&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>Linux驱动：module_platform_driver</title>
            <link>/hardware/kernel/linux-platform-module_platform_driver/</link>
            <pubDate>Mon, 09 Dec 2019 19:45:38 CST</pubDate>
            <author>rinetd</author>
            <guid>/hardware/kernel/linux-platform-module_platform_driver/</guid>
            <description>

&lt;p&gt;该宏是用来定义驱动的入口函数 &lt;code&gt;module_platform_driver(hibvt_i2c_driver);&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;static SIMPLE_DEV_PM_OPS(hibvt_i2c_dev_pm, hibvt_i2c_suspend,
		hibvt_i2c_resume);

static const struct of_device_id hibvt_i2c_match[] = {
	{ .compatible = &amp;quot;hisilicon,hibvt-i2c&amp;quot;},
	{ .compatible = &amp;quot;hisilicon,hi3516cv300-i2c&amp;quot;},
	{ .compatible = &amp;quot;hisilicon,hi3536dv100-i2c&amp;quot;},
	{},
};
MODULE_DEVICE_TABLE(of, hibvt_i2c_match);

static struct platform_driver hibvt_i2c_driver = {
	.driver		= {
		.name	= &amp;quot;hibvt-i2c&amp;quot;,
		.of_match_table = hibvt_i2c_match,
		.pm	= &amp;amp;hibvt_i2c_dev_pm,
	},
	.probe		= hibvt_i2c_probe,
	.remove		= hibvt_i2c_remove,
};

module_platform_driver(hibvt_i2c_driver);

/* 

module_platform_driver 宏等价于以下代码

static int __init hibvt_i2c_driver_init(void)
 {
   return platform_driver_register(&amp;amp;(hibvt_i2c_driver) );
 }
module_init(hibvt_i2c_driver_init); 

static void __exit hibvt_i2c_driver_exit(void) 
{ 
    platform_driver_unregister(&amp;amp;(hibvt_i2c_driver) ); 
} 
module_exit(hibvt_i2c_driver_exit);
*/
MODULE_AUTHOR(&amp;quot;Pan Wen, &amp;lt;wenpan@hisilicon.com&amp;gt;&amp;quot;);
MODULE_DESCRIPTION(&amp;quot;HISILICON BVT I2C Bus driver&amp;quot;);
MODULE_LICENSE(&amp;quot;GPL v2&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使用方法&#34;&gt;使用方法：&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;include &amp;lt;linux/platform_device.h&amp;gt;

static struct platform_driver power_supply_driver ={
      .driver  = { 
        .name = &amp;quot;power-supply&amp;quot;,
        .owner = THIS_MODULE,
        .of_match_table = power_supply_id_table,
    },
    .probe   = power_supply_probe,
    .remove  = power_supply_remove,
    .suspend = power_supply_suspend,
    .resume  = power_supply_resume,
}

module_platform_driver(power_supply_driver)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;platform_driver&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;struct platform_driver {
    int (*probe)(struct platform_device *);
    int (*remove)(struct platform_device *);
    void (*shutdown)(struct platform_device *);
    int (*suspend)(struct platform_device *, pm_message_t state);
    int (*resume)(struct platform_device *);
    struct device_driver driver;
    const struct platform_device_id *id_table;
    bool prevent_deferred_probe;
 };
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;原理&#34;&gt;原理：&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;module_platform_driver ：&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#define module_platform_driver(__platform_driver) \
    module_driver(__platform_driver, platform_driver_register, \
            platform_driver_unregister)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面的宏定义展开如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;module_driver(power_supply_driver, platform_driver_register, \
            platform_driver_unregister)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;module_driver&lt;/p&gt;

&lt;p&gt;定义：linux/device.h&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#define module_driver(__driver, __register, __unregister, ...) \
static int __init __driver##_init(void) \
{ \
    return __register(&amp;amp;(__driver) , ##__VA_ARGS__); \
} \
module_init(__driver##_init); \
static void __exit __driver##_exit(void) \
{ \
    __unregister(&amp;amp;(__driver) , ##__VA_ARGS__); \
} \
module_exit(__driver##_exit);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;展开规则：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;__driver         -----&amp;gt;    power_supply_driver
    __register      -----&amp;gt;   platform_driver_unregister
    __unregister  -----&amp;gt;   platform_driver_register
    __VA_ARGS__ -----&amp;gt;  .....
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-hljs&#34;&gt;C99 引入了对参数个数可变的函数式宏的正式支持。在宏原型的末尾加上符号 ... (就像在参数可变的函数定义中), 宏定义中的伪宏__VA_ARGS__ 就会在调用是 替换成可变参数。
    ##__VA_ARGS__
    当可变参数...的个数为0 时，&#39;##&#39;操作将使预处理器(preprocessor)去除掉它前面的那个逗号，保证编译能够通过
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以上面的方法最终形式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;static int __init power_supply_driver_init(void)
 {
   return platform_driver_register(&amp;amp;(power_supply_driver) );
 }
module_init(power_supply_driver_init); 

static void __exit power_supply_driver_exit(void) 
{ 
    platform_driver_unregister(&amp;amp;(power_supply_driver) ); 
} 
module_exit(power_supply_driver_exit);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;

&lt;p&gt;驱动中使用module_platform_driver 来注册驱动 跟自定义module_init &amp;amp;&amp;amp;module_exit 的结果是一致的，module_platform_driver 更加简洁，推荐使用&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>yolo anchorbox</title>
            <link>/ai/yolo/yolo-anchorbox/</link>
            <pubDate>Mon, 09 Dec 2019 19:01:55 CST</pubDate>
            <author>rinetd</author>
            <guid>/ai/yolo/yolo-anchorbox/</guid>
            <description>&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/mxdsdo09/article/details/85157162&#34; target=&#34;_blank&#34;&gt;AnchorBox的一些理解 - mxdsdo09的博客&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/qq_41997920/article/details/88657618&#34; target=&#34;_blank&#34;&gt;深度学习笔记（四）---YOLO目标检测 - 一一&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.jianshu.com/p/4544e4c06c5f&#34; target=&#34;_blank&#34;&gt;yolov3 生成对应自己样本的 anchor box 尺寸的代码 - 简书&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>yolo bounding box</title>
            <link>/ai/yolo/yolo-bounding-box/</link>
            <pubDate>Mon, 09 Dec 2019 18:45:41 CST</pubDate>
            <author>rinetd</author>
            <guid>/ai/yolo/yolo-bounding-box/</guid>
            <description>&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/zijin0802034/article/details/77685438&#34; target=&#34;_blank&#34;&gt;边框回归(Bounding Box Regression)详解 - 南有乔木NTU的博客&lt;/a&gt;&lt;br /&gt;
bounding box：你的答案&lt;br /&gt;
ground truth：标准答案（真实值，标签)&lt;br /&gt;
anchor box是锚框&lt;/p&gt;

&lt;p&gt;IoU：交并比&lt;br /&gt;
NMS：非极大抑制，只保留极大值&lt;br /&gt;
RoI：a region of interest，感兴趣区域&lt;br /&gt;
multi-task loss：多任务损失函数，是什么？是因为同时计算分类和回归两个才叫多任务的吗？&lt;br /&gt;
SPP：空间金字塔池化,如何实现的？  upsample 反操作&lt;/p&gt;

&lt;p&gt;一、yolov1 bounding box&lt;br /&gt;
bounding box是怎么产生的？回归产生的，回归？哎呀，妈呀，回归是什么我都忘记了，好吧，我来举个初中的线性回归例子简单回顾一下&lt;/p&gt;

&lt;p&gt;有两组数据A和B&lt;/p&gt;

&lt;p&gt;A = [1,2,3,4,5] &lt;/p&gt;

&lt;p&gt;B = [2,4,6,8,10]&lt;/p&gt;

&lt;p&gt;利用回归的思想预测当A为6的时候B对应的值是多少&lt;/p&gt;

&lt;p&gt;很明显回归参数就是2，回归程为y = 2x，所以A为6的时候B对应的值应该是12，这就是最简单的回归思路了。&lt;/p&gt;

&lt;p&gt;然后yolo1中提到的就是直接利用整幅图像经过网络结构产生7*7的grid cell，每个grid cell预测x，y，w，h，confidence等几个值，其中confidence就是IOU的值啦。关键这个bounding box怎么回归产生的，对我这种小白来说还是纠结了好几天才算理解到一丢丢，所以赶紧小笔记记一下，欢迎各位大佬斧正（请原谅我想多了，就我这个级别的blog还有大佬看[笑哭]）&lt;/p&gt;

&lt;p&gt;好，我们去看一下Annotation里面的XML文件，截取一部分看一下&lt;/p&gt;

&lt;p&gt;看到标注文件里面的Bounding box的格式了没，都是由[xmin,xmax,ymin,ymax]组成，中心坐标呢？&lt;/p&gt;

&lt;p&gt;回顾yolov1论文还说要把坐标和长宽归一化，你中心坐标都没有如何回归？最后在scipts找到了&lt;code&gt;vol_label.py&lt;/code&gt;文件才知道中心坐标是根据标签中的xmin，xmax，ymin，ymax计算出来的，哈哈哈，详见下面代码注释。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def convert(size, box):
    dw = 1./size[0] # 归一化的时候就是使用宽度除以整个image_size的宽度 
    dh = 1./size[1] # 归一化的时候就是使用高度除以整个image_size的高度
    x = (box[0] + box[1])/2.0 # 使用(xmin+xmax)/2得到x的中心点
    y = (box[2] + box[3])/2.0 # 使用(ymin+ymax)/2得到y的中心点
    w = box[1] - box[0] # 然后宽度就是使用xmax-xmin计算得到
    h = box[3] - box[2] # 然后高度就是使用ymax-ymin计算得到
    x = x*dw # 归一化中心坐标x
    w = w*dw # 归一化bbox宽度
    y = y*dh # 归一化中心坐标y
    h = h*dh # 归一化bbox高度
    return (x,y,w,h)
 
def convert_annotation(year, image_id):
    in_file = open(&#39;VOCdevkit/VOC%s/Annotations/%s.xml&#39;%(year, image_id))
    out_file = open(&#39;VOCdevkit/VOC%s/labels/%s.txt&#39;%(year, image_id), &#39;w&#39;)
    tree=ET.parse(in_file)
    root = tree.getroot()
    size = root.find(&#39;size&#39;)
    w = int(size.find(&#39;width&#39;).text)
    h = int(size.find(&#39;height&#39;).text)
 
    for obj in root.iter(&#39;object&#39;):
        difficult = obj.find(&#39;difficult&#39;).text
        cls = obj.find(&#39;name&#39;).text
        if cls not in classes or int(difficult) == 1:
            continue
        cls_id = classes.index(cls)
        xmlbox = obj.find(&#39;bndbox&#39;)
	# 获取标注中bbox的数据并以元组方式返回
        b = (float(xmlbox.find(&#39;xmin&#39;).text), float(xmlbox.find(&#39;xmax&#39;).text), float(xmlbox.find(&#39;ymin&#39;).text), float(xmlbox.find(&#39;ymax&#39;).text))
	# 向convert传入size = (w,h),和b,注释中分别是(xmin,xmax,ymin,ymax)   
	bb = convert((w,h), b)
        out_file.write(str(cls_id) + &amp;quot; &amp;quot; + &amp;quot; &amp;quot;.join([str(a) for a in bb]) + &#39;\n&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有了中心坐标x，y和bbox的w，h这样就可以愉快地训练出回归参数了&lt;br /&gt;
————————————————&lt;br /&gt;
版权声明：本文为CSDN博主「Li_haiyu」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。&lt;br /&gt;
原文链接：&lt;a href=&#34;https://blog.csdn.net/Li_haiyu/article/details/80509268&#34; target=&#34;_blank&#34;&gt;https://blog.csdn.net/Li_haiyu/article/details/80509268&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>nnie yolov3 jpg2bgr</title>
            <link>/hardware/hisilicon/nnie/nnie-yolov3-jpg2bgr/</link>
            <pubDate>Mon, 09 Dec 2019 17:28:14 CST</pubDate>
            <author>rinetd</author>
            <guid>/hardware/hisilicon/nnie/nnie-yolov3-jpg2bgr/</guid>
            <description>&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import cv2
 
imgpath = &amp;quot;./BGR_img/917.jpg&amp;quot;
saveimg = r&amp;quot;./BGR_img/917_608x608.bgr&amp;quot;
 
img = cv2.imread(imgpath)
save_img_size = 608
 
if img is None:
    print(&amp;quot;img is none&amp;quot;)
else:
    img = cv2.resize(img,(save_img_size,save_img_size))
    (B, G, R) = cv2.split(img)
    with open(saveimg,&#39;wb&#39;)as fp:
        for i in range(save_img_size):
            for j in range(save_img_size):
                fp.write(B[i, j])
        for i in range(save_img_size):
            for j in range(save_img_size):
                fp.write(G[i, j])
        for i in range(save_img_size):
            for j in range(save_img_size):
                fp.write(R[i, j])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;————————————————&lt;br /&gt;
版权声明：本文为CSDN博主「他们叫我高老师」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。&lt;br /&gt;
原文链接：&lt;a href=&#34;https://blog.csdn.net/qq_34533248/article/details/102497297&#34; target=&#34;_blank&#34;&gt;https://blog.csdn.net/qq_34533248/article/details/102497297&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
